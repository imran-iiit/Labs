{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of AI & ML\n",
    "## Session 08\n",
    "### Experiment 3 Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross-validation\n",
    "\n",
    "In this form of cross-validation the data set is divided into k subsets. Each time, one of the k subsets is used as the test set and the other k-1 subsets are put together to form a training set. Then the average error across all k trials is computed. \n",
    "\n",
    "In this experiment we are going to apply k-fold cross-validation method on the MNIST datasets and then tune the hyper parameters of MLPClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing required packages\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading MNIST dataset from sklearn\n",
    "digits = datasets.load_digits(n_class=10)\n",
    "## Loding the data and storing in x\n",
    "X = digits.data\n",
    "## Loading the target data and storing it in y\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hyper parameters\n",
    "# activation\n",
    "a = [\"identity\",\"logistic\",\"tanh\",\"relu\"]\n",
    "#solvers\n",
    "s = [\"lbfgs\",\"sgd\",\"adam\"]\n",
    "#learning rate\n",
    "lr = [0.0001,0.001,0.01,0.1]\n",
    "#hidden layers\n",
    "h = [(5,2),(3,2),(6,3),(7,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying K-Folds cross-validator\n",
    "kf = KFold(n_splits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to Create MLP classifier object with hyper parameters\n",
    "def mlp(a,s,h,lr):\n",
    "    clf = MLPClassifier(activation= a ,solver= s ,hidden_layer_sizes = h,max_iter = 5000 ,learning_rate = 'constant',learning_rate_init=lr)\n",
    "    return clf  \n",
    "#function to calculate the accuracy\n",
    "def accuracy(actual,predicted):\n",
    "    return np.count_nonzero(actual == predicted)*1.0/len(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise 1** Predict the values using test data and calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 450  451  452 ... 1794 1795 1796] [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449]\n",
      "[   0    1    2 ... 1794 1795 1796] [450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
      " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
      " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
      " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
      " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
      " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
      " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
      " 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791\n",
      " 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809\n",
      " 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827\n",
      " 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845\n",
      " 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863\n",
      " 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881\n",
      " 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898]\n",
      "[   0    1    2 ... 1794 1795 1796] [ 899  900  901  902  903  904  905  906  907  908  909  910  911  912\n",
      "  913  914  915  916  917  918  919  920  921  922  923  924  925  926\n",
      "  927  928  929  930  931  932  933  934  935  936  937  938  939  940\n",
      "  941  942  943  944  945  946  947  948  949  950  951  952  953  954\n",
      "  955  956  957  958  959  960  961  962  963  964  965  966  967  968\n",
      "  969  970  971  972  973  974  975  976  977  978  979  980  981  982\n",
      "  983  984  985  986  987  988  989  990  991  992  993  994  995  996\n",
      "  997  998  999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010\n",
      " 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024\n",
      " 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038\n",
      " 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052\n",
      " 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066\n",
      " 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080\n",
      " 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094\n",
      " 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108\n",
      " 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122\n",
      " 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136\n",
      " 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150\n",
      " 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164\n",
      " 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178\n",
      " 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192\n",
      " 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206\n",
      " 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220\n",
      " 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234\n",
      " 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248\n",
      " 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262\n",
      " 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276\n",
      " 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290\n",
      " 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304\n",
      " 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318\n",
      " 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332\n",
      " 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346\n",
      " 1347]\n",
      "[   0    1    2 ... 1345 1346 1347] [1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361\n",
      " 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375\n",
      " 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389\n",
      " 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403\n",
      " 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417\n",
      " 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431\n",
      " 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445\n",
      " 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459\n",
      " 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473\n",
      " 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487\n",
      " 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501\n",
      " 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515\n",
      " 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529\n",
      " 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543\n",
      " 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557\n",
      " 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571\n",
      " 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585\n",
      " 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599\n",
      " 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613\n",
      " 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627\n",
      " 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641\n",
      " 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655\n",
      " 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669\n",
      " 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683\n",
      " 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697\n",
      " 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711\n",
      " 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725\n",
      " 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739\n",
      " 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753\n",
      " 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767\n",
      " 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781\n",
      " 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795\n",
      " 1796]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "    print(train_index, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kf_accuracy(kf):\n",
    "    test_accuracy = []\n",
    "    train_accuracy = []\n",
    "    for i in range(10):\n",
    "        k1 = np.random.randint(0,len(a))\n",
    "        k2 = np.random.randint(0,len(s))\n",
    "        k3 = np.random.randint(0,len(lr))\n",
    "        k4 = np.random.randint(0,len(h))\n",
    "        print(\"\\nHyper-parameters = \\n activation = \", a[k1],    \"\\n solver = \", s[k2], \"\\n learning_rate_init = \", lr[k3],         \"\\n hidden_layer_sizes = \", h[k4])\n",
    "        # calling the mlp function with random hyper paramters\n",
    "        clf = mlp(a[k1],s[k2],h[k4],lr[k3])\n",
    "        tempTrain = 0\n",
    "        tempTest = 0\n",
    "        #In this experiment we are going to apply leave one out method on the MNIST datasets then we tune the hyper parameters of MulitiLayer perceptron classifier.\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            ## Splitting the data into train and test\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            Y_train, Y_test  = y[train_index], y[test_index]\n",
    "            ##fit the data into the model\n",
    "            clf.fit(X_train,Y_train)\n",
    "            ##predicting the values on the fitted model using train data\n",
    "            predTrain = clf.predict((X_train))\n",
    "            #adding the accuracy\n",
    "            tempTrain = tempTrain + accuracy(Y_train,predTrain)\n",
    "            ##predict the values on the fitted model using test data\n",
    "            predTest = clf.predict((X_test))\n",
    "            #adding the accuracy\n",
    "            tempTest = tempTest + accuracy(Y_test, predTest)\n",
    "        ##Calculating the train accuracy\n",
    "        train_accuracy.append(tempTrain*1.0/4)\n",
    "        ##Calculating the test accuracy\n",
    "        test_accuracy.append(tempTest*1.0/4)\n",
    "        print(\"(train,test) accuracy = \",tempTrain*1.0/4, tempTest*1.0/4)\n",
    "    return train_accuracy, test_accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper-parameters = \n",
      " activation =  logistic \n",
      " solver =  adam \n",
      " learning_rate_init =  0.001 \n",
      " hidden_layer_sizes =  (5, 2)\n",
      "(train,test) accuracy =  0.6045421301099927 0.5052684978965603\n"
     ]
    }
   ],
   "source": [
    "train_accuracy, test_accuracy = kf_accuracy(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plotting the data\n",
    "def plot(train_accuracy, test_accuracy):\n",
    "    xx = np.array(range(1,11))\n",
    "    plt.bar(xx-0.2,train_accuracy,width=0.2)\n",
    "    plt.bar(xx, test_accuracy,width=0.2)\n",
    "    plt.legend([\"Train\",\"Test\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper-parameters = \n",
      " activation =  logistic \n",
      " solver =  lbfgs \n",
      " learning_rate_init =  0.1 \n",
      " hidden_layer_sizes =  (5, 2)\n",
      "(train,test) accuracy =  0.4817412141278894 0.44236327641672857\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  tanh \n",
      " solver =  lbfgs \n",
      " learning_rate_init =  0.001 \n",
      " hidden_layer_sizes =  (7, 2)\n",
      "(train,test) accuracy =  0.7135741255983734 0.6244221727295225\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  logistic \n",
      " solver =  sgd \n",
      " learning_rate_init =  0.001 \n",
      " hidden_layer_sizes =  (6, 3)\n",
      "(train,test) accuracy =  0.48432815312189526 0.4284892353377877\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  logistic \n",
      " solver =  lbfgs \n",
      " learning_rate_init =  0.0001 \n",
      " hidden_layer_sizes =  (6, 3)\n",
      "(train,test) accuracy =  0.6987809760782837 0.5936946300420687\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  identity \n",
      " solver =  sgd \n",
      " learning_rate_init =  0.01 \n",
      " hidden_layer_sizes =  (5, 2)\n",
      "(train,test) accuracy =  0.6477431439025949 0.5653674832962138\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  logistic \n",
      " solver =  lbfgs \n",
      " learning_rate_init =  0.001 \n",
      " hidden_layer_sizes =  (5, 2)\n",
      "(train,test) accuracy =  0.5635392365494043 0.4997141796585004\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  relu \n",
      " solver =  sgd \n",
      " learning_rate_init =  0.1 \n",
      " hidden_layer_sizes =  (5, 2)\n",
      "(train,test) accuracy =  0.10202196770931776 0.09905592675080425\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  tanh \n",
      " solver =  adam \n",
      " learning_rate_init =  0.0001 \n",
      " hidden_layer_sizes =  (7, 2)\n",
      "(train,test) accuracy =  0.5514727749763735 0.45631526849789655\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  identity \n",
      " solver =  sgd \n",
      " learning_rate_init =  0.001 \n",
      " hidden_layer_sizes =  (7, 2)\n",
      "(train,test) accuracy =  0.7083994215081761 0.6388294976490968\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  identity \n",
      " solver =  sgd \n",
      " learning_rate_init =  0.01 \n",
      " hidden_layer_sizes =  (5, 2)\n",
      "(train,test) accuracy =  0.6642620208882692 0.6059663449641178\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0aa6fd2f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(*kf_accuracy(kf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2 ** Vary the number of k-fold splits and observe the changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper-parameters = \n",
      " activation =  tanh \n",
      " solver =  lbfgs \n",
      " learning_rate_init =  0.0001 \n",
      " hidden_layer_sizes =  (5, 2)\n",
      "(train,test) accuracy =  0.9444919669182428 0.8526225946617008\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  identity \n",
      " solver =  sgd \n",
      " learning_rate_init =  0.001 \n",
      " hidden_layer_sizes =  (7, 2)\n",
      "(train,test) accuracy =  1.6363736695936943 1.5052917442582248\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  logistic \n",
      " solver =  sgd \n",
      " learning_rate_init =  0.01 \n",
      " hidden_layer_sizes =  (6, 3)\n",
      "(train,test) accuracy =  1.6295486842899876 1.4692659838609563\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  relu \n",
      " solver =  sgd \n",
      " learning_rate_init =  0.0001 \n",
      " hidden_layer_sizes =  (5, 2)\n",
      "(train,test) accuracy =  0.5998708675514255 0.5749068901303539\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  relu \n",
      " solver =  sgd \n",
      " learning_rate_init =  0.1 \n",
      " hidden_layer_sizes =  (3, 2)\n",
      "(train,test) accuracy =  0.26649587242470874 0.24345903165735566\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  relu \n",
      " solver =  lbfgs \n",
      " learning_rate_init =  0.0001 \n",
      " hidden_layer_sizes =  (5, 2)\n",
      "(train,test) accuracy =  0.8117103847944391 0.7205152079453756\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  identity \n",
      " solver =  sgd \n",
      " learning_rate_init =  0.01 \n",
      " hidden_layer_sizes =  (5, 2)\n",
      "(train,test) accuracy =  1.6192119729114254 1.4816495965238983\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  logistic \n",
      " solver =  lbfgs \n",
      " learning_rate_init =  0.001 \n",
      " hidden_layer_sizes =  (5, 2)\n",
      "(train,test) accuracy =  1.4008349940717943 1.2308116076970825\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  relu \n",
      " solver =  lbfgs \n",
      " learning_rate_init =  0.0001 \n",
      " hidden_layer_sizes =  (3, 2)\n",
      "(train,test) accuracy =  0.5954999147653218 0.5267846058348852\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  tanh \n",
      " solver =  lbfgs \n",
      " learning_rate_init =  0.0001 \n",
      " hidden_layer_sizes =  (5, 2)\n",
      "(train,test) accuracy =  1.2015717007108495 1.077925201738051\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0aa673b2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kf = KFold(n_splits=10)\n",
    "plot(*kf_accuracy(kf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solutions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predTest = clf.predict((X_test))\n",
    "tempTest = tempTest + accuracy(Y_test,predTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
