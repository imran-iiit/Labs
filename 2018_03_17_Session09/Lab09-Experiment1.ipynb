{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of AI & ML\n",
    "## Session 09\n",
    "### Experiment 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "It’s a Python based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "1. A replacement for NumPy to use the power of GPUs\n",
    "\n",
    "2. a deep learning research platform that provides maximum flexibility and speed\n",
    "\n",
    "http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html\n",
    "\n",
    "In this experiment we will use MNIST dataset and will be implementing MLP using Pytorch. We are going to do this step-by-step\n",
    "\n",
    "1. Loading MNIST dataset and Visualize\n",
    "2. Defining Loss functions\n",
    "3. Doing forward pass\n",
    "4. Run the classifier the complete test set and compute accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install the pytorch run the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading pip-9.0.3-py2.py3-none-any.whl (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 577kB/s ta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 9.0.1\n",
      "    Uninstalling pip-9.0.1:\n",
      "      Successfully uninstalled pip-9.0.1\n",
      "Successfully installed pip-9.0.3\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtorch-0.3.1-cp35-cp35m-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install http://download.pytorch.org/whl/cpu/torch-0.3.1-cp35-cp35m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing required pytorch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters \n",
    "input_size = 784\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll load the MNIST data. First time we may have to download the data, which can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Loading the train set file\n",
    "train_dataset = dsets.MNIST(root='../data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),  \n",
    "                            download=True)\n",
    "#Loading the test set file\n",
    "test_dataset = dsets.MNIST(root='../data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dsets.MNIST?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the train dataset\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# loading the test dataset\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test data are provided via data loaders that provide iterators over the datasets. Loading X and Y train values from the loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([10, 1, 28, 28]) type: torch.FloatTensor\n",
      "y_train: torch.Size([10]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting first 10 training digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112559fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pltsize=1\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i,:,:,:].numpy().reshape(28,28), cmap=\"gray\")\n",
    "    plt.title('Class: '+str(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the network as a Python class. We have to write the __init__() and forward() methods, and PyTorch will automatically generate a backward() method for computing the gradients for the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Linear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a neural network object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CrossEntropyLoss function uses inputs, labels  to calculate the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.CrossEntropyLoss?\n",
    "nn.Module?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/6000], Loss: 0.2056\n",
      "Epoch [1/5], Step [200/6000], Loss: 0.0601\n",
      "Epoch [1/5], Step [300/6000], Loss: 0.5993\n",
      "Epoch [1/5], Step [400/6000], Loss: 0.3932\n",
      "Epoch [1/5], Step [500/6000], Loss: 0.1116\n",
      "Epoch [1/5], Step [600/6000], Loss: 0.1122\n",
      "Epoch [1/5], Step [700/6000], Loss: 0.3272\n",
      "Epoch [1/5], Step [800/6000], Loss: 0.0616\n",
      "Epoch [1/5], Step [900/6000], Loss: 0.2600\n",
      "Epoch [1/5], Step [1000/6000], Loss: 0.1075\n",
      "Epoch [1/5], Step [1100/6000], Loss: 0.0186\n",
      "Epoch [1/5], Step [1200/6000], Loss: 0.7629\n",
      "Epoch [1/5], Step [1300/6000], Loss: 0.0373\n",
      "Epoch [1/5], Step [1400/6000], Loss: 0.0388\n",
      "Epoch [1/5], Step [1500/6000], Loss: 0.1475\n",
      "Epoch [1/5], Step [1600/6000], Loss: 0.8414\n",
      "Epoch [1/5], Step [1700/6000], Loss: 0.0806\n",
      "Epoch [1/5], Step [1800/6000], Loss: 0.1269\n",
      "Epoch [1/5], Step [1900/6000], Loss: 0.2006\n",
      "Epoch [1/5], Step [2000/6000], Loss: 0.7020\n",
      "Epoch [1/5], Step [2100/6000], Loss: 0.1661\n",
      "Epoch [1/5], Step [2200/6000], Loss: 0.2803\n",
      "Epoch [1/5], Step [2300/6000], Loss: 0.0765\n",
      "Epoch [1/5], Step [2400/6000], Loss: 0.4666\n",
      "Epoch [1/5], Step [2500/6000], Loss: 0.6146\n",
      "Epoch [1/5], Step [2600/6000], Loss: 0.0401\n",
      "Epoch [1/5], Step [2700/6000], Loss: 1.2095\n",
      "Epoch [1/5], Step [2800/6000], Loss: 0.0515\n",
      "Epoch [1/5], Step [2900/6000], Loss: 0.3190\n",
      "Epoch [1/5], Step [3000/6000], Loss: 0.0574\n",
      "Epoch [1/5], Step [3100/6000], Loss: 0.0371\n",
      "Epoch [1/5], Step [3200/6000], Loss: 0.1059\n",
      "Epoch [1/5], Step [3300/6000], Loss: 0.0259\n",
      "Epoch [1/5], Step [3400/6000], Loss: 0.0167\n",
      "Epoch [1/5], Step [3500/6000], Loss: 0.4937\n",
      "Epoch [1/5], Step [3600/6000], Loss: 0.0078\n",
      "Epoch [1/5], Step [3700/6000], Loss: 0.0756\n",
      "Epoch [1/5], Step [3800/6000], Loss: 0.0360\n",
      "Epoch [1/5], Step [3900/6000], Loss: 0.0016\n",
      "Epoch [1/5], Step [4000/6000], Loss: 0.1120\n",
      "Epoch [1/5], Step [4100/6000], Loss: 0.4899\n",
      "Epoch [1/5], Step [4200/6000], Loss: 0.0092\n",
      "Epoch [1/5], Step [4300/6000], Loss: 0.0077\n",
      "Epoch [1/5], Step [4400/6000], Loss: 0.0024\n",
      "Epoch [1/5], Step [4500/6000], Loss: 0.2563\n",
      "Epoch [1/5], Step [4600/6000], Loss: 0.2465\n",
      "Epoch [1/5], Step [4700/6000], Loss: 0.0305\n",
      "Epoch [1/5], Step [4800/6000], Loss: 0.0842\n",
      "Epoch [1/5], Step [4900/6000], Loss: 0.0397\n",
      "Epoch [1/5], Step [5000/6000], Loss: 0.2441\n",
      "Epoch [1/5], Step [5100/6000], Loss: 0.0318\n",
      "Epoch [1/5], Step [5200/6000], Loss: 0.1317\n",
      "Epoch [1/5], Step [5300/6000], Loss: 0.2664\n",
      "Epoch [1/5], Step [5400/6000], Loss: 0.2076\n",
      "Epoch [1/5], Step [5500/6000], Loss: 0.0620\n",
      "Epoch [1/5], Step [5600/6000], Loss: 0.0796\n",
      "Epoch [1/5], Step [5700/6000], Loss: 0.1656\n",
      "Epoch [1/5], Step [5800/6000], Loss: 0.0267\n",
      "Epoch [1/5], Step [5900/6000], Loss: 0.0008\n",
      "Epoch [1/5], Step [6000/6000], Loss: 0.0217\n",
      "Epoch [2/5], Step [100/6000], Loss: 0.0102\n",
      "Epoch [2/5], Step [200/6000], Loss: 0.0640\n",
      "Epoch [2/5], Step [300/6000], Loss: 0.0093\n",
      "Epoch [2/5], Step [400/6000], Loss: 0.1501\n",
      "Epoch [2/5], Step [500/6000], Loss: 0.0125\n",
      "Epoch [2/5], Step [600/6000], Loss: 0.0045\n",
      "Epoch [2/5], Step [700/6000], Loss: 0.0069\n",
      "Epoch [2/5], Step [800/6000], Loss: 0.0332\n",
      "Epoch [2/5], Step [900/6000], Loss: 0.0181\n",
      "Epoch [2/5], Step [1000/6000], Loss: 0.0230\n",
      "Epoch [2/5], Step [1100/6000], Loss: 0.0102\n",
      "Epoch [2/5], Step [1200/6000], Loss: 0.3135\n",
      "Epoch [2/5], Step [1300/6000], Loss: 0.1499\n",
      "Epoch [2/5], Step [1400/6000], Loss: 0.1068\n",
      "Epoch [2/5], Step [1500/6000], Loss: 0.0035\n",
      "Epoch [2/5], Step [1600/6000], Loss: 0.0426\n",
      "Epoch [2/5], Step [1700/6000], Loss: 0.1217\n",
      "Epoch [2/5], Step [1800/6000], Loss: 0.1197\n",
      "Epoch [2/5], Step [1900/6000], Loss: 0.0064\n",
      "Epoch [2/5], Step [2000/6000], Loss: 0.0549\n",
      "Epoch [2/5], Step [2100/6000], Loss: 0.2412\n",
      "Epoch [2/5], Step [2200/6000], Loss: 0.0280\n",
      "Epoch [2/5], Step [2300/6000], Loss: 0.1649\n",
      "Epoch [2/5], Step [2400/6000], Loss: 0.0022\n",
      "Epoch [2/5], Step [2500/6000], Loss: 0.0014\n",
      "Epoch [2/5], Step [2600/6000], Loss: 0.0067\n",
      "Epoch [2/5], Step [2700/6000], Loss: 0.0050\n",
      "Epoch [2/5], Step [2800/6000], Loss: 0.0107\n",
      "Epoch [2/5], Step [2900/6000], Loss: 0.0021\n",
      "Epoch [2/5], Step [3000/6000], Loss: 0.0017\n",
      "Epoch [2/5], Step [3100/6000], Loss: 0.0607\n",
      "Epoch [2/5], Step [3200/6000], Loss: 0.0038\n",
      "Epoch [2/5], Step [3300/6000], Loss: 0.0013\n",
      "Epoch [2/5], Step [3400/6000], Loss: 0.0105\n",
      "Epoch [2/5], Step [3500/6000], Loss: 0.0045\n",
      "Epoch [2/5], Step [3600/6000], Loss: 0.0098\n",
      "Epoch [2/5], Step [3700/6000], Loss: 0.5800\n",
      "Epoch [2/5], Step [3800/6000], Loss: 0.0019\n",
      "Epoch [2/5], Step [3900/6000], Loss: 0.3630\n",
      "Epoch [2/5], Step [4000/6000], Loss: 0.0057\n",
      "Epoch [2/5], Step [4100/6000], Loss: 0.0425\n",
      "Epoch [2/5], Step [4200/6000], Loss: 1.1531\n",
      "Epoch [2/5], Step [4300/6000], Loss: 0.0130\n",
      "Epoch [2/5], Step [4400/6000], Loss: 0.0169\n",
      "Epoch [2/5], Step [4500/6000], Loss: 0.0021\n",
      "Epoch [2/5], Step [4600/6000], Loss: 0.1047\n",
      "Epoch [2/5], Step [4700/6000], Loss: 0.3007\n",
      "Epoch [2/5], Step [4800/6000], Loss: 0.0307\n",
      "Epoch [2/5], Step [4900/6000], Loss: 0.0090\n",
      "Epoch [2/5], Step [5000/6000], Loss: 0.0037\n",
      "Epoch [2/5], Step [5100/6000], Loss: 0.0699\n",
      "Epoch [2/5], Step [5200/6000], Loss: 0.0461\n",
      "Epoch [2/5], Step [5300/6000], Loss: 0.1750\n",
      "Epoch [2/5], Step [5400/6000], Loss: 0.2481\n",
      "Epoch [2/5], Step [5500/6000], Loss: 0.0024\n",
      "Epoch [2/5], Step [5600/6000], Loss: 0.0351\n",
      "Epoch [2/5], Step [5700/6000], Loss: 0.0197\n",
      "Epoch [2/5], Step [5800/6000], Loss: 0.8764\n",
      "Epoch [2/5], Step [5900/6000], Loss: 0.0044\n",
      "Epoch [2/5], Step [6000/6000], Loss: 0.0030\n",
      "Epoch [3/5], Step [100/6000], Loss: 0.0782\n",
      "Epoch [3/5], Step [200/6000], Loss: 0.0008\n",
      "Epoch [3/5], Step [300/6000], Loss: 0.0323\n",
      "Epoch [3/5], Step [400/6000], Loss: 0.0003\n",
      "Epoch [3/5], Step [500/6000], Loss: 0.0014\n",
      "Epoch [3/5], Step [600/6000], Loss: 0.0002\n",
      "Epoch [3/5], Step [700/6000], Loss: 0.0849\n",
      "Epoch [3/5], Step [800/6000], Loss: 0.0225\n",
      "Epoch [3/5], Step [900/6000], Loss: 0.0442\n",
      "Epoch [3/5], Step [1000/6000], Loss: 0.0053\n",
      "Epoch [3/5], Step [1100/6000], Loss: 0.0013\n",
      "Epoch [3/5], Step [1200/6000], Loss: 0.0005\n",
      "Epoch [3/5], Step [1300/6000], Loss: 0.0006\n",
      "Epoch [3/5], Step [1400/6000], Loss: 0.4457\n",
      "Epoch [3/5], Step [1500/6000], Loss: 0.0018\n",
      "Epoch [3/5], Step [1600/6000], Loss: 0.0141\n",
      "Epoch [3/5], Step [1700/6000], Loss: 0.0287\n",
      "Epoch [3/5], Step [1800/6000], Loss: 0.0079\n",
      "Epoch [3/5], Step [1900/6000], Loss: 0.0150\n",
      "Epoch [3/5], Step [2000/6000], Loss: 0.0236\n",
      "Epoch [3/5], Step [2100/6000], Loss: 0.0040\n",
      "Epoch [3/5], Step [2200/6000], Loss: 0.0027\n",
      "Epoch [3/5], Step [2300/6000], Loss: 0.0005\n",
      "Epoch [3/5], Step [2400/6000], Loss: 0.1242\n",
      "Epoch [3/5], Step [2500/6000], Loss: 0.0000\n",
      "Epoch [3/5], Step [2600/6000], Loss: 0.0007\n",
      "Epoch [3/5], Step [2700/6000], Loss: 0.0019\n",
      "Epoch [3/5], Step [2800/6000], Loss: 0.0053\n",
      "Epoch [3/5], Step [2900/6000], Loss: 0.0010\n",
      "Epoch [3/5], Step [3000/6000], Loss: 0.0844\n",
      "Epoch [3/5], Step [3100/6000], Loss: 0.4434\n",
      "Epoch [3/5], Step [3200/6000], Loss: 0.0291\n",
      "Epoch [3/5], Step [3300/6000], Loss: 0.0338\n",
      "Epoch [3/5], Step [3400/6000], Loss: 0.0108\n",
      "Epoch [3/5], Step [3500/6000], Loss: 0.0016\n",
      "Epoch [3/5], Step [3600/6000], Loss: 0.0003\n",
      "Epoch [3/5], Step [3700/6000], Loss: 0.0003\n",
      "Epoch [3/5], Step [3800/6000], Loss: 0.5073\n",
      "Epoch [3/5], Step [3900/6000], Loss: 0.0004\n",
      "Epoch [3/5], Step [4000/6000], Loss: 0.0320\n",
      "Epoch [3/5], Step [4100/6000], Loss: 0.0172\n",
      "Epoch [3/5], Step [4200/6000], Loss: 0.0060\n",
      "Epoch [3/5], Step [4300/6000], Loss: 0.0084\n",
      "Epoch [3/5], Step [4400/6000], Loss: 0.0010\n",
      "Epoch [3/5], Step [4500/6000], Loss: 0.0186\n",
      "Epoch [3/5], Step [4600/6000], Loss: 0.0680\n",
      "Epoch [3/5], Step [4700/6000], Loss: 0.1498\n",
      "Epoch [3/5], Step [4800/6000], Loss: 0.0012\n",
      "Epoch [3/5], Step [4900/6000], Loss: 0.1652\n",
      "Epoch [3/5], Step [5000/6000], Loss: 0.0038\n",
      "Epoch [3/5], Step [5100/6000], Loss: 0.0105\n",
      "Epoch [3/5], Step [5200/6000], Loss: 0.0054\n",
      "Epoch [3/5], Step [5300/6000], Loss: 0.0181\n",
      "Epoch [3/5], Step [5400/6000], Loss: 0.0015\n",
      "Epoch [3/5], Step [5500/6000], Loss: 0.0376\n",
      "Epoch [3/5], Step [5600/6000], Loss: 0.0169\n",
      "Epoch [3/5], Step [5700/6000], Loss: 0.0103\n",
      "Epoch [3/5], Step [5800/6000], Loss: 0.0012\n",
      "Epoch [3/5], Step [5900/6000], Loss: 0.0066\n",
      "Epoch [3/5], Step [6000/6000], Loss: 0.0024\n",
      "Epoch [4/5], Step [100/6000], Loss: 0.0017\n",
      "Epoch [4/5], Step [200/6000], Loss: 0.0015\n",
      "Epoch [4/5], Step [300/6000], Loss: 0.0000\n",
      "Epoch [4/5], Step [400/6000], Loss: 0.0009\n",
      "Epoch [4/5], Step [500/6000], Loss: 0.0051\n",
      "Epoch [4/5], Step [600/6000], Loss: 0.0005\n",
      "Epoch [4/5], Step [700/6000], Loss: 0.0367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [800/6000], Loss: 0.0314\n",
      "Epoch [4/5], Step [900/6000], Loss: 0.0003\n",
      "Epoch [4/5], Step [1000/6000], Loss: 0.0000\n",
      "Epoch [4/5], Step [1100/6000], Loss: 0.0586\n",
      "Epoch [4/5], Step [1200/6000], Loss: 0.2106\n",
      "Epoch [4/5], Step [1300/6000], Loss: 0.1109\n",
      "Epoch [4/5], Step [1400/6000], Loss: 0.0210\n",
      "Epoch [4/5], Step [1500/6000], Loss: 0.0009\n",
      "Epoch [4/5], Step [1600/6000], Loss: 0.1050\n",
      "Epoch [4/5], Step [1700/6000], Loss: 0.0180\n",
      "Epoch [4/5], Step [1800/6000], Loss: 0.0030\n",
      "Epoch [4/5], Step [1900/6000], Loss: 0.0065\n",
      "Epoch [4/5], Step [2000/6000], Loss: 0.0009\n",
      "Epoch [4/5], Step [2100/6000], Loss: 0.0001\n",
      "Epoch [4/5], Step [2200/6000], Loss: 0.2135\n",
      "Epoch [4/5], Step [2300/6000], Loss: 0.0017\n",
      "Epoch [4/5], Step [2400/6000], Loss: 0.0115\n",
      "Epoch [4/5], Step [2500/6000], Loss: 0.0000\n",
      "Epoch [4/5], Step [2600/6000], Loss: 0.0036\n",
      "Epoch [4/5], Step [2700/6000], Loss: 0.0003\n",
      "Epoch [4/5], Step [2800/6000], Loss: 0.0001\n",
      "Epoch [4/5], Step [2900/6000], Loss: 0.0094\n",
      "Epoch [4/5], Step [3000/6000], Loss: 0.0003\n",
      "Epoch [4/5], Step [3100/6000], Loss: 0.0044\n",
      "Epoch [4/5], Step [3200/6000], Loss: 0.0002\n",
      "Epoch [4/5], Step [3300/6000], Loss: 0.0000\n",
      "Epoch [4/5], Step [3400/6000], Loss: 0.0334\n",
      "Epoch [4/5], Step [3500/6000], Loss: 0.0001\n",
      "Epoch [4/5], Step [3600/6000], Loss: 0.0001\n",
      "Epoch [4/5], Step [3700/6000], Loss: 0.0006\n",
      "Epoch [4/5], Step [3800/6000], Loss: 0.0002\n",
      "Epoch [4/5], Step [3900/6000], Loss: 0.2379\n",
      "Epoch [4/5], Step [4000/6000], Loss: 0.0000\n",
      "Epoch [4/5], Step [4100/6000], Loss: 0.7954\n",
      "Epoch [4/5], Step [4200/6000], Loss: 0.0021\n",
      "Epoch [4/5], Step [4300/6000], Loss: 0.0040\n",
      "Epoch [4/5], Step [4400/6000], Loss: 0.0256\n",
      "Epoch [4/5], Step [4500/6000], Loss: 0.0001\n",
      "Epoch [4/5], Step [4600/6000], Loss: 0.0001\n",
      "Epoch [4/5], Step [4700/6000], Loss: 0.0518\n",
      "Epoch [4/5], Step [4800/6000], Loss: 0.0000\n",
      "Epoch [4/5], Step [4900/6000], Loss: 0.0023\n",
      "Epoch [4/5], Step [5000/6000], Loss: 0.0006\n",
      "Epoch [4/5], Step [5100/6000], Loss: 0.0001\n",
      "Epoch [4/5], Step [5200/6000], Loss: 0.0000\n",
      "Epoch [4/5], Step [5300/6000], Loss: 0.0030\n",
      "Epoch [4/5], Step [5400/6000], Loss: 0.1182\n",
      "Epoch [4/5], Step [5500/6000], Loss: 0.0031\n",
      "Epoch [4/5], Step [5600/6000], Loss: 0.4072\n",
      "Epoch [4/5], Step [5700/6000], Loss: 0.0074\n",
      "Epoch [4/5], Step [5800/6000], Loss: 0.1663\n",
      "Epoch [4/5], Step [5900/6000], Loss: 0.7706\n",
      "Epoch [4/5], Step [6000/6000], Loss: 0.0065\n",
      "Epoch [5/5], Step [100/6000], Loss: 0.0019\n",
      "Epoch [5/5], Step [200/6000], Loss: 0.0043\n",
      "Epoch [5/5], Step [300/6000], Loss: 0.0021\n",
      "Epoch [5/5], Step [400/6000], Loss: 0.0017\n",
      "Epoch [5/5], Step [500/6000], Loss: 0.0006\n",
      "Epoch [5/5], Step [600/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [700/6000], Loss: 0.0002\n",
      "Epoch [5/5], Step [800/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [900/6000], Loss: 0.0122\n",
      "Epoch [5/5], Step [1000/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [1100/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [1200/6000], Loss: 0.0003\n",
      "Epoch [5/5], Step [1300/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [1400/6000], Loss: 0.0029\n",
      "Epoch [5/5], Step [1500/6000], Loss: 0.0097\n",
      "Epoch [5/5], Step [1600/6000], Loss: 0.0224\n",
      "Epoch [5/5], Step [1700/6000], Loss: 0.0036\n",
      "Epoch [5/5], Step [1800/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [1900/6000], Loss: 0.0004\n",
      "Epoch [5/5], Step [2000/6000], Loss: 0.0002\n",
      "Epoch [5/5], Step [2100/6000], Loss: 0.0481\n",
      "Epoch [5/5], Step [2200/6000], Loss: 0.0458\n",
      "Epoch [5/5], Step [2300/6000], Loss: 0.0123\n",
      "Epoch [5/5], Step [2400/6000], Loss: 0.0132\n",
      "Epoch [5/5], Step [2500/6000], Loss: 0.0018\n",
      "Epoch [5/5], Step [2600/6000], Loss: 0.0023\n",
      "Epoch [5/5], Step [2700/6000], Loss: 0.0661\n",
      "Epoch [5/5], Step [2800/6000], Loss: 0.0003\n",
      "Epoch [5/5], Step [2900/6000], Loss: 0.0132\n",
      "Epoch [5/5], Step [3000/6000], Loss: 0.0002\n",
      "Epoch [5/5], Step [3100/6000], Loss: 0.0519\n",
      "Epoch [5/5], Step [3200/6000], Loss: 0.0045\n",
      "Epoch [5/5], Step [3300/6000], Loss: 0.2464\n",
      "Epoch [5/5], Step [3400/6000], Loss: 0.0143\n",
      "Epoch [5/5], Step [3500/6000], Loss: 0.0004\n",
      "Epoch [5/5], Step [3600/6000], Loss: 0.0015\n",
      "Epoch [5/5], Step [3700/6000], Loss: 0.0037\n",
      "Epoch [5/5], Step [3800/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [3900/6000], Loss: 0.0022\n",
      "Epoch [5/5], Step [4000/6000], Loss: 0.0037\n",
      "Epoch [5/5], Step [4100/6000], Loss: 0.0002\n",
      "Epoch [5/5], Step [4200/6000], Loss: 0.0002\n",
      "Epoch [5/5], Step [4300/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [4400/6000], Loss: 0.0762\n",
      "Epoch [5/5], Step [4500/6000], Loss: 0.0104\n",
      "Epoch [5/5], Step [4600/6000], Loss: 0.0590\n",
      "Epoch [5/5], Step [4700/6000], Loss: 0.0005\n",
      "Epoch [5/5], Step [4800/6000], Loss: 0.0079\n",
      "Epoch [5/5], Step [4900/6000], Loss: 0.0000\n",
      "Epoch [5/5], Step [5000/6000], Loss: 0.0014\n",
      "Epoch [5/5], Step [5100/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [5200/6000], Loss: 0.0021\n",
      "Epoch [5/5], Step [5300/6000], Loss: 0.0201\n",
      "Epoch [5/5], Step [5400/6000], Loss: 0.0047\n",
      "Epoch [5/5], Step [5500/6000], Loss: 0.0003\n",
      "Epoch [5/5], Step [5600/6000], Loss: 0.0450\n",
      "Epoch [5/5], Step [5700/6000], Loss: 0.0104\n",
      "Epoch [5/5], Step [5800/6000], Loss: 0.0011\n",
      "Epoch [5/5], Step [5900/6000], Loss: 0.1232\n",
      "Epoch [5/5], Step [6000/6000], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Convert torch tensor to Variable\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()  ### zero the gradient buffer ------- Clears the gradients of all optimized\n",
    "        inputs = net(images) ### IAS: This will call forward() from __call__\n",
    "        loss = criterion(inputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.zero_grad?\n",
    "# loss.backward?\n",
    "# optimizer.step?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, 28*28))\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1:\n",
    "\n",
    "Change the number of epochs to 10 and batch size to 50. Check the output for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/1200], Loss: 0.0007\n",
      "Epoch [1/10], Step [200/1200], Loss: 0.0002\n",
      "Epoch [1/10], Step [300/1200], Loss: 0.0001\n",
      "Epoch [1/10], Step [400/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [500/1200], Loss: 0.0013\n",
      "Epoch [1/10], Step [600/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [700/1200], Loss: 0.0011\n",
      "Epoch [1/10], Step [800/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [900/1200], Loss: 0.2621\n",
      "Epoch [1/10], Step [1000/1200], Loss: 0.0027\n",
      "Epoch [1/10], Step [1100/1200], Loss: 0.0003\n",
      "Epoch [1/10], Step [1200/1200], Loss: 0.0003\n",
      "Epoch [1/10], Step [1300/1200], Loss: 0.0031\n",
      "Epoch [1/10], Step [1400/1200], Loss: 0.0364\n",
      "Epoch [1/10], Step [1500/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [1600/1200], Loss: 0.0004\n",
      "Epoch [1/10], Step [1700/1200], Loss: 0.0020\n",
      "Epoch [1/10], Step [1800/1200], Loss: 0.0002\n",
      "Epoch [1/10], Step [1900/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [2000/1200], Loss: 0.0159\n",
      "Epoch [1/10], Step [2100/1200], Loss: 0.0055\n",
      "Epoch [1/10], Step [2200/1200], Loss: 0.0758\n",
      "Epoch [1/10], Step [2300/1200], Loss: 0.0209\n",
      "Epoch [1/10], Step [2400/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [2500/1200], Loss: 0.0001\n",
      "Epoch [1/10], Step [2600/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [2700/1200], Loss: 0.0530\n",
      "Epoch [1/10], Step [2800/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [2900/1200], Loss: 0.0021\n",
      "Epoch [1/10], Step [3000/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [3100/1200], Loss: 0.0377\n",
      "Epoch [1/10], Step [3200/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [3300/1200], Loss: 0.1072\n",
      "Epoch [1/10], Step [3400/1200], Loss: 0.0085\n",
      "Epoch [1/10], Step [3500/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [3600/1200], Loss: 0.0002\n",
      "Epoch [1/10], Step [3700/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [3800/1200], Loss: 0.1547\n",
      "Epoch [1/10], Step [3900/1200], Loss: 0.0001\n",
      "Epoch [1/10], Step [4000/1200], Loss: 0.0001\n",
      "Epoch [1/10], Step [4100/1200], Loss: 0.0294\n",
      "Epoch [1/10], Step [4200/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [4300/1200], Loss: 0.0001\n",
      "Epoch [1/10], Step [4400/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [4500/1200], Loss: 0.0232\n",
      "Epoch [1/10], Step [4600/1200], Loss: 0.0007\n",
      "Epoch [1/10], Step [4700/1200], Loss: 0.0126\n",
      "Epoch [1/10], Step [4800/1200], Loss: 0.0067\n",
      "Epoch [1/10], Step [4900/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [5000/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [5100/1200], Loss: 0.0001\n",
      "Epoch [1/10], Step [5200/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [5300/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [5400/1200], Loss: 0.0026\n",
      "Epoch [1/10], Step [5500/1200], Loss: 0.0002\n",
      "Epoch [1/10], Step [5600/1200], Loss: 0.0001\n",
      "Epoch [1/10], Step [5700/1200], Loss: 0.0002\n",
      "Epoch [1/10], Step [5800/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [5900/1200], Loss: 0.0021\n",
      "Epoch [1/10], Step [6000/1200], Loss: 0.0006\n",
      "Epoch [2/10], Step [100/1200], Loss: 0.0008\n",
      "Epoch [2/10], Step [200/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [300/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [400/1200], Loss: 0.0006\n",
      "Epoch [2/10], Step [500/1200], Loss: 0.0001\n",
      "Epoch [2/10], Step [600/1200], Loss: 0.0002\n",
      "Epoch [2/10], Step [700/1200], Loss: 0.0001\n",
      "Epoch [2/10], Step [800/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [900/1200], Loss: 0.0009\n",
      "Epoch [2/10], Step [1000/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [1100/1200], Loss: 0.0013\n",
      "Epoch [2/10], Step [1200/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [1300/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [1400/1200], Loss: 0.0008\n",
      "Epoch [2/10], Step [1500/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [1600/1200], Loss: 0.0137\n",
      "Epoch [2/10], Step [1700/1200], Loss: 0.0015\n",
      "Epoch [2/10], Step [1800/1200], Loss: 0.0007\n",
      "Epoch [2/10], Step [1900/1200], Loss: 0.0997\n",
      "Epoch [2/10], Step [2000/1200], Loss: 0.1957\n",
      "Epoch [2/10], Step [2100/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [2200/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [2300/1200], Loss: 0.0001\n",
      "Epoch [2/10], Step [2400/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [2500/1200], Loss: 0.0003\n",
      "Epoch [2/10], Step [2600/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [2700/1200], Loss: 0.0001\n",
      "Epoch [2/10], Step [2800/1200], Loss: 0.0001\n",
      "Epoch [2/10], Step [2900/1200], Loss: 0.0011\n",
      "Epoch [2/10], Step [3000/1200], Loss: 0.0057\n",
      "Epoch [2/10], Step [3100/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [3200/1200], Loss: 0.0003\n",
      "Epoch [2/10], Step [3300/1200], Loss: 0.0002\n",
      "Epoch [2/10], Step [3400/1200], Loss: 0.6776\n",
      "Epoch [2/10], Step [3500/1200], Loss: 0.0564\n",
      "Epoch [2/10], Step [3600/1200], Loss: 0.0003\n",
      "Epoch [2/10], Step [3700/1200], Loss: 0.0002\n",
      "Epoch [2/10], Step [3800/1200], Loss: 0.0606\n",
      "Epoch [2/10], Step [3900/1200], Loss: 0.0001\n",
      "Epoch [2/10], Step [4000/1200], Loss: 0.1939\n",
      "Epoch [2/10], Step [4100/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [4200/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [4300/1200], Loss: 0.0438\n",
      "Epoch [2/10], Step [4400/1200], Loss: 0.0210\n",
      "Epoch [2/10], Step [4500/1200], Loss: 0.0017\n",
      "Epoch [2/10], Step [4600/1200], Loss: 0.0003\n",
      "Epoch [2/10], Step [4700/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [4800/1200], Loss: 0.5335\n",
      "Epoch [2/10], Step [4900/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [5000/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [5100/1200], Loss: 0.0056\n",
      "Epoch [2/10], Step [5200/1200], Loss: 0.0034\n",
      "Epoch [2/10], Step [5300/1200], Loss: 0.0006\n",
      "Epoch [2/10], Step [5400/1200], Loss: 0.0018\n",
      "Epoch [2/10], Step [5500/1200], Loss: 0.0010\n",
      "Epoch [2/10], Step [5600/1200], Loss: 0.0050\n",
      "Epoch [2/10], Step [5700/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [5800/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [5900/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [6000/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [100/1200], Loss: 0.0003\n",
      "Epoch [3/10], Step [200/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [300/1200], Loss: 0.0016\n",
      "Epoch [3/10], Step [400/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [500/1200], Loss: 0.3881\n",
      "Epoch [3/10], Step [600/1200], Loss: 0.7923\n",
      "Epoch [3/10], Step [700/1200], Loss: 0.0006\n",
      "Epoch [3/10], Step [800/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [900/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [1000/1200], Loss: 0.0059\n",
      "Epoch [3/10], Step [1100/1200], Loss: 0.0018\n",
      "Epoch [3/10], Step [1200/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [1300/1200], Loss: 0.0403\n",
      "Epoch [3/10], Step [1400/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [1500/1200], Loss: 0.0137\n",
      "Epoch [3/10], Step [1600/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [1700/1200], Loss: 0.0171\n",
      "Epoch [3/10], Step [1800/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [1900/1200], Loss: 0.0008\n",
      "Epoch [3/10], Step [2000/1200], Loss: 0.0028\n",
      "Epoch [3/10], Step [2100/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [2200/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [2300/1200], Loss: 0.0026\n",
      "Epoch [3/10], Step [2400/1200], Loss: 0.0205\n",
      "Epoch [3/10], Step [2500/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [2600/1200], Loss: 0.0010\n",
      "Epoch [3/10], Step [2700/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [2800/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [2900/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [3000/1200], Loss: 0.0002\n",
      "Epoch [3/10], Step [3100/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [3200/1200], Loss: 0.0001\n",
      "Epoch [3/10], Step [3300/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [3400/1200], Loss: 0.0312\n",
      "Epoch [3/10], Step [3500/1200], Loss: 0.0008\n",
      "Epoch [3/10], Step [3600/1200], Loss: 0.0922\n",
      "Epoch [3/10], Step [3700/1200], Loss: 0.0001\n",
      "Epoch [3/10], Step [3800/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [3900/1200], Loss: 0.0001\n",
      "Epoch [3/10], Step [4000/1200], Loss: 0.0003\n",
      "Epoch [3/10], Step [4100/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [4200/1200], Loss: 0.0006\n",
      "Epoch [3/10], Step [4300/1200], Loss: 0.0023\n",
      "Epoch [3/10], Step [4400/1200], Loss: 0.0001\n",
      "Epoch [3/10], Step [4500/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [4600/1200], Loss: 0.0003\n",
      "Epoch [3/10], Step [4700/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [4800/1200], Loss: 0.0006\n",
      "Epoch [3/10], Step [4900/1200], Loss: 0.0001\n",
      "Epoch [3/10], Step [5000/1200], Loss: 0.0479\n",
      "Epoch [3/10], Step [5100/1200], Loss: 0.0002\n",
      "Epoch [3/10], Step [5200/1200], Loss: 0.0013\n",
      "Epoch [3/10], Step [5300/1200], Loss: 0.1946\n",
      "Epoch [3/10], Step [5400/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [5500/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [5600/1200], Loss: 0.0571\n",
      "Epoch [3/10], Step [5700/1200], Loss: 0.0004\n",
      "Epoch [3/10], Step [5800/1200], Loss: 0.0005\n",
      "Epoch [3/10], Step [5900/1200], Loss: 0.0001\n",
      "Epoch [3/10], Step [6000/1200], Loss: 0.0001\n",
      "Epoch [4/10], Step [100/1200], Loss: 0.7944\n",
      "Epoch [4/10], Step [200/1200], Loss: 0.0001\n",
      "Epoch [4/10], Step [300/1200], Loss: 0.0030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Step [400/1200], Loss: 0.4152\n",
      "Epoch [4/10], Step [500/1200], Loss: 0.0004\n",
      "Epoch [4/10], Step [600/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [700/1200], Loss: 0.0022\n",
      "Epoch [4/10], Step [800/1200], Loss: 0.0319\n",
      "Epoch [4/10], Step [900/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [1000/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [1100/1200], Loss: 0.0037\n",
      "Epoch [4/10], Step [1200/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [1300/1200], Loss: 0.0001\n",
      "Epoch [4/10], Step [1400/1200], Loss: 0.2406\n",
      "Epoch [4/10], Step [1500/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [1600/1200], Loss: 0.0001\n",
      "Epoch [4/10], Step [1700/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [1800/1200], Loss: 0.0001\n",
      "Epoch [4/10], Step [1900/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [2000/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [2100/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [2200/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [2300/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [2400/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [2500/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [2600/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [2700/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [2800/1200], Loss: 0.0037\n",
      "Epoch [4/10], Step [2900/1200], Loss: 0.0001\n",
      "Epoch [4/10], Step [3000/1200], Loss: 0.0009\n",
      "Epoch [4/10], Step [3100/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [3200/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [3300/1200], Loss: 0.0003\n",
      "Epoch [4/10], Step [3400/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [3500/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [3600/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [3700/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [3800/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [3900/1200], Loss: 0.0014\n",
      "Epoch [4/10], Step [4000/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [4100/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [4200/1200], Loss: 0.0007\n",
      "Epoch [4/10], Step [4300/1200], Loss: 0.0002\n",
      "Epoch [4/10], Step [4400/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [4500/1200], Loss: 0.0059\n",
      "Epoch [4/10], Step [4600/1200], Loss: 0.0006\n",
      "Epoch [4/10], Step [4700/1200], Loss: 0.0002\n",
      "Epoch [4/10], Step [4800/1200], Loss: 0.0003\n",
      "Epoch [4/10], Step [4900/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [5000/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [5100/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [5200/1200], Loss: 0.0002\n",
      "Epoch [4/10], Step [5300/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [5400/1200], Loss: 0.1053\n",
      "Epoch [4/10], Step [5500/1200], Loss: 0.1070\n",
      "Epoch [4/10], Step [5600/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [5700/1200], Loss: 0.0051\n",
      "Epoch [4/10], Step [5800/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [5900/1200], Loss: 0.0380\n",
      "Epoch [4/10], Step [6000/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [100/1200], Loss: 0.0006\n",
      "Epoch [5/10], Step [200/1200], Loss: 0.0040\n",
      "Epoch [5/10], Step [300/1200], Loss: 0.0399\n",
      "Epoch [5/10], Step [400/1200], Loss: 0.0001\n",
      "Epoch [5/10], Step [500/1200], Loss: 0.0095\n",
      "Epoch [5/10], Step [600/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [700/1200], Loss: 0.0488\n",
      "Epoch [5/10], Step [800/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [900/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [1000/1200], Loss: 0.1108\n",
      "Epoch [5/10], Step [1100/1200], Loss: 0.0001\n",
      "Epoch [5/10], Step [1200/1200], Loss: 0.0001\n",
      "Epoch [5/10], Step [1300/1200], Loss: 0.0001\n",
      "Epoch [5/10], Step [1400/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [1500/1200], Loss: 0.0326\n",
      "Epoch [5/10], Step [1600/1200], Loss: 0.4026\n",
      "Epoch [5/10], Step [1700/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [1800/1200], Loss: 0.3372\n",
      "Epoch [5/10], Step [1900/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [2000/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [2100/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [2200/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [2300/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [2400/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [2500/1200], Loss: 0.0042\n",
      "Epoch [5/10], Step [2600/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [2700/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [2800/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [2900/1200], Loss: 0.0230\n",
      "Epoch [5/10], Step [3000/1200], Loss: 0.0001\n",
      "Epoch [5/10], Step [3100/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [3200/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [3300/1200], Loss: 0.0005\n",
      "Epoch [5/10], Step [3400/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [3500/1200], Loss: 0.0006\n",
      "Epoch [5/10], Step [3600/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [3700/1200], Loss: 0.0067\n",
      "Epoch [5/10], Step [3800/1200], Loss: 0.0001\n",
      "Epoch [5/10], Step [3900/1200], Loss: 0.0003\n",
      "Epoch [5/10], Step [4000/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [4100/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [4200/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [4300/1200], Loss: 0.0019\n",
      "Epoch [5/10], Step [4400/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [4500/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [4600/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [4700/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [4800/1200], Loss: 0.0001\n",
      "Epoch [5/10], Step [4900/1200], Loss: 0.0007\n",
      "Epoch [5/10], Step [5000/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [5100/1200], Loss: 0.0112\n",
      "Epoch [5/10], Step [5200/1200], Loss: 0.0011\n",
      "Epoch [5/10], Step [5300/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [5400/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [5500/1200], Loss: 0.1032\n",
      "Epoch [5/10], Step [5600/1200], Loss: 0.0012\n",
      "Epoch [5/10], Step [5700/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [5800/1200], Loss: 0.0001\n",
      "Epoch [5/10], Step [5900/1200], Loss: 0.0002\n",
      "Epoch [5/10], Step [6000/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [100/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [200/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [300/1200], Loss: 0.0001\n",
      "Epoch [6/10], Step [400/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [500/1200], Loss: 0.1026\n",
      "Epoch [6/10], Step [600/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [700/1200], Loss: 0.0004\n",
      "Epoch [6/10], Step [800/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [900/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [1000/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [1100/1200], Loss: 0.0206\n",
      "Epoch [6/10], Step [1200/1200], Loss: 0.0007\n",
      "Epoch [6/10], Step [1300/1200], Loss: 0.0001\n",
      "Epoch [6/10], Step [1400/1200], Loss: 0.0001\n",
      "Epoch [6/10], Step [1500/1200], Loss: 0.2502\n",
      "Epoch [6/10], Step [1600/1200], Loss: 0.0021\n",
      "Epoch [6/10], Step [1700/1200], Loss: 0.0023\n",
      "Epoch [6/10], Step [1800/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [1900/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [2000/1200], Loss: 0.0001\n",
      "Epoch [6/10], Step [2100/1200], Loss: 0.0010\n",
      "Epoch [6/10], Step [2200/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [2300/1200], Loss: 0.0148\n",
      "Epoch [6/10], Step [2400/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [2500/1200], Loss: 0.0016\n",
      "Epoch [6/10], Step [2600/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [2700/1200], Loss: 0.0487\n",
      "Epoch [6/10], Step [2800/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [2900/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [3000/1200], Loss: 0.0003\n",
      "Epoch [6/10], Step [3100/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [3200/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [3300/1200], Loss: 0.0004\n",
      "Epoch [6/10], Step [3400/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [3500/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [3600/1200], Loss: 0.0015\n",
      "Epoch [6/10], Step [3700/1200], Loss: 0.0001\n",
      "Epoch [6/10], Step [3800/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [3900/1200], Loss: 0.0037\n",
      "Epoch [6/10], Step [4000/1200], Loss: 0.0004\n",
      "Epoch [6/10], Step [4100/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [4200/1200], Loss: 0.0108\n",
      "Epoch [6/10], Step [4300/1200], Loss: 0.0001\n",
      "Epoch [6/10], Step [4400/1200], Loss: 0.0001\n",
      "Epoch [6/10], Step [4500/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [4600/1200], Loss: 0.0002\n",
      "Epoch [6/10], Step [4700/1200], Loss: 0.0002\n",
      "Epoch [6/10], Step [4800/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [4900/1200], Loss: 0.0474\n",
      "Epoch [6/10], Step [5000/1200], Loss: 0.0084\n",
      "Epoch [6/10], Step [5100/1200], Loss: 0.0006\n",
      "Epoch [6/10], Step [5200/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [5300/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [5400/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [5500/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [5600/1200], Loss: 0.3429\n",
      "Epoch [6/10], Step [5700/1200], Loss: 0.0001\n",
      "Epoch [6/10], Step [5800/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [5900/1200], Loss: 0.1568\n",
      "Epoch [6/10], Step [6000/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [100/1200], Loss: 0.0002\n",
      "Epoch [7/10], Step [200/1200], Loss: 0.1109\n",
      "Epoch [7/10], Step [300/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [400/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [500/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [600/1200], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Step [700/1200], Loss: 0.0005\n",
      "Epoch [7/10], Step [800/1200], Loss: 0.0024\n",
      "Epoch [7/10], Step [900/1200], Loss: 0.0011\n",
      "Epoch [7/10], Step [1000/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [1100/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [1200/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [1300/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [1400/1200], Loss: 0.0109\n",
      "Epoch [7/10], Step [1500/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [1600/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [1700/1200], Loss: 0.0037\n",
      "Epoch [7/10], Step [1800/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [1900/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [2000/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [2100/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [2200/1200], Loss: 0.0001\n",
      "Epoch [7/10], Step [2300/1200], Loss: 0.0469\n",
      "Epoch [7/10], Step [2400/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [2500/1200], Loss: 0.0004\n",
      "Epoch [7/10], Step [2600/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [2700/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [2800/1200], Loss: 0.0051\n",
      "Epoch [7/10], Step [2900/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [3000/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [3100/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [3200/1200], Loss: 0.2087\n",
      "Epoch [7/10], Step [3300/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [3400/1200], Loss: 0.0001\n",
      "Epoch [7/10], Step [3500/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [3600/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [3700/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [3800/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [3900/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [4000/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [4100/1200], Loss: 0.0009\n",
      "Epoch [7/10], Step [4200/1200], Loss: 0.0001\n",
      "Epoch [7/10], Step [4300/1200], Loss: 0.0533\n",
      "Epoch [7/10], Step [4400/1200], Loss: 0.0001\n",
      "Epoch [7/10], Step [4500/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [4600/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [4700/1200], Loss: 0.0053\n",
      "Epoch [7/10], Step [4800/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [4900/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [5000/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [5100/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [5200/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [5300/1200], Loss: 0.2411\n",
      "Epoch [7/10], Step [5400/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [5500/1200], Loss: 0.0002\n",
      "Epoch [7/10], Step [5600/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [5700/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [5800/1200], Loss: 0.0048\n",
      "Epoch [7/10], Step [5900/1200], Loss: 0.0792\n",
      "Epoch [7/10], Step [6000/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [100/1200], Loss: 0.0006\n",
      "Epoch [8/10], Step [200/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [300/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [400/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [500/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [600/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [700/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [800/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [900/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [1000/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [1100/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [1200/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [1300/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [1400/1200], Loss: 0.0001\n",
      "Epoch [8/10], Step [1500/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [1600/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [1700/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [1800/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [1900/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [2000/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [2100/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [2200/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [2300/1200], Loss: 0.0030\n",
      "Epoch [8/10], Step [2400/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [2500/1200], Loss: 0.0032\n",
      "Epoch [8/10], Step [2600/1200], Loss: 0.0001\n",
      "Epoch [8/10], Step [2700/1200], Loss: 0.0001\n",
      "Epoch [8/10], Step [2800/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [2900/1200], Loss: 0.0006\n",
      "Epoch [8/10], Step [3000/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [3100/1200], Loss: 0.0004\n",
      "Epoch [8/10], Step [3200/1200], Loss: 0.0002\n",
      "Epoch [8/10], Step [3300/1200], Loss: 0.0042\n",
      "Epoch [8/10], Step [3400/1200], Loss: 0.0101\n",
      "Epoch [8/10], Step [3500/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [3600/1200], Loss: 0.2012\n",
      "Epoch [8/10], Step [3700/1200], Loss: 0.0001\n",
      "Epoch [8/10], Step [3800/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [3900/1200], Loss: 0.0032\n",
      "Epoch [8/10], Step [4000/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [4100/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [4200/1200], Loss: 0.0006\n",
      "Epoch [8/10], Step [4300/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [4400/1200], Loss: 0.1070\n",
      "Epoch [8/10], Step [4500/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [4600/1200], Loss: 0.0068\n",
      "Epoch [8/10], Step [4700/1200], Loss: 0.0229\n",
      "Epoch [8/10], Step [4800/1200], Loss: 0.0160\n",
      "Epoch [8/10], Step [4900/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [5000/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [5100/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [5200/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [5300/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [5400/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [5500/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [5600/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [5700/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [5800/1200], Loss: 0.0002\n",
      "Epoch [8/10], Step [5900/1200], Loss: 0.0001\n",
      "Epoch [8/10], Step [6000/1200], Loss: 0.0006\n",
      "Epoch [9/10], Step [100/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [200/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [300/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [400/1200], Loss: 0.0008\n",
      "Epoch [9/10], Step [500/1200], Loss: 0.0258\n",
      "Epoch [9/10], Step [600/1200], Loss: 0.0091\n",
      "Epoch [9/10], Step [700/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [800/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [900/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [1000/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [1100/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [1200/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [1300/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [1400/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [1500/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [1600/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [1700/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [1800/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [1900/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [2000/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [2100/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [2200/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [2300/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [2400/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [2500/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [2600/1200], Loss: 0.0012\n",
      "Epoch [9/10], Step [2700/1200], Loss: 0.0230\n",
      "Epoch [9/10], Step [2800/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [2900/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [3000/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [3100/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [3200/1200], Loss: 0.0007\n",
      "Epoch [9/10], Step [3300/1200], Loss: 0.0022\n",
      "Epoch [9/10], Step [3400/1200], Loss: 0.0839\n",
      "Epoch [9/10], Step [3500/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [3600/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [3700/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [3800/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [3900/1200], Loss: 0.0002\n",
      "Epoch [9/10], Step [4000/1200], Loss: 0.0039\n",
      "Epoch [9/10], Step [4100/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [4200/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [4300/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [4400/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [4500/1200], Loss: 0.0005\n",
      "Epoch [9/10], Step [4600/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [4700/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [4800/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [4900/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [5000/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [5100/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [5200/1200], Loss: 0.2974\n",
      "Epoch [9/10], Step [5300/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [5400/1200], Loss: 0.0128\n",
      "Epoch [9/10], Step [5500/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [5600/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [5700/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [5800/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [5900/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [6000/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [100/1200], Loss: 0.3197\n",
      "Epoch [10/10], Step [200/1200], Loss: 0.0064\n",
      "Epoch [10/10], Step [300/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [400/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [500/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [600/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [700/1200], Loss: 0.0002\n",
      "Epoch [10/10], Step [800/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [900/1200], Loss: 0.0002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Step [1000/1200], Loss: 0.0652\n",
      "Epoch [10/10], Step [1100/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [1200/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [1300/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [1400/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [1500/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [1600/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [1700/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [1800/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [1900/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [2000/1200], Loss: 0.0009\n",
      "Epoch [10/10], Step [2100/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [2200/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [2300/1200], Loss: 0.0001\n",
      "Epoch [10/10], Step [2400/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [2500/1200], Loss: 0.0003\n",
      "Epoch [10/10], Step [2600/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [2700/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [2800/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [2900/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [3000/1200], Loss: 1.4339\n",
      "Epoch [10/10], Step [3100/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [3200/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [3300/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [3400/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [3500/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [3600/1200], Loss: 0.0031\n",
      "Epoch [10/10], Step [3700/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [3800/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [3900/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [4000/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [4100/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [4200/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [4300/1200], Loss: 0.0002\n",
      "Epoch [10/10], Step [4400/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [4500/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [4600/1200], Loss: 0.0003\n",
      "Epoch [10/10], Step [4700/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [4800/1200], Loss: 0.0002\n",
      "Epoch [10/10], Step [4900/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [5000/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [5100/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [5200/1200], Loss: 0.0025\n",
      "Epoch [10/10], Step [5300/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [5400/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [5500/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [5600/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [5700/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [5800/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [5900/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [6000/1200], Loss: 0.0000\n",
      "Accuracy of the network on the 10000 test images: 97 %\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Convert torch tensor to Variable\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()  ### zero the gradient buffer ------- Clears the gradients of all optimized\n",
    "        inputs = net(images) ### IAS: This will call forward() from __call__\n",
    "        loss = criterion(inputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "            \n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, 28*28))\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We see that the accuracy increased by 1% :-) !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
