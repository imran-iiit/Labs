{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of AI & ML\n",
    "## Session 05\n",
    "### CaseStudy\n",
    "### Lab\n",
    "\n",
    "**Objectives:** Create a linear regression based product rating solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Unnamed: 0        ratings\n",
      "count  167597.00000  167597.000000\n",
      "mean    83798.00000       4.356307\n",
      "std     48381.23087       0.993501\n",
      "min         0.00000       1.000000\n",
      "25%     41899.00000       4.000000\n",
      "50%     83798.00000       5.000000\n",
      "75%    125697.00000       5.000000\n",
      "max    167596.00000       5.000000\n",
      "          Unnamed: 0        ratings\n",
      "count  167504.000000  167504.000000\n",
      "mean    83798.019253       4.356427\n",
      "std     48380.619090       0.993334\n",
      "min         0.000000       1.000000\n",
      "25%     41899.750000       4.000000\n",
      "50%     83795.500000       5.000000\n",
      "75%    125699.250000       5.000000\n",
      "max    167596.000000       5.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"../Datasets/amazon_reviews.csv\")\n",
    "print(data.describe())\n",
    "data = data.dropna()\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>reviews</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I like the item pricing. My granddaughter want...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Love the magnet easel... great for moving to d...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Both sides are magnetic.  A real plus when you...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Bought one a few years ago for my daughter and...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I have a stainless steel refrigerator therefor...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            reviews  ratings\n",
       "0           0  I like the item pricing. My granddaughter want...      5.0\n",
       "1           1  Love the magnet easel... great for moving to d...      4.0\n",
       "2           2  Both sides are magnetic.  A real plus when you...      5.0\n",
       "3           3  Bought one a few years ago for my daughter and...      5.0\n",
       "4           4  I have a stainless steel refrigerator therefor...      4.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>reviews</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>167592</th>\n",
       "      <td>167592</td>\n",
       "      <td>This drone is very fun and super duarable. Its...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167593</th>\n",
       "      <td>167593</td>\n",
       "      <td>This is my brother's most prized toy. It's ext...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167594</th>\n",
       "      <td>167594</td>\n",
       "      <td>This Panther Drone toy is awesome. I definitel...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167595</th>\n",
       "      <td>167595</td>\n",
       "      <td>This is my first drone and it has proven to be...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167596</th>\n",
       "      <td>167596</td>\n",
       "      <td>This is a super fun toy to have around. In our...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                            reviews  ratings\n",
       "167592      167592  This drone is very fun and super duarable. Its...      5.0\n",
       "167593      167593  This is my brother's most prized toy. It's ext...      5.0\n",
       "167594      167594  This Panther Drone toy is awesome. I definitel...      5.0\n",
       "167595      167595  This is my first drone and it has proven to be...      5.0\n",
       "167596      167596  This is a super fun toy to have around. In our...      4.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = data['ratings'].values\n",
    "reviews = data['reviews'].values\n",
    "lengths = [len(r) for r in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5., 4., 5., 5., 4.]),\n",
       " array(['I like the item pricing. My granddaughter wanted to mark on it but I wanted it just for the letters.',\n",
       "        'Love the magnet easel... great for moving to different areas... Wish it had some sort of non skid pad on bottom though...',\n",
       "        \"Both sides are magnetic.  A real plus when you're entertaining more than one child.  The four-year old can find the letters for the words, while the two-year old can find the pictures the words spell.  (I bought letters and magnetic pictures to go with this board).  Both grandkids liked it a lot, which means I like it a lot as well.  Have not even introduced markers, as this will be used strictly as a magnetic board.\",\n",
       "        'Bought one a few years ago for my daughter and she loves it, still using it today. For the holidays we bought one for our niece and she loved it too.',\n",
       "        'I have a stainless steel refrigerator therefore there are not much space for my son to play with his magnet. Brought this for him to put his magnet on. He enjoys sticking his magnet on it. Great to have so he can play with his alphabet magnets.'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings[:5], reviews[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP TEMP TEMP DEBUGGING\n",
    "# ratings = ratings[:2000]\n",
    "# reviews = reviews[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We first preprocess the data by removing all the incorrect rows (that have missing rating or reviews), unwanted columns, removing stopwords and soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "only_alnum = re.compile(r\"[^a-z0-9]+\")\n",
    "## Replaces one or more occurrence of any characters other than a-z and 0-9 with a space\n",
    "## This automatically replaces multiple spaces by 1 space\n",
    "\n",
    "## The try ... except ensures that if a review is mal-formed then the review is replaced with the word ERROR\n",
    "def cleanUp(s):\n",
    "    return re.sub(only_alnum, \" \", s.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We make a set for testing if a word is not useful\n",
    "## sets are way faster than lists for this purpose\n",
    "fluff = set([w.strip() for w in open(\"../Datasets/fluff.txt\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cool\n",
      "amaazzing\n",
      "cool\n"
     ]
    }
   ],
   "source": [
    "## Replace words like coooooool with cool, amaaaaaazing with amaazing and so on\n",
    "def dedup(s):\n",
    "    return re.sub(r'([a-z])\\1+', r'\\1\\1', s)\n",
    "print(dedup(\"cooooool\"))\n",
    "print(dedup(\"amaaaaaazzzzing\"))\n",
    "print(dedup('cool'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_useful_words(s):\n",
    "    return [dedup(w) for w in cleanUp(s).split() if len(w) > 2 and w not in fluff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100 I like the item pricing. My granddaughter wanted to mark on it but I wanted it just for the letters. \n",
      "==> ['like', 'item', 'pricing', 'granddaughter', 'mark', 'letters']\n",
      " 121 Love the magnet easel... great for moving to different areas... Wish it had some sort of non skid pad on bottom though... \n",
      "==> ['love', 'magnet', 'easel', 'great', 'moving', 'wish', 'sort', 'skid', 'pad', 'bottom']\n",
      " 420 Both sides are magnetic.  A real plus when you're entertaining more than one child.  The four-year old can find the letters for the words, while the two-year old can find the pictures the words spell.  (I bought letters and magnetic pictures to go with this board).  Both grandkids liked it a lot, which means I like it a lot as well.  Have not even introduced markers, as this will be used strictly as a magnetic board. \n",
      "==> ['magnetic', 'real', 'plus', 'entertaining', 'more', 'child', 'letters', 'words', 'pictures', 'words', 'spell', 'bought', 'letters', 'magnetic', 'pictures', 'board', 'grandkids', 'liked', 'lot', 'means', 'like', 'lot', 'introduced', 'markers', 'strictly', 'magnetic', 'board']\n",
      " 149 Bought one a few years ago for my daughter and she loves it, still using it today. For the holidays we bought one for our niece and she loved it too. \n",
      "==> ['bought', 'few', 'ago', 'daughter', 'loves', 'using', 'holidays', 'bought', 'niece', 'loved']\n",
      " 244 I have a stainless steel refrigerator therefore there are not much space for my son to play with his magnet. Brought this for him to put his magnet on. He enjoys sticking his magnet on it. Great to have so he can play with his alphabet magnets. \n",
      "==> ['stainless', 'steel', 'refrigerator', 'space', 'son', 'play', 'magnet', 'brought', 'magnet', 'enjoys', 'sticking', 'magnet', 'great', 'play', 'alphabet', 'magnets']\n"
     ]
    }
   ],
   "source": [
    "clean_reviews = [get_useful_words(review) for review in reviews]\n",
    "for i in range(5):\n",
    "    print(\"%4d\" %(len(reviews[i])), reviews[i], \"\\n==>\", clean_reviews[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['love', 'games', 'like', 'stone', 'age', 'lords', 'waterdeep', 'tried', 'hawaii', 'game', 'bit', 'more', 'complicated', 'learn', 'first', 'bit', 'more', 'fiddly', 'lot', 'double', 'sided', 'bits', 'sort', 'worth', 'hang', 'shouldn', 'more', 'play', 'game', 'kinds', 'awesome', 'stone', 'age', 'lords', 'waterdeep', 'aren', 'first', 'meeple', 'chief', 'move', 'modular', 'board', 'instead', 'bunch', 'workers', 'like', 'regular', 'worker', 'placement', 'games', 'forms', 'wooden', 'currency', 'shells', 'money', 'feet', 'movement', 'fruit', 'wild', 'card', 'build', 'villages', 'adding', 'buildings', 'items', 'people', 'plus', 'buy', 'boats', 'visit', 'neighboring', 'islands', 'contain', 'random', 'goodies', 'bunch', 'help', 'king', 'hawaii', 'first', 'form', 'shells', 'feet', 'round', 'generosity', 'diminishes', 'means', 'supplement', 'declining', 'income', 'purchasing', 'various', 'shell', 'feet', 'producing', 'huts', 'buying', 'fruits', 'layout', 'tiles', 'stuff', 'buy', 'change', 'game', 'tiles', 'price', 'changes', 'round', 'slots', 'chief', 'change', 'round', 'prices', 'stuff', 'available', 'chance', 'definitely', 'slots', 'move', 'player', 'game', 'player', 'game', 'villages', 'five', 'villages', 'top', 'score', 'village', 'reach', 'tiki', 'idol', 'buy', 'more', 'tikis', 'less', 'stuff', 'reach', 'score', 'buy', 'kahunas', 'village', 'worth', 'bonus', 'game', 'scoring', 'game', 'scoring', 'round', 'escalating', 'total', 'meet', 'claim', 'bonus', 'buying', 'stuff', 'ooh', 'buy', 'pay', 'double', 'depending', 'tikis', 'kahunas', 'else', 'better', 'version', 'usually', 'double', 'hut', 'tat', 'produces', 'shell', 'round', 'produce', 'bought', 'upgraded', 'version', 'double', 'bunch', 'shrines', 'hawaiian', 'gods', 'buy', 'top', 'stack', 'visible', 'randomized', 'buy', 'add', 'god', 'shrine', 'villages', 'bonus', 'game', 'game', 'example', 'shrine', 'god', 'pele', 'move', 'main', 'island', 'feet', 'base', 'version', 'buy', 'upgraded', 'shrine', 'move', 'main', 'island', 'foot', 'considering', 'spots', 'board', 'feet', 'starting', 'position', 'beach', 'grabbing', 'god', 'save', 'ton', 'feet', 'tokens', 'game', 'handles', 'players', 'scales', 'surprisingly', 'maxed', 'play', 'time', 'hour', 'minutes', 'depending', 'analysis', 'paralysis', 'players', 'roughly', 'minutes', 'player', 'version', 'realistic', 'player', 'game', 'hours', 'especially', 'mostly', 'players', 'people', 'hate', 'math', 'like', 'dice', 'roll', 'hard', 'figure', 'costs', 'currency', 'requires', 'real', 'annoying', 'bit', 'math', 'remember', 'add', 'price', 'tokens', 'stuff', 'bought', 'round', 'check', 'bonus', 'meet', 'round', 'isn', 'hard', 'hawaii', 'delicate', 'balancing', 'act', 'full', 'hard', 'choices', 'intentionally', 'harm', 'opponents', 'grab', 'cheaply', 'leaving', 'higher', 'priced', 'version', 'buying', 'slot', 'means', 'careful', 'eye', 'opponents', 'doing', 'annoying', 'mechanic', 'hate', 'great', 'game', 'couples', 'game', 'looking', 'more', 'challenging', 'stone', 'age', 'lords', 'waterdeep', 'complicated', 'tropical', 'island', 'theme', 'tightly', 'integrated', 'artwork', 'gorgeous', 'colorful', 'pieces', 'durable', 'fun', 'love', 'hawaii', 'instant', 'classic', 'withticket', 'ride', 'dominion', 'stone', 'age', 'board', 'game', 'hawaii', 'most', 'closely', 'resembles', 'lords', 'waterdeep', 'dungeons', 'dragons', 'board', 'game', 'trains', 'board', 'game', 'thurn', 'taxis', 'etc', 'more', 'complicated', 'tropical', 'themed', 'game', 'look', 'bora', 'bora', 'strategy', 'board', 'game', 'ravensburger', 'toys', 'games', 'note', 'try', 'hawaii', 'stone', 'age', 'free', 'online', 'against', 'real', 'opponents', 'board', 'game', 'arena', 'live', 'yucata', 'remote'], 5.0, 4984)\n",
      "(['disappointed', 'broke', 'ripped', 'send', 'free', 'considering', 'bought', 'paid', 'boys', 'son', 'dont', 'ticked'], 2.0, 260)\n",
      "(['experienced', 'carcassonne', 'children', 'families', 'share', 'kids', 'loved', 'involved', 'searching', 'family', 'history', 'middle', 'ages', 'information', 'discovered', 'church', 'records', 'etc', 'carcassonne', 'scene', 'fascinating'], 5.0, 335)\n",
      "(['discovered', 'blythe', 'dolls', 'couple', 'ago', 'exposed', 'gina', 'garan', 'book', 'blythe', 'photos', 'purchased', 'amazon', 'sweet', 'afford', 'remake', 'minis', 'especially', 'kenner', 'originals', 'higher', 'send', 'anyway', 'add', 'cents', 'lovely', 'sets', 'soon', 'try', 'help', 'issues', 'read', 'skimmed', 'review', 'sets', 'goes', 'agree', 'cardboard', 'pic', 'wardrobe', 'box', 'misleading', 'concluded', 'good', 'true', 'verify', 'looked', 'pics', 'sets', 'determined', 'indeed', 'piece', 'cardboard', 'look', 'box', 'person', 'wardrobe', 'includes', 'seventeen', 'pieces', 'doubles', 'weird', 'hasbro', 'didn', 'photo', 'price', 'set', 'bad', 'considering', 'quality', 'stand', 'indeed', 'great', 'blythes', 'kid', 'utilize', 'playtime', 'hand', 'mentioned', 'eyes', 'move', 'real', 'mini', 'blythe', 'correct', 'move', 'full', 'size', 'version', 'bend', 'legs', 'earth', 'shattering', 'somehow', 'makes', 'more', 'charming', 'snort', 'don', 'started', 'lost', 'accessories', 'understand', 'moms', 'whine', 'contained', 'set', 'purchasing', 'hide', 'child', 'deemed', 'fit', 'accessory', 'responsibility', 'like', 'age', 'good', 'stuff', 'strong', 'eyelashes', 'tilt', 'head', 'direction', 'maximum', 'body', 'language', 'removed', 'head', 'repeated', 'exposure', 'cute', 'painted', 'underwear', 'shoes', 'stay', 'experimented', 'stylish', 'wardrobe', 'accessories', 'wear', 'rubber', 'polly', 'pocket', 'clothes', 'mini', 'bratz', 'clothes', 'strawberry', 'shortcake', 'clothing', 'kelly', 'doll', 'clothes', 'pinch', 'experimented', 'fit', 'virtually', 'vehicle', 'dollhouse', 'diy', 'setup', 'girls', 'residences', 'hangs', 'standard', 'dollhouse', 'barbie', 'vacation', 'house', 'sorry', 'mouse', 'yeah', 'cute', 'ignored', 'basking', 'presence', 'blythe', 'doll', 'totally', 'heart', 'attn', 'school', 'blythe', 'dolls', 'found', 'great', 'prices', 'big', 'lots', 'suprisingly', 'toys'], 5.0, 2460)\n",
      "(['loves', 'toy', 'tough', 'time', 'switching', 'forms', 'six', 'expected', 'good', 'toy', 'spend', 'quality', 'time', 'kids'], 4.0, 166)\n",
      "(['pretty', 'low', 'price', 'picked', 'gift', 'daughter', 'absolutely', 'loved', 'traces', 'comes', 'outfits', 'colors', 'brings', 'sleepovers', 'imagination', 'outfits', 'colors', 'great', 'little', 'girls'], 5.0, 335)\n",
      "(['kid', 'doesn', 'love', 'birthday', 'parties', 'great', 'idea', 'cute', 'fun', 'kid', 'love', 'kid'], 5.0, 117)\n",
      "(['niece', 'wish', 'list', 'holidays', 'thrilled', 'receive', 'products', 'pay', 'attention', 'pieces', 'recommended', 'age', 'starting', 'build', 'furniture', 'barbie', 'dollhouse', 'liked', 'versatility', 'table', 'little', 'accessories'], 5.0, 327)\n",
      "(['love', 'idea', 'entirely', 'glowing', 'utensils', 'kit', 'forks', 'spoons', 'pictured', 'obviously', 'glow', 'sticks', 'dollar', 'store', 'discount', 'store', 'less', 'money', 'paper', 'products', 'cheapy', 'main', 'draw', 'glowing', 'utensils', 'never', 'seen', 'cool', 'weglow', 'sets', 'pretty', 'pink', 'set', 'sets', 'include', 'spoons', 'instead', 'forks', 'sets', 'colored', 'fork', 'spoon', 'heads', 'buy', 'forks', 'spoons', 'half', 'price', 'sold', 'sets', 'moment', 'nearly', 'price', 'cute', 'idea', 'kid', 'birthday', 'graduation', 'party', 'value', 'hope'], 4.0, 803)\n",
      "(['shopping', 'spree', 'buying', 'helicopters', 'company', 'pleased', 'gifts', 'theres', 'variation', 'toy', 'kids', 'love', 'helicopter', 'fun', 'fly', 'remote', 'easy', 'glitches', 'flying', 'recommend'], 4.0, 309)\n"
     ]
    }
   ],
   "source": [
    "final_reviews = list(zip(clean_reviews, ratings, lengths))\n",
    "#We look at a Random sample of 10 cleaned data.\n",
    "import random\n",
    "for i in range(10):\n",
    "    r = random.randrange(0, len(final_reviews))\n",
    "    print(final_reviews[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['like', 'item', 'pricing', 'granddaughter', 'mark', 'letters'],\n",
       "  ['love',\n",
       "   'magnet',\n",
       "   'easel',\n",
       "   'great',\n",
       "   'moving',\n",
       "   'wish',\n",
       "   'sort',\n",
       "   'skid',\n",
       "   'pad',\n",
       "   'bottom']],\n",
       " array([5., 4.]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_reviews[:2], ratings[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Case-Study:** Use the list of substantive words extracted from the Review as well as the length of the original Review. Decide how you would like to Derive a feature set to predict the Rating, which is a float (1.0 to 5.0).\n",
    "\n",
    "Remember to split the Data into training, testing and Validation sets.\n",
    "1. Select 10% of the Data for testing and put it away.\n",
    "2. Select 20% of the Data for Validation and 70% for Training.\n",
    "3. Vary the above ratio between Validation and Testing: 30 - 60, 45 - 45, 60 - 30 and Verify the effect if any on the prediction accuracy.\n",
    "\n",
    "\n",
    "Some Possibilities:\n",
    "\n",
    "1. You can use a single feature namely, the difference between number of Positive & Negative words. \n",
    "\n",
    "2. You can also considering predicting the rating based on the above difference and add the length of the Review as two independent Variables.\n",
    "\n",
    "3. You could consider the Positive Words and Negative Words as two independent Variables rather than treating their difference as single independent Variable, giving you more possibilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = pd.read_csv('../Datasets/positive-words.txt').values\n",
    "negative_words=  pd.read_csv('../Datasets/negative-words.txt').values\n",
    "\n",
    "def getwordcount(words):    \n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    neutral_count = 0\n",
    "    \n",
    "    total = []\n",
    "    #print words\n",
    "    \n",
    "    for w in words:        \n",
    "        if w in positive_words:\n",
    "            positive_count += 1\n",
    "        elif w in negative_words:\n",
    "            negative_count += 1\n",
    "        else:\n",
    "            neutral_count += 1\n",
    "            \n",
    "    total.append(positive_count)\n",
    "    total.append(negative_count)\n",
    "#     total.append(neutral_count)\n",
    "#     total.append(labels[count])\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n",
      "26500\n",
      "26600\n",
      "26700\n",
      "26800\n",
      "26900\n",
      "27000\n",
      "27100\n",
      "27200\n",
      "27300\n",
      "27400\n",
      "27500\n",
      "27600\n",
      "27700\n",
      "27800\n",
      "27900\n",
      "28000\n",
      "28100\n",
      "28200\n",
      "28300\n",
      "28400\n",
      "28500\n",
      "28600\n",
      "28700\n",
      "28800\n",
      "28900\n",
      "29000\n",
      "29100\n",
      "29200\n",
      "29300\n",
      "29400\n",
      "29500\n",
      "29600\n",
      "29700\n",
      "29800\n",
      "29900\n",
      "30000\n",
      "30100\n",
      "30200\n",
      "30300\n",
      "30400\n",
      "30500\n",
      "30600\n",
      "30700\n",
      "30800\n",
      "30900\n",
      "31000\n",
      "31100\n",
      "31200\n",
      "31300\n",
      "31400\n",
      "31500\n",
      "31600\n",
      "31700\n",
      "31800\n",
      "31900\n",
      "32000\n",
      "32100\n",
      "32200\n",
      "32300\n",
      "32400\n",
      "32500\n",
      "32600\n",
      "32700\n",
      "32800\n",
      "32900\n",
      "33000\n",
      "33100\n",
      "33200\n",
      "33300\n",
      "33400\n",
      "33500\n",
      "33600\n",
      "33700\n",
      "33800\n",
      "33900\n",
      "34000\n",
      "34100\n",
      "34200\n",
      "34300\n",
      "34400\n",
      "34500\n",
      "34600\n",
      "34700\n",
      "34800\n",
      "34900\n",
      "35000\n",
      "35100\n",
      "35200\n",
      "35300\n",
      "35400\n",
      "35500\n",
      "35600\n",
      "35700\n",
      "35800\n",
      "35900\n",
      "36000\n",
      "36100\n",
      "36200\n",
      "36300\n",
      "36400\n",
      "36500\n",
      "36600\n",
      "36700\n",
      "36800\n",
      "36900\n",
      "37000\n",
      "37100\n",
      "37200\n",
      "37300\n",
      "37400\n",
      "37500\n",
      "37600\n",
      "37700\n",
      "37800\n",
      "37900\n",
      "38000\n",
      "38100\n",
      "38200\n",
      "38300\n",
      "38400\n",
      "38500\n",
      "38600\n",
      "38700\n",
      "38800\n",
      "38900\n",
      "39000\n",
      "39100\n",
      "39200\n",
      "39300\n",
      "39400\n",
      "39500\n",
      "39600\n",
      "39700\n",
      "39800\n",
      "39900\n",
      "40000\n",
      "40100\n",
      "40200\n",
      "40300\n",
      "40400\n",
      "40500\n",
      "40600\n",
      "40700\n",
      "40800\n",
      "40900\n",
      "41000\n",
      "41100\n",
      "41200\n",
      "41300\n",
      "41400\n",
      "41500\n",
      "41600\n",
      "41700\n",
      "41800\n",
      "41900\n",
      "42000\n",
      "42100\n",
      "42200\n",
      "42300\n",
      "42400\n",
      "42500\n",
      "42600\n",
      "42700\n",
      "42800\n",
      "42900\n",
      "43000\n",
      "43100\n",
      "43200\n",
      "43300\n",
      "43400\n",
      "43500\n",
      "43600\n",
      "43700\n",
      "43800\n",
      "43900\n",
      "44000\n",
      "44100\n",
      "44200\n",
      "44300\n",
      "44400\n",
      "44500\n",
      "44600\n",
      "44700\n",
      "44800\n",
      "44900\n",
      "45000\n",
      "45100\n",
      "45200\n",
      "45300\n",
      "45400\n",
      "45500\n",
      "45600\n",
      "45700\n",
      "45800\n",
      "45900\n",
      "46000\n",
      "46100\n",
      "46200\n",
      "46300\n",
      "46400\n",
      "46500\n",
      "46600\n",
      "46700\n",
      "46800\n",
      "46900\n",
      "47000\n",
      "47100\n",
      "47200\n",
      "47300\n",
      "47400\n",
      "47500\n",
      "47600\n",
      "47700\n",
      "47800\n",
      "47900\n",
      "48000\n",
      "48100\n",
      "48200\n",
      "48300\n",
      "48400\n",
      "48500\n",
      "48600\n",
      "48700\n",
      "48800\n",
      "48900\n",
      "49000\n",
      "49100\n",
      "49200\n",
      "49300\n",
      "49400\n",
      "49500\n",
      "49600\n",
      "49700\n",
      "49800\n",
      "49900\n",
      "50000\n",
      "50100\n",
      "50200\n",
      "50300\n",
      "50400\n",
      "50500\n",
      "50600\n",
      "50700\n",
      "50800\n",
      "50900\n",
      "51000\n",
      "51100\n",
      "51200\n",
      "51300\n",
      "51400\n",
      "51500\n",
      "51600\n",
      "51700\n",
      "51800\n",
      "51900\n",
      "52000\n",
      "52100\n",
      "52200\n",
      "52300\n",
      "52400\n",
      "52500\n",
      "52600\n",
      "52700\n",
      "52800\n",
      "52900\n",
      "53000\n",
      "53100\n",
      "53200\n",
      "53300\n",
      "53400\n",
      "53500\n",
      "53600\n",
      "53700\n",
      "53800\n",
      "53900\n",
      "54000\n",
      "54100\n",
      "54200\n",
      "54300\n",
      "54400\n",
      "54500\n",
      "54600\n",
      "54700\n",
      "54800\n",
      "54900\n",
      "55000\n",
      "55100\n",
      "55200\n",
      "55300\n",
      "55400\n",
      "55500\n",
      "55600\n",
      "55700\n",
      "55800\n",
      "55900\n",
      "56000\n",
      "56100\n",
      "56200\n",
      "56300\n",
      "56400\n",
      "56500\n",
      "56600\n",
      "56700\n",
      "56800\n",
      "56900\n",
      "57000\n",
      "57100\n",
      "57200\n",
      "57300\n",
      "57400\n",
      "57500\n",
      "57600\n",
      "57700\n",
      "57800\n",
      "57900\n",
      "58000\n",
      "58100\n",
      "58200\n",
      "58300\n",
      "58400\n",
      "58500\n",
      "58600\n",
      "58700\n",
      "58800\n",
      "58900\n",
      "59000\n",
      "59100\n",
      "59200\n",
      "59300\n",
      "59400\n",
      "59500\n",
      "59600\n",
      "59700\n",
      "59800\n",
      "59900\n",
      "60000\n",
      "60100\n",
      "60200\n",
      "60300\n",
      "60400\n",
      "60500\n",
      "60600\n",
      "60700\n",
      "60800\n",
      "60900\n",
      "61000\n",
      "61100\n",
      "61200\n",
      "61300\n",
      "61400\n",
      "61500\n",
      "61600\n",
      "61700\n",
      "61800\n",
      "61900\n",
      "62000\n",
      "62100\n",
      "62200\n",
      "62300\n",
      "62400\n",
      "62500\n",
      "62600\n",
      "62700\n",
      "62800\n",
      "62900\n",
      "63000\n",
      "63100\n",
      "63200\n",
      "63300\n",
      "63400\n",
      "63500\n",
      "63600\n",
      "63700\n",
      "63800\n",
      "63900\n",
      "64000\n",
      "64100\n",
      "64200\n",
      "64300\n",
      "64400\n",
      "64500\n",
      "64600\n",
      "64700\n",
      "64800\n",
      "64900\n",
      "65000\n",
      "65100\n",
      "65200\n",
      "65300\n",
      "65400\n",
      "65500\n",
      "65600\n",
      "65700\n",
      "65800\n",
      "65900\n",
      "66000\n",
      "66100\n",
      "66200\n",
      "66300\n",
      "66400\n",
      "66500\n",
      "66600\n",
      "66700\n",
      "66800\n",
      "66900\n",
      "67000\n",
      "67100\n",
      "67200\n",
      "67300\n",
      "67400\n",
      "67500\n",
      "67600\n",
      "67700\n",
      "67800\n",
      "67900\n",
      "68000\n",
      "68100\n",
      "68200\n",
      "68300\n",
      "68400\n",
      "68500\n",
      "68600\n",
      "68700\n",
      "68800\n",
      "68900\n",
      "69000\n",
      "69100\n",
      "69200\n",
      "69300\n",
      "69400\n",
      "69500\n",
      "69600\n",
      "69700\n",
      "69800\n",
      "69900\n",
      "70000\n",
      "70100\n",
      "70200\n",
      "70300\n",
      "70400\n",
      "70500\n",
      "70600\n",
      "70700\n",
      "70800\n",
      "70900\n",
      "71000\n",
      "71100\n",
      "71200\n",
      "71300\n",
      "71400\n",
      "71500\n",
      "71600\n",
      "71700\n",
      "71800\n",
      "71900\n",
      "72000\n",
      "72100\n",
      "72200\n",
      "72300\n",
      "72400\n",
      "72500\n",
      "72600\n",
      "72700\n",
      "72800\n",
      "72900\n",
      "73000\n",
      "73100\n",
      "73200\n",
      "73300\n",
      "73400\n",
      "73500\n",
      "73600\n",
      "73700\n",
      "73800\n",
      "73900\n",
      "74000\n",
      "74100\n",
      "74200\n",
      "74300\n",
      "74400\n",
      "74500\n",
      "74600\n",
      "74700\n",
      "74800\n",
      "74900\n",
      "75000\n",
      "75100\n",
      "75200\n",
      "75300\n",
      "75400\n",
      "75500\n",
      "75600\n",
      "75700\n",
      "75800\n",
      "75900\n",
      "76000\n",
      "76100\n",
      "76200\n",
      "76300\n",
      "76400\n",
      "76500\n",
      "76600\n",
      "76700\n",
      "76800\n",
      "76900\n",
      "77000\n",
      "77100\n",
      "77200\n",
      "77300\n",
      "77400\n",
      "77500\n",
      "77600\n",
      "77700\n",
      "77800\n",
      "77900\n",
      "78000\n",
      "78100\n",
      "78200\n",
      "78300\n",
      "78400\n",
      "78500\n",
      "78600\n",
      "78700\n",
      "78800\n",
      "78900\n",
      "79000\n",
      "79100\n",
      "79200\n",
      "79300\n",
      "79400\n",
      "79500\n",
      "79600\n",
      "79700\n",
      "79800\n",
      "79900\n",
      "80000\n",
      "80100\n",
      "80200\n",
      "80300\n",
      "80400\n",
      "80500\n",
      "80600\n",
      "80700\n",
      "80800\n",
      "80900\n",
      "81000\n",
      "81100\n",
      "81200\n",
      "81300\n",
      "81400\n",
      "81500\n",
      "81600\n",
      "81700\n",
      "81800\n",
      "81900\n",
      "82000\n",
      "82100\n",
      "82200\n",
      "82300\n",
      "82400\n",
      "82500\n",
      "82600\n",
      "82700\n",
      "82800\n",
      "82900\n",
      "83000\n",
      "83100\n",
      "83200\n",
      "83300\n",
      "83400\n",
      "83500\n",
      "83600\n",
      "83700\n",
      "83800\n",
      "83900\n",
      "84000\n",
      "84100\n",
      "84200\n",
      "84300\n",
      "84400\n",
      "84500\n",
      "84600\n",
      "84700\n",
      "84800\n",
      "84900\n",
      "85000\n",
      "85100\n",
      "85200\n",
      "85300\n",
      "85400\n",
      "85500\n",
      "85600\n",
      "85700\n",
      "85800\n",
      "85900\n",
      "86000\n",
      "86100\n",
      "86200\n",
      "86300\n",
      "86400\n",
      "86500\n",
      "86600\n",
      "86700\n",
      "86800\n",
      "86900\n",
      "87000\n",
      "87100\n",
      "87200\n",
      "87300\n",
      "87400\n",
      "87500\n",
      "87600\n",
      "87700\n",
      "87800\n",
      "87900\n",
      "88000\n",
      "88100\n",
      "88200\n",
      "88300\n",
      "88400\n",
      "88500\n",
      "88600\n",
      "88700\n",
      "88800\n",
      "88900\n",
      "89000\n",
      "89100\n",
      "89200\n",
      "89300\n",
      "89400\n",
      "89500\n",
      "89600\n",
      "89700\n",
      "89800\n",
      "89900\n",
      "90000\n",
      "90100\n",
      "90200\n",
      "90300\n",
      "90400\n",
      "90500\n",
      "90600\n",
      "90700\n",
      "90800\n",
      "90900\n",
      "91000\n",
      "91100\n",
      "91200\n",
      "91300\n",
      "91400\n",
      "91500\n",
      "91600\n",
      "91700\n",
      "91800\n",
      "91900\n",
      "92000\n",
      "92100\n",
      "92200\n",
      "92300\n",
      "92400\n",
      "92500\n",
      "92600\n",
      "92700\n",
      "92800\n",
      "92900\n",
      "93000\n",
      "93100\n",
      "93200\n",
      "93300\n",
      "93400\n",
      "93500\n",
      "93600\n",
      "93700\n",
      "93800\n",
      "93900\n",
      "94000\n",
      "94100\n",
      "94200\n",
      "94300\n",
      "94400\n",
      "94500\n",
      "94600\n",
      "94700\n",
      "94800\n",
      "94900\n",
      "95000\n",
      "95100\n",
      "95200\n",
      "95300\n",
      "95400\n",
      "95500\n",
      "95600\n",
      "95700\n",
      "95800\n",
      "95900\n",
      "96000\n",
      "96100\n",
      "96200\n",
      "96300\n",
      "96400\n",
      "96500\n",
      "96600\n",
      "96700\n",
      "96800\n",
      "96900\n",
      "97000\n",
      "97100\n",
      "97200\n",
      "97300\n",
      "97400\n",
      "97500\n",
      "97600\n",
      "97700\n",
      "97800\n",
      "97900\n",
      "98000\n",
      "98100\n",
      "98200\n",
      "98300\n",
      "98400\n",
      "98500\n",
      "98600\n",
      "98700\n",
      "98800\n",
      "98900\n",
      "99000\n",
      "99100\n",
      "99200\n",
      "99300\n",
      "99400\n",
      "99500\n",
      "99600\n",
      "99700\n",
      "99800\n",
      "99900\n",
      "100000\n",
      "100100\n",
      "100200\n",
      "100300\n",
      "100400\n",
      "100500\n",
      "100600\n",
      "100700\n",
      "100800\n",
      "100900\n",
      "101000\n",
      "101100\n",
      "101200\n",
      "101300\n",
      "101400\n",
      "101500\n",
      "101600\n",
      "101700\n",
      "101800\n",
      "101900\n",
      "102000\n",
      "102100\n",
      "102200\n",
      "102300\n",
      "102400\n",
      "102500\n",
      "102600\n",
      "102700\n",
      "102800\n",
      "102900\n",
      "103000\n",
      "103100\n",
      "103200\n",
      "103300\n",
      "103400\n",
      "103500\n",
      "103600\n",
      "103700\n",
      "103800\n",
      "103900\n",
      "104000\n",
      "104100\n",
      "104200\n",
      "104300\n",
      "104400\n",
      "104500\n",
      "104600\n",
      "104700\n",
      "104800\n",
      "104900\n",
      "105000\n",
      "105100\n",
      "105200\n",
      "105300\n",
      "105400\n",
      "105500\n",
      "105600\n",
      "105700\n",
      "105800\n",
      "105900\n",
      "106000\n",
      "106100\n",
      "106200\n",
      "106300\n",
      "106400\n",
      "106500\n",
      "106600\n",
      "106700\n",
      "106800\n",
      "106900\n",
      "107000\n",
      "107100\n",
      "107200\n",
      "107300\n",
      "107400\n",
      "107500\n",
      "107600\n",
      "107700\n",
      "107800\n",
      "107900\n",
      "108000\n",
      "108100\n",
      "108200\n",
      "108300\n",
      "108400\n",
      "108500\n",
      "108600\n",
      "108700\n",
      "108800\n",
      "108900\n",
      "109000\n",
      "109100\n",
      "109200\n",
      "109300\n",
      "109400\n",
      "109500\n",
      "109600\n",
      "109700\n",
      "109800\n",
      "109900\n",
      "110000\n",
      "110100\n",
      "110200\n",
      "110300\n",
      "110400\n",
      "110500\n",
      "110600\n",
      "110700\n",
      "110800\n",
      "110900\n",
      "111000\n",
      "111100\n",
      "111200\n",
      "111300\n",
      "111400\n",
      "111500\n",
      "111600\n",
      "111700\n",
      "111800\n",
      "111900\n",
      "112000\n",
      "112100\n",
      "112200\n",
      "112300\n",
      "112400\n",
      "112500\n",
      "112600\n",
      "112700\n",
      "112800\n",
      "112900\n",
      "113000\n",
      "113100\n",
      "113200\n",
      "113300\n",
      "113400\n",
      "113500\n",
      "113600\n",
      "113700\n",
      "113800\n",
      "113900\n",
      "114000\n",
      "114100\n",
      "114200\n",
      "114300\n",
      "114400\n",
      "114500\n",
      "114600\n",
      "114700\n",
      "114800\n",
      "114900\n",
      "115000\n",
      "115100\n",
      "115200\n",
      "115300\n",
      "115400\n",
      "115500\n",
      "115600\n",
      "115700\n",
      "115800\n",
      "115900\n",
      "116000\n",
      "116100\n",
      "116200\n",
      "116300\n",
      "116400\n",
      "116500\n",
      "116600\n",
      "116700\n",
      "116800\n",
      "116900\n",
      "117000\n",
      "117100\n",
      "117200\n",
      "117300\n",
      "117400\n",
      "117500\n",
      "117600\n",
      "117700\n",
      "117800\n",
      "117900\n",
      "118000\n",
      "118100\n",
      "118200\n",
      "118300\n",
      "118400\n",
      "118500\n",
      "118600\n",
      "118700\n",
      "118800\n",
      "118900\n",
      "119000\n",
      "119100\n",
      "119200\n",
      "119300\n",
      "119400\n",
      "119500\n",
      "119600\n",
      "119700\n",
      "119800\n",
      "119900\n",
      "120000\n",
      "120100\n",
      "120200\n",
      "120300\n",
      "120400\n",
      "120500\n",
      "120600\n",
      "120700\n",
      "120800\n",
      "120900\n",
      "121000\n",
      "121100\n",
      "121200\n",
      "121300\n",
      "121400\n",
      "121500\n",
      "121600\n",
      "121700\n",
      "121800\n",
      "121900\n",
      "122000\n",
      "122100\n",
      "122200\n",
      "122300\n",
      "122400\n",
      "122500\n",
      "122600\n",
      "122700\n",
      "122800\n",
      "122900\n",
      "123000\n",
      "123100\n",
      "123200\n",
      "123300\n",
      "123400\n",
      "123500\n",
      "123600\n",
      "123700\n",
      "123800\n",
      "123900\n",
      "124000\n",
      "124100\n",
      "124200\n",
      "124300\n",
      "124400\n",
      "124500\n",
      "124600\n",
      "124700\n",
      "124800\n",
      "124900\n",
      "125000\n",
      "125100\n",
      "125200\n",
      "125300\n",
      "125400\n",
      "125500\n",
      "125600\n",
      "125700\n",
      "125800\n",
      "125900\n",
      "126000\n",
      "126100\n",
      "126200\n",
      "126300\n",
      "126400\n",
      "126500\n",
      "126600\n",
      "126700\n",
      "126800\n",
      "126900\n",
      "127000\n",
      "127100\n",
      "127200\n",
      "127300\n",
      "127400\n",
      "127500\n",
      "127600\n",
      "127700\n",
      "127800\n",
      "127900\n",
      "128000\n",
      "128100\n",
      "128200\n",
      "128300\n",
      "128400\n",
      "128500\n",
      "128600\n",
      "128700\n",
      "128800\n",
      "128900\n",
      "129000\n",
      "129100\n",
      "129200\n",
      "129300\n",
      "129400\n",
      "129500\n",
      "129600\n",
      "129700\n",
      "129800\n",
      "129900\n",
      "130000\n",
      "130100\n",
      "130200\n",
      "130300\n",
      "130400\n",
      "130500\n",
      "130600\n",
      "130700\n",
      "130800\n",
      "130900\n",
      "131000\n",
      "131100\n",
      "131200\n",
      "131300\n",
      "131400\n",
      "131500\n",
      "131600\n",
      "131700\n",
      "131800\n",
      "131900\n",
      "132000\n",
      "132100\n",
      "132200\n",
      "132300\n",
      "132400\n",
      "132500\n",
      "132600\n",
      "132700\n",
      "132800\n",
      "132900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133000\n",
      "133100\n",
      "133200\n",
      "133300\n",
      "133400\n",
      "133500\n",
      "133600\n",
      "133700\n",
      "133800\n",
      "133900\n",
      "134000\n",
      "134100\n",
      "134200\n",
      "134300\n",
      "134400\n",
      "134500\n",
      "134600\n",
      "134700\n",
      "134800\n",
      "134900\n",
      "135000\n",
      "135100\n",
      "135200\n",
      "135300\n",
      "135400\n",
      "135500\n",
      "135600\n",
      "135700\n",
      "135800\n",
      "135900\n",
      "136000\n",
      "136100\n",
      "136200\n",
      "136300\n",
      "136400\n",
      "136500\n",
      "136600\n",
      "136700\n",
      "136800\n",
      "136900\n",
      "137000\n",
      "137100\n",
      "137200\n",
      "137300\n",
      "137400\n",
      "137500\n",
      "137600\n",
      "137700\n",
      "137800\n",
      "137900\n",
      "138000\n",
      "138100\n",
      "138200\n",
      "138300\n",
      "138400\n",
      "138500\n",
      "138600\n",
      "138700\n",
      "138800\n",
      "138900\n",
      "139000\n",
      "139100\n",
      "139200\n",
      "139300\n",
      "139400\n",
      "139500\n",
      "139600\n",
      "139700\n",
      "139800\n",
      "139900\n",
      "140000\n",
      "140100\n",
      "140200\n",
      "140300\n",
      "140400\n",
      "140500\n",
      "140600\n",
      "140700\n",
      "140800\n",
      "140900\n",
      "141000\n",
      "141100\n",
      "141200\n",
      "141300\n",
      "141400\n",
      "141500\n",
      "141600\n",
      "141700\n",
      "141800\n",
      "141900\n",
      "142000\n",
      "142100\n",
      "142200\n",
      "142300\n",
      "142400\n",
      "142500\n",
      "142600\n",
      "142700\n",
      "142800\n",
      "142900\n",
      "143000\n",
      "143100\n",
      "143200\n",
      "143300\n",
      "143400\n",
      "143500\n",
      "143600\n",
      "143700\n",
      "143800\n",
      "143900\n",
      "144000\n",
      "144100\n",
      "144200\n",
      "144300\n",
      "144400\n",
      "144500\n",
      "144600\n",
      "144700\n",
      "144800\n",
      "144900\n",
      "145000\n",
      "145100\n",
      "145200\n",
      "145300\n",
      "145400\n",
      "145500\n",
      "145600\n",
      "145700\n",
      "145800\n",
      "145900\n",
      "146000\n",
      "146100\n",
      "146200\n",
      "146300\n",
      "146400\n",
      "146500\n",
      "146600\n",
      "146700\n",
      "146800\n",
      "146900\n",
      "147000\n",
      "147100\n",
      "147200\n",
      "147300\n",
      "147400\n",
      "147500\n",
      "147600\n",
      "147700\n",
      "147800\n",
      "147900\n",
      "148000\n",
      "148100\n",
      "148200\n",
      "148300\n",
      "148400\n",
      "148500\n",
      "148600\n",
      "148700\n",
      "148800\n",
      "148900\n",
      "149000\n",
      "149100\n",
      "149200\n",
      "149300\n",
      "149400\n",
      "149500\n",
      "149600\n",
      "149700\n",
      "149800\n",
      "149900\n",
      "150000\n",
      "150100\n",
      "150200\n",
      "150300\n",
      "150400\n",
      "150500\n",
      "150600\n",
      "150700\n",
      "150800\n",
      "150900\n",
      "151000\n",
      "151100\n",
      "151200\n",
      "151300\n",
      "151400\n",
      "151500\n",
      "151600\n",
      "151700\n",
      "151800\n",
      "151900\n",
      "152000\n",
      "152100\n",
      "152200\n",
      "152300\n",
      "152400\n",
      "152500\n",
      "152600\n",
      "152700\n",
      "152800\n",
      "152900\n",
      "153000\n",
      "153100\n",
      "153200\n",
      "153300\n",
      "153400\n",
      "153500\n",
      "153600\n",
      "153700\n",
      "153800\n",
      "153900\n",
      "154000\n",
      "154100\n",
      "154200\n",
      "154300\n",
      "154400\n",
      "154500\n",
      "154600\n",
      "154700\n",
      "154800\n",
      "154900\n",
      "155000\n",
      "155100\n",
      "155200\n",
      "155300\n",
      "155400\n",
      "155500\n",
      "155600\n",
      "155700\n",
      "155800\n",
      "155900\n",
      "156000\n",
      "156100\n",
      "156200\n",
      "156300\n",
      "156400\n",
      "156500\n",
      "156600\n",
      "156700\n",
      "156800\n",
      "156900\n",
      "157000\n",
      "157100\n",
      "157200\n",
      "157300\n",
      "157400\n",
      "157500\n",
      "157600\n",
      "157700\n",
      "157800\n",
      "157900\n",
      "158000\n",
      "158100\n",
      "158200\n",
      "158300\n",
      "158400\n",
      "158500\n",
      "158600\n",
      "158700\n",
      "158800\n",
      "158900\n",
      "159000\n",
      "159100\n",
      "159200\n",
      "159300\n",
      "159400\n",
      "159500\n",
      "159600\n",
      "159700\n",
      "159800\n",
      "159900\n",
      "160000\n",
      "160100\n",
      "160200\n",
      "160300\n",
      "160400\n",
      "160500\n",
      "160600\n",
      "160700\n",
      "160800\n",
      "160900\n",
      "161000\n",
      "161100\n",
      "161200\n",
      "161300\n",
      "161400\n",
      "161500\n",
      "161600\n",
      "161700\n",
      "161800\n",
      "161900\n",
      "162000\n",
      "162100\n",
      "162200\n",
      "162300\n",
      "162400\n",
      "162500\n",
      "162600\n",
      "162700\n",
      "162800\n",
      "162900\n",
      "163000\n",
      "163100\n",
      "163200\n",
      "163300\n",
      "163400\n",
      "163500\n",
      "163600\n",
      "163700\n",
      "163800\n",
      "163900\n",
      "164000\n",
      "164100\n",
      "164200\n",
      "164300\n",
      "164400\n",
      "164500\n",
      "164600\n",
      "164700\n",
      "164800\n",
      "164900\n",
      "165000\n",
      "165100\n",
      "165200\n",
      "165300\n",
      "165400\n",
      "165500\n",
      "165600\n",
      "165700\n",
      "165800\n",
      "165900\n",
      "166000\n",
      "166100\n",
      "166200\n",
      "166300\n",
      "166400\n",
      "166500\n",
      "166600\n",
      "166700\n",
      "166800\n",
      "166900\n",
      "167000\n",
      "167100\n",
      "167200\n",
      "167300\n",
      "167400\n",
      "167500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 0, 100], [2, 0, 121]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_feature_counts = []\n",
    "for i in range(len(clean_reviews)):\n",
    "    if i % 100 == 0: print(i)\n",
    "\n",
    "    # Derive feature vector with - +ve words, -ve words, len(reviews)\n",
    "    reviews_feature_counts.append(getwordcount(clean_reviews[i]) + [lengths[i]]) \n",
    "\n",
    "reviews_feature_counts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, TRAIN_TEST_RATIO = 0.8):\n",
    "    picker = list(range(len(data)))\n",
    "    random.shuffle(picker)       \n",
    "\n",
    "    FEATURE_COLUMNS = list(range(2)) # 0, 1\n",
    "    ALL_COLUMNS = FEATURE_COLUMNS + [2]\n",
    "\n",
    "    ## Raw Data ###\n",
    "#     data = data.reindex(columns = ALL_COLUMNS)\n",
    "    trainMax = int(len(picker) * TRAIN_TEST_RATIO)\n",
    "    train = []\n",
    "    test = []\n",
    "    for pick in picker[:trainMax]:\n",
    "        train.append(list(data.values[pick]))         ### select 80% of data to be used as training set\n",
    "    for pick in picker[trainMax:]:\n",
    "        test.append(list(data.values[pick])) \n",
    "        \n",
    "    train = pd.DataFrame(train)\n",
    "    test = pd.DataFrame(test)\n",
    "#     print(train.head())\n",
    "    return train.iloc[:, 0:-1], train.iloc[:, -1:], test.iloc[:, 0:-1], test.iloc[:, -1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "type(reviews_feature_counts), type(ratings)\n",
    "\n",
    "data = []\n",
    "# np.hstack((features, ratings))\n",
    "for i, r in enumerate(reviews_feature_counts):\n",
    "     data.append(r + [ratings[i]])\n",
    "data = pd.DataFrame(data)\n",
    "train_X, train_y, test_X, test_y = train_test_split(data, TRAIN_TEST_RATIO=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11.,   4., 954.],\n",
       "       [  7.,   0., 625.],\n",
       "       [  8.,   0., 555.],\n",
       "       ...,\n",
       "       [  5.,   1., 443.],\n",
       "       [  3.,   0., 117.],\n",
       "       [  5.,   1., 113.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_X, train_y, test_X, test_y\n",
    "# np.where(train_y[3] == 5.0, 1, 0)\n",
    "# train_y[3]\n",
    "train_X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now that we have divided our data into training and testing set, we can apply KNN to easily deduce\n",
    "### the predicted ratings (multi-class classification). But it is not very scalable for this huge dataset. \n",
    "### So, I will be applying Linear Classification to predict the ratings.\n",
    "\n",
    "from classifiers import *\n",
    "\n",
    "def get_linear_accuracies(train_X, train_y, test_X, test_y):\n",
    "    accuracies = np.zeros((5, 5))\n",
    "    # list(test_y.columns.values)\n",
    "    # test_y.loc[test_y[3]==4.0]\n",
    "    for i, r in enumerate([1.0, 2.0, 3.0, 4.0, 5.0]):\n",
    "        train_y_binary = np.where(train_y[3] == r, 1, 0)\n",
    "        for j, t_r in enumerate([1.0, 2.0, 3.0, 4.0, 5.0]):    \n",
    "            test_y_binary = np.where(test_y[3] == t_r, 1, 0)\n",
    "#             print(r, t_r, test_y_binary[:10])\n",
    "            accuracies[i][j] = predict_and_find_accuracy('linear', train_X.values, train_y_binary, test_X.values, test_y_binary)\n",
    "\n",
    "    return accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97214041, 0.96217066, 0.90344663, 0.77941176, 0.38241264],\n",
       "       [0.97214041, 0.96125527, 0.90237204, 0.77817798, 0.38133806],\n",
       "       [0.97204091, 0.96230996, 0.90358593, 0.77955106, 0.38251214],\n",
       "       [0.8449017 , 0.84189684, 0.80613707, 0.72759293, 0.37644273],\n",
       "       [0.10325957, 0.11032397, 0.15388442, 0.26450689, 0.65744647]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X, train_y, test_X, test_y = train_test_split(data, TRAIN_TEST_RATIO=.7)\n",
    "get_linear_accuracies(train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97229917, 0.9611472 , 0.90131579, 0.77502627, 0.38677285],\n",
       "       [0.96630528, 0.95288471, 0.89355478, 0.76585634, 0.37898796],\n",
       "       [0.0876994 , 0.09777677, 0.15526793, 0.26844732, 0.57144904],\n",
       "       [0.97225141, 0.96203076, 0.90198443, 0.77600535, 0.38772805],\n",
       "       [0.02774859, 0.03796924, 0.09801557, 0.22399465, 0.61227195]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X, train_y, test_X, test_y = train_test_split(data, TRAIN_TEST_RATIO=.5)\n",
    "get_linear_accuracies(train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97301654, 0.96370366, 0.90245358, 0.77619247, 0.38463375],\n",
       "       [0.96871829, 0.95618172, 0.89600621, 0.76819294, 0.37723121],\n",
       "       [0.97301654, 0.96370366, 0.90245358, 0.77619247, 0.38463375],\n",
       "       [0.74759716, 0.74795535, 0.72431497, 0.68437705, 0.38188765],\n",
       "       [0.03766939, 0.04722106, 0.10763537, 0.23509044, 0.62450003]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X, train_y, test_X, test_y = train_test_split(data, TRAIN_TEST_RATIO=.9)\n",
    "get_linear_accuracies(train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5303020017024405"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict_and_find_accuracy('kNN', train_X.values, train_y.values, test_X.values, test_y.values, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Apply logistic regression for each rating to create 5 classifiers.\n",
    "    Predict based on the best probability\n",
    "\"\"\"\n",
    "\n",
    "def logf(a, b, X):\n",
    "    return 1.0 / (1.0 + np.exp(-a * X - b))\n",
    "\n",
    "def dlogf(a, b, X):\n",
    "    return logf(a, b, X) * (1 - logf(a, b, X))\n",
    "##\n",
    "## The derivative of the logistic function is f * (1 - f)\n",
    "##\n",
    "def one_step(X, y, a, b, eta):\n",
    "    ycalc = logf(a, b, X)\n",
    "    delta_a = sum((y - ycalc) * ycalc * (1 - ycalc) * X)\n",
    "    delta_b = sum((y - ycalc) * ycalc * (1 - ycalc))\n",
    "    a = a + delta_a * eta\n",
    "    b = b + delta_b * eta\n",
    "    error = sum((y - ycalc)**2)\n",
    "    return a, b, error\n",
    "\n",
    "def train_logistic(train_X, train_y, a=1, b=1, eta=.001, iterations=100000):\n",
    "    for times in range(iterations):\n",
    "        a, b, error = one_step(train_X, train_y, a, b, eta)\n",
    "        if times % 1000 == 0:\n",
    "            eta = max(0.00001, eta * 0.99)\n",
    "            if times % 5000 == 0:\n",
    "                print(a, b, error)\n",
    "                \n",
    "    return a, b, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_multi_classifier(train_X, train_y, iterations=100000):\n",
    "    # Store a, b and error for each ratings in this\n",
    "    accuracies = []\n",
    "\n",
    "    for i, r in enumerate([1.0, 2.0, 3.0, 4.0, 5.0]):\n",
    "        train_y_binary = np.where(train_y[3] == r, 1, 0)\n",
    "        accuracies.append(train_logistic(train_X, train_y_binary, iterations=iterations))\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "def get_logistic_predictions(reg, my_test_X):\n",
    "    m = np.zeros(len(test_X))\n",
    "    result = np.zeros(len(test_X))\n",
    "    for i, r in enumerate(reg):\n",
    "#         print('rating = %d, args = %s' % (i+1, r))\n",
    "        result = np.vstack((result, logf(r[0], r[1], my_test_X).values))\n",
    "\n",
    "    return np.argmax(result, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.233857642544481 -4.864547058536086 124334.77417816465\n",
      "-0.5810938434690696 -6.082953298031106 4188.104919498645\n",
      "-0.5326519863067984 -3.096177055587078 3999.2814336671954\n",
      "-0.5185367811398574 -3.057851397052744 3984.8456006951255\n",
      "-0.5021622554950662 -3.019568072888305 3970.0487587564417\n",
      "-0.48297313862488045 -2.981246169089002 3954.933123872618\n",
      "-0.4602449678028369 -2.942915618755123 3939.533714465045\n",
      "-0.43285169734729256 -2.9047963485957706 3923.870870338413\n",
      "-0.39822806821335477 -2.8673372577218745 3907.9107210548436\n",
      "-0.34391741413489696 -2.8317793589444844 3891.4280041825355\n",
      "-0.3079618518800988 -2.826089938744539 3887.735106060809\n",
      "-0.30796185187996156 -2.826089938744547 3887.735106060789\n",
      "-0.3079618518799614 -2.8260899387445466 3887.735106060789\n",
      "-0.3079618518799616 -2.8260899387445475 3887.735106060789\n",
      "-0.3079618518799615 -2.826089938744547 3887.735106060789\n",
      "-0.30796185187996145 -2.8260899387445466 3887.735106060789\n",
      "-0.3079618518799616 -2.8260899387445475 3887.735106060789\n",
      "-0.3079618518799615 -2.826089938744547 3887.735106060789\n",
      "-0.30796185187996156 -2.826089938744547 3887.735106060789\n",
      "-0.30796185187996156 -2.826089938744547 3887.735106060789\n",
      "-4.096583025431897 -4.77781652597536 122660.22876489772\n",
      "-17.961198601806014 -2.523123359363257 15524.867022667262\n",
      "-17.9559831004287 -2.52312345868382 15524.867010880715\n",
      "-17.95099784751094 -2.523123554105152 15524.866999545247\n",
      "-17.94623375374145 -2.523123645738948 15524.866988671905\n",
      "-17.941681994645297 -2.523123733697407 15524.866978226617\n",
      "-17.93733401127818 -2.523123818092817 15524.866968215505\n",
      "-17.93318150997881 -2.5231238990371865 15524.866958600054\n",
      "-17.929216461309153 -2.5231239766419082 15524.866949390347\n",
      "-17.925431098238874 -2.5231240510174575 15524.866940554904\n",
      "-17.921817913650692 -2.523124122273129 15524.86693210597\n",
      "-17.918369657242497 -2.5231241905167874 15524.866924002452\n",
      "-17.915079331892127 -2.5231242558546363 15524.866916247756\n",
      "-17.911940189550986 -2.523124318391103 15524.866908821687\n",
      "-17.908945726728415 -2.52312437822855 15524.866901713689\n",
      "-17.906089679625122 -2.5231244354672837 15524.866894922736\n",
      "-17.903366018955765 -2.5231244902053334 15524.866888427308\n",
      "-17.900768944530387 -2.5231245425383944 15524.866882212915\n",
      "-17.898292879625792 -2.5231245925597534 15524.866876270971\n",
      "-17.895932465193866 -2.523124640360189 15524.866870599366\n",
      "-3.9509518114761564 -4.111695875261408 115795.57482805736\n",
      "-10.226129940375818 -1.6223512886559126 22632.00105888675\n",
      "-12.074246519465433 -1.6190727289773854 22633.621977134288\n",
      "-10.980764663046193 -1.6202873199183172 22633.020620206673\n",
      "-13.320710290663795 -1.6186373069671731 22633.837787961227\n",
      "-13.130994317019887 -1.6186740021449337 22633.819595299003\n",
      "-12.90950657891028 -1.618726654264196 22633.793492878507\n",
      "-12.640590769904462 -1.61880843339487 22633.752953880055\n",
      "-12.293233643198851 -1.6189525059468692 22633.68154464319\n",
      "-11.790656177938175 -1.6192732220133255 22633.52262565167\n",
      "-10.81971298243397 -1.6206069232404374 22632.86239502172\n",
      "-0.3570202750256381 -7.3323985748202 14695.881374993689\n",
      "-0.19403434067707936 -4.57704524963214 14489.03529124589\n",
      "-4.707190945825698 -5.525313559292068 18962.307906660895\n",
      "-3.745392192656846 -3.985263454813898 15241.025007579228\n",
      "-7.174133615588651 -1.705993683487406 22592.109546700438\n",
      "-0.12394208711779736 -2.8596168043251518 14042.429303201556\n",
      "-0.23777729293530145 -4.30466115879998 14428.566244245594\n",
      "-0.34491117321839937 -4.211261339255238 14440.178854754855\n",
      "-0.2952691724916455 -5.491677271285955 14615.482107003083\n",
      "-3.202544003953361 -3.4540679445784157 98278.68311864688\n",
      "-0.39695058766399766 -7.159475240998423 33654.08328242084\n",
      "-6.654466171184527 -7.952078686521689 37855.087844082074\n",
      "-6.517583488470388 -7.837171474862507 37851.46599698068\n",
      "-6.058815702393122 -7.4667238576012 37834.80281583556\n",
      "-0.3010651411975088 -6.797365975194863 33638.04781146282\n",
      "-1.29076167468613 -6.706339988007877 34429.298510664645\n",
      "-6.28574282077362 -7.6411135987671575 37843.91172520669\n",
      "-0.3399392463270877 -6.554861754209969 33628.56556990097\n",
      "0.05236833465170727 -6.817545961446636 33610.851122385575\n",
      "-0.21001554353684318 -6.801755151457431 33634.38162766934\n",
      "-0.4255890030065659 -6.36255823796209 33630.55927020767\n",
      "-0.4199513356370233 -6.967251376058726 33649.81803078178\n",
      "-0.6857669866855685 -5.905817715949653 33781.31758546626\n",
      "-0.5438179045634531 -6.231234572578176 33674.933340143005\n",
      "-0.15323110029728304 -5.685664133332531 33539.92298933016\n",
      "-0.8402191417356624 -5.985640651526505 33892.22283423127\n",
      "0.008667472732212439 -5.570551959558426 33489.632214044206\n",
      "-0.07565493097394296 -6.0685604560312 33569.74354610983\n",
      "-0.33010983524701887 -5.396760019971801 33527.71484304185\n",
      "-0.612167531023943 -0.9421225955916013 45869.661204568496\n",
      "36.286962994683186 -0.07916078886793369 48778.594592441266\n",
      "36.286962994683186 -0.07916078886793358 48778.594592441266\n",
      "36.286962994683186 -0.07916078886793369 48778.594592441266\n",
      "36.286962994683186 -0.07916078886793372 48778.594592441266\n",
      "36.286962994683186 -0.07916078886793358 48778.594592441266\n",
      "36.286962994683186 -0.07916078886793361 48778.594592441266\n",
      "36.286962994683186 -0.07916078886793364 48778.594592441266\n",
      "36.286962994683186 -0.07916078886793375 48778.594592441266\n",
      "36.286962994683186 -0.07916078886793368 48778.594592441266\n",
      "36.286962994683186 -0.07916078886793362 48778.594592441266\n",
      "36.286962994683186 -0.07916078886793367 48778.594592441266\n",
      "36.286962994683186 -0.07916078886793375 48778.594592441266\n",
      "36.286962994683186 -0.07916078886793369 48778.594592441266\n",
      "36.286962994683186 -0.07916078886793369 48778.594592441266\n",
      "36.286962994683186 -0.07916078886793369 48778.594592441266\n",
      "36.286962994683186 -0.07916078886793376 48778.594592441266\n",
      "36.286962994683186 -0.0791607888679337 48778.594592441266\n",
      "36.286962994683186 -0.07916078886793375 48778.594592441266\n",
      "36.286962994683186 -0.07916078886793375 48778.594592441266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 5, 2, ..., 5, 5, 5])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can use a single feature namely, the difference between number of Positive & Negative words.\n",
    "# Train with my_X\n",
    "my_X = train_X[0]-train_X[1]\n",
    "my_test_X = test_X[0] - test_X[1]\n",
    "\n",
    "reg = logistic_multi_classifier(my_X, train_y, iterations=100000) ### TEMP TEMP TEMP DEBUGGING - set low iteration\n",
    "predictions = get_logistic_predictions(reg, my_test_X)\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
