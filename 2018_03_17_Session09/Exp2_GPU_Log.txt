
Logs   Refresh logs 
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:47:54] [stdout]: [32m[I 10:17:54.544 NotebookApp](B[m Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:47:56] [stdout]: [32m[I 10:17:56.192 NotebookApp](B[m JupyterLab beta preview extension loaded from /usr/local/lib/python3.5/dist-packages/jupyterlab
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:47:56] [stdout]: [32m[I 10:17:56.192 NotebookApp](B[m JupyterLab application directory is /usr/local/share/jupyter/lab
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:47:56] [stdout]: [32m[I 10:17:56.265 NotebookApp](B[m Serving notebooks from local directory: /notebooks
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:47:56] [stdout]: [32m[I 10:17:56.265 NotebookApp](B[m 0 active kernels
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:47:56] [stdout]: [32m[I 10:17:56.265 NotebookApp](B[m The Jupyter Notebook is running at:
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:47:56] [stdout]: [32m[I 10:17:56.265 NotebookApp](B[m http://0.0.0.0:8888/?token=cde3e3c5d8333582fa09010f9516e06695f732791476030a
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:47:56] [stdout]: [32m[I 10:17:56.265 NotebookApp](B[m Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:47:56] [stdout]: [33m[W 10:17:56.266 NotebookApp](B[m No web browser found: could not locate runnable browser.
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:47:56] [stdout]: [C 10:17:56.266 NotebookApp] 
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:47:56] [stdout]: 
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:47:56] [stdout]: Copy/paste this URL into your browser when you connect for the first time,
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:47:56] [stdout]: to login with a token:
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:47:56] [stdout]: http://0.0.0.0:8888/?token=cde3e3c5d8333582fa09010f9516e06695f732791476030a
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:47:57] [stdout]: Collecting seaborn
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:47:57] [stdout]: Downloading seaborn-0.8.1.tar.gz (178kB)
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:47:57] [stdout]: [?25l
[K 5% |# | 10kB 9.3MB/s eta 0:00:01
[K 11% |### | 20kB 4.1MB/s eta 0:00:01
[K 17% |##### | 30kB 3.9MB/s eta 0:00:01
[K 22% |####### | 40kB 3.7MB/s eta 0:00:01
[K 28% |######### | 51kB 3.8MB/s eta 0:00:01
[K 34% |########## | 61kB 4.5MB/s eta 0:00:01
[K 40% |############ | 71kB 4.6MB/s eta 0:00:01
[K 45% |############## | 81kB 5.2MB/s eta 0:00:01
[K 51% |################ | 92kB 5.3MB/s eta 0:00:01
[K 57% |################## | 102kB 5.5MB/s eta 0:00:01
[K 62% |#################### | 112kB 5.8MB/s eta 0:00:01
[K 68% |##################### | 122kB 7.0MB/s eta 0:00:01
[K 74% |####################### | 133kB 7.6MB/s eta 0:00:01
[K 80% |######################### | 143kB 9.8MB/s eta 0:00:01
[K 85% |########################### | 153kB 11.1MB/s eta 0:00:01
[K 91% |############################# | 163kB 11.1MB/s eta 0:00:01
[K 97% |############################### | 174kB 12.8MB/s eta 0:00:01
[K 100% |################################| 184kB 5.1MB/s 
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:48:02] [stdout]: [?25hBuilding wheels for collected packages: seaborn
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:48:03] [stdout]: Running setup.py bdist_wheel for seaborn ... [?25l- done
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:48:03] [stdout]: [?25h Stored in directory: /root/.cache/pip/wheels/29/af/4b/ac6b04ec3e2da1a450e74c6a0e86ade83807b4aaf40466ecda
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:48:03] [stdout]: Successfully built seaborn
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:48:03] [stdout]: Installing collected packages: seaborn
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:48:03] [stdout]: Successfully installed seaborn-0.8.1
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:48:28] [stdout]: Using PyTorch version: 0.3.1 CUDA: True
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:48:29] [stdout]: Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:48:30] [stdout]: Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:48:30] [stdout]: Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:48:31] [stdout]: Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:48:31] [stdout]: Processing...
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:48:58] [stdout]: Done!
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:14] [stdout]: X_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:14] [stdout]: y_train: torch.Size([32]) type: torch.LongTensor
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:15] [stdout]: Net(
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:15] [stdout]: (fc1): Linear(in_features=784, out_features=50, bias=True)
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:15] [stdout]: (fc1_drop): Dropout(p=0.2)
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:15] [stdout]: (fc2): Linear(in_features=50, out_features=50, bias=True)
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:15] [stdout]: (fc2_drop): Dropout(p=0.2)
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:15] [stdout]: (fc3): Linear(in_features=50, out_features=10, bias=True)
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:15] [stdout]: )
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:15] [stdout]: Sat Mar 17 10:19:15 UTC 2018
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:19] [stdout]: Lab09-Experiment2-GPU.py:118: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:19] [stdout]: return F.log_softmax(self.fc3(x))
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:20] [stdout]: Train Epoch: 1 [0/60000 (0%)]	Loss: 2.341520
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:20] [stdout]: Train Epoch: 1 [3200/60000 (5%)]	Loss: 1.623827
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:21] [stdout]: Train Epoch: 1 [6400/60000 (11%)]	Loss: 0.811118
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:21] [stdout]: Train Epoch: 1 [9600/60000 (16%)]	Loss: 0.800901
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:22] [stdout]: Train Epoch: 1 [12800/60000 (21%)]	Loss: 0.625071
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:22] [stdout]: Train Epoch: 1 [16000/60000 (27%)]	Loss: 0.573182
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:23] [stdout]: Train Epoch: 1 [19200/60000 (32%)]	Loss: 0.435778
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:23] [stdout]: Train Epoch: 1 [22400/60000 (37%)]	Loss: 0.511635
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:24] [stdout]: Train Epoch: 1 [25600/60000 (43%)]	Loss: 0.385146
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:24] [stdout]: Train Epoch: 1 [28800/60000 (48%)]	Loss: 0.255143
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:25] [stdout]: Train Epoch: 1 [32000/60000 (53%)]	Loss: 0.270897
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:25] [stdout]: Train Epoch: 1 [35200/60000 (59%)]	Loss: 0.298059
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:26] [stdout]: Train Epoch: 1 [38400/60000 (64%)]	Loss: 0.468825
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:26] [stdout]: Train Epoch: 1 [41600/60000 (69%)]	Loss: 0.455703
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:26] [stdout]: Train Epoch: 1 [44800/60000 (75%)]	Loss: 0.229964
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:27] [stdout]: Train Epoch: 1 [48000/60000 (80%)]	Loss: 0.281298
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:27] [stdout]: Train Epoch: 1 [51200/60000 (85%)]	Loss: 0.304514
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:28] [stdout]: Train Epoch: 1 [54400/60000 (91%)]	Loss: 0.286128
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:28] [stdout]: Train Epoch: 1 [57600/60000 (96%)]	Loss: 0.122344
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:30] [stdout]: 
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:30] [stdout]: Test set: Average loss: 0.2431, Accuracy: 9248/10000 (92%)
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:30] [stdout]: 
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:30] [stdout]: Train Epoch: 2 [0/60000 (0%)]	Loss: 0.572890
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:31] [stdout]: Train Epoch: 2 [3200/60000 (5%)]	Loss: 0.106322
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:31] [stdout]: Train Epoch: 2 [6400/60000 (11%)]	Loss: 0.163328
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:32] [stdout]: Train Epoch: 2 [9600/60000 (16%)]	Loss: 0.291695
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:32] [stdout]: Train Epoch: 2 [12800/60000 (21%)]	Loss: 0.173577
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:33] [stdout]: Train Epoch: 2 [16000/60000 (27%)]	Loss: 0.316361
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:33] [stdout]: Train Epoch: 2 [19200/60000 (32%)]	Loss: 0.361473
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:34] [stdout]: Train Epoch: 2 [22400/60000 (37%)]	Loss: 0.244519
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:34] [stdout]: Train Epoch: 2 [25600/60000 (43%)]	Loss: 0.536234
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:35] [stdout]: Train Epoch: 2 [28800/60000 (48%)]	Loss: 0.506604
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:35] [stdout]: Train Epoch: 2 [32000/60000 (53%)]	Loss: 0.161084
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:36] [stdout]: Train Epoch: 2 [35200/60000 (59%)]	Loss: 0.336926
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:36] [stdout]: Train Epoch: 2 [38400/60000 (64%)]	Loss: 0.254280
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:37] [stdout]: Train Epoch: 2 [41600/60000 (69%)]	Loss: 0.241769
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:37] [stdout]: Train Epoch: 2 [44800/60000 (75%)]	Loss: 0.127520
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:38] [stdout]: Train Epoch: 2 [48000/60000 (80%)]	Loss: 0.555389
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:38] [stdout]: Train Epoch: 2 [51200/60000 (85%)]	Loss: 0.215868
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:39] [stdout]: Train Epoch: 2 [54400/60000 (91%)]	Loss: 0.478298
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:39] [stdout]: Train Epoch: 2 [57600/60000 (96%)]	Loss: 0.279411
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:41] [stdout]: 
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:41] [stdout]: Test set: Average loss: 0.1800, Accuracy: 9445/10000 (94%)
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:41] [stdout]: 
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:41] [stdout]: Train Epoch: 3 [0/60000 (0%)]	Loss: 0.345576
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:42] [stdout]: Train Epoch: 3 [3200/60000 (5%)]	Loss: 0.127236
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:42] [stdout]: Train Epoch: 3 [6400/60000 (11%)]	Loss: 0.728258
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:42] [stdout]: Train Epoch: 3 [9600/60000 (16%)]	Loss: 0.226356
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:43] [stdout]: Train Epoch: 3 [12800/60000 (21%)]	Loss: 0.273407
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:43] [stdout]: Train Epoch: 3 [16000/60000 (27%)]	Loss: 0.332566
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:44] [stdout]: Train Epoch: 3 [19200/60000 (32%)]	Loss: 0.357374
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:44] [stdout]: Train Epoch: 3 [22400/60000 (37%)]	Loss: 0.324739
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:45] [stdout]: Train Epoch: 3 [25600/60000 (43%)]	Loss: 0.163474
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:45] [stdout]: Train Epoch: 3 [28800/60000 (48%)]	Loss: 0.208299
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:46] [stdout]: Train Epoch: 3 [32000/60000 (53%)]	Loss: 0.408111
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:46] [stdout]: Train Epoch: 3 [35200/60000 (59%)]	Loss: 0.096743
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:47] [stdout]: Train Epoch: 3 [38400/60000 (64%)]	Loss: 0.240091
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:47] [stdout]: Train Epoch: 3 [41600/60000 (69%)]	Loss: 0.342919
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:48] [stdout]: Train Epoch: 3 [44800/60000 (75%)]	Loss: 0.102101
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:48] [stdout]: Train Epoch: 3 [48000/60000 (80%)]	Loss: 0.149658
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:49] [stdout]: Train Epoch: 3 [51200/60000 (85%)]	Loss: 0.107580
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:49] [stdout]: Train Epoch: 3 [54400/60000 (91%)]	Loss: 0.058809
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:50] [stdout]: Train Epoch: 3 [57600/60000 (96%)]	Loss: 0.193505
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:52] [stdout]: 
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:52] [stdout]: Test set: Average loss: 0.1506, Accuracy: 9550/10000 (96%)
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:52] [stdout]: 
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:52] [stdout]: Train Epoch: 4 [0/60000 (0%)]	Loss: 0.258695
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:52] [stdout]: Train Epoch: 4 [3200/60000 (5%)]	Loss: 0.147279
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:53] [stdout]: Train Epoch: 4 [6400/60000 (11%)]	Loss: 0.291885
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:53] [stdout]: Train Epoch: 4 [9600/60000 (16%)]	Loss: 0.390580
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:54] [stdout]: Train Epoch: 4 [12800/60000 (21%)]	Loss: 0.177538
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:54] [stdout]: Train Epoch: 4 [16000/60000 (27%)]	Loss: 0.087649
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:55] [stdout]: Train Epoch: 4 [19200/60000 (32%)]	Loss: 0.325012
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:55] [stdout]: Train Epoch: 4 [22400/60000 (37%)]	Loss: 0.094272
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:56] [stdout]: Train Epoch: 4 [25600/60000 (43%)]	Loss: 0.096295
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:56] [stdout]: Train Epoch: 4 [28800/60000 (48%)]	Loss: 0.056431
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:57] [stdout]: Train Epoch: 4 [32000/60000 (53%)]	Loss: 0.146725
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:57] [stdout]: Train Epoch: 4 [35200/60000 (59%)]	Loss: 0.399910
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:58] [stdout]: Train Epoch: 4 [38400/60000 (64%)]	Loss: 0.173072
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:58] [stdout]: Train Epoch: 4 [41600/60000 (69%)]	Loss: 0.153188
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:59] [stdout]: Train Epoch: 4 [44800/60000 (75%)]	Loss: 0.520149
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:49:59] [stdout]: Train Epoch: 4 [48000/60000 (80%)]	Loss: 0.311795
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:00] [stdout]: Train Epoch: 4 [51200/60000 (85%)]	Loss: 0.231266
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:00] [stdout]: Train Epoch: 4 [54400/60000 (91%)]	Loss: 0.217354
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:00] [stdout]: Train Epoch: 4 [57600/60000 (96%)]	Loss: 0.482486
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:02] [stdout]: 
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:02] [stdout]: Test set: Average loss: 0.1396, Accuracy: 9584/10000 (96%)
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:02] [stdout]: 
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:02] [stdout]: Train Epoch: 5 [0/60000 (0%)]	Loss: 0.140468
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:03] [stdout]: Train Epoch: 5 [3200/60000 (5%)]	Loss: 0.040232
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:03] [stdout]: Train Epoch: 5 [6400/60000 (11%)]	Loss: 0.266170
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:04] [stdout]: Train Epoch: 5 [9600/60000 (16%)]	Loss: 0.293068
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:04] [stdout]: Train Epoch: 5 [12800/60000 (21%)]	Loss: 0.397776
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:05] [stdout]: Train Epoch: 5 [16000/60000 (27%)]	Loss: 0.694193
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:05] [stdout]: Train Epoch: 5 [19200/60000 (32%)]	Loss: 0.248157
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:06] [stdout]: Train Epoch: 5 [22400/60000 (37%)]	Loss: 0.044636
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:06] [stdout]: Train Epoch: 5 [25600/60000 (43%)]	Loss: 0.088821
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:07] [stdout]: Train Epoch: 5 [28800/60000 (48%)]	Loss: 0.380628
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:07] [stdout]: Train Epoch: 5 [32000/60000 (53%)]	Loss: 0.129619
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:08] [stdout]: Train Epoch: 5 [35200/60000 (59%)]	Loss: 0.193073
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:08] [stdout]: Train Epoch: 5 [38400/60000 (64%)]	Loss: 0.080020
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:09] [stdout]: Train Epoch: 5 [41600/60000 (69%)]	Loss: 0.151776
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:09] [stdout]: Train Epoch: 5 [44800/60000 (75%)]	Loss: 0.410607
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:10] [stdout]: Train Epoch: 5 [48000/60000 (80%)]	Loss: 0.033988
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:10] [stdout]: Train Epoch: 5 [51200/60000 (85%)]	Loss: 0.209576
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:11] [stdout]: Train Epoch: 5 [54400/60000 (91%)]	Loss: 0.269814
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:11] [stdout]: Train Epoch: 5 [57600/60000 (96%)]	Loss: 0.203218
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:13] [stdout]: 
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:13] [stdout]: Test set: Average loss: 0.1268, Accuracy: 9610/10000 (96%)
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:13] [stdout]: 
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:13] [stdout]: Train Epoch: 6 [0/60000 (0%)]	Loss: 0.101179
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:13] [stdout]: Train Epoch: 6 [3200/60000 (5%)]	Loss: 0.084789
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:14] [stdout]: Train Epoch: 6 [6400/60000 (11%)]	Loss: 0.052756
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:15] [stdout]: Train Epoch: 6 [9600/60000 (16%)]	Loss: 0.057646
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:15] [stdout]: Train Epoch: 6 [12800/60000 (21%)]	Loss: 0.444654
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:16] [stdout]: Train Epoch: 6 [16000/60000 (27%)]	Loss: 0.076464
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:16] [stdout]: Train Epoch: 6 [19200/60000 (32%)]	Loss: 0.149269
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:17] [stdout]: Train Epoch: 6 [22400/60000 (37%)]	Loss: 0.114976
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:17] [stdout]: Train Epoch: 6 [25600/60000 (43%)]	Loss: 0.206794
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:18] [stdout]: Train Epoch: 6 [28800/60000 (48%)]	Loss: 0.093237
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:18] [stdout]: Train Epoch: 6 [32000/60000 (53%)]	Loss: 0.207031
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:19] [stdout]: Train Epoch: 6 [35200/60000 (59%)]	Loss: 0.072584
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:19] [stdout]: Train Epoch: 6 [38400/60000 (64%)]	Loss: 0.213829
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:20] [stdout]: Train Epoch: 6 [41600/60000 (69%)]	Loss: 0.287198
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:20] [stdout]: Train Epoch: 6 [44800/60000 (75%)]	Loss: 0.236963
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:21] [stdout]: Train Epoch: 6 [48000/60000 (80%)]	Loss: 0.170621
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:21] [stdout]: Train Epoch: 6 [51200/60000 (85%)]	Loss: 0.195540
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:21] [stdout]: Train Epoch: 6 [54400/60000 (91%)]	Loss: 0.104384
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:22] [stdout]: Train Epoch: 6 [57600/60000 (96%)]	Loss: 0.083940
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:24] [stdout]: 
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:24] [stdout]: Test set: Average loss: 0.1224, Accuracy: 9630/10000 (96%)
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:24] [stdout]: 
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:24] [stdout]: Train Epoch: 7 [0/60000 (0%)]	Loss: 0.190592
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:24] [stdout]: Train Epoch: 7 [3200/60000 (5%)]	Loss: 0.043009
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:25] [stdout]: Train Epoch: 7 [6400/60000 (11%)]	Loss: 0.050237
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:25] [stdout]: Train Epoch: 7 [9600/60000 (16%)]	Loss: 0.246587
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:26] [stdout]: Train Epoch: 7 [12800/60000 (21%)]	Loss: 0.078391
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:26] [stdout]: Train Epoch: 7 [16000/60000 (27%)]	Loss: 0.201275
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:27] [stdout]: Train Epoch: 7 [19200/60000 (32%)]	Loss: 0.018863
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:27] [stdout]: Train Epoch: 7 [22400/60000 (37%)]	Loss: 0.092426
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:28] [stdout]: Train Epoch: 7 [25600/60000 (43%)]	Loss: 0.031792
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:28] [stdout]: Train Epoch: 7 [28800/60000 (48%)]	Loss: 0.032400
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:29] [stdout]: Train Epoch: 7 [32000/60000 (53%)]	Loss: 0.045534
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:29] [stdout]: Train Epoch: 7 [35200/60000 (59%)]	Loss: 0.203333
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:30] [stdout]: Train Epoch: 7 [38400/60000 (64%)]	Loss: 0.345851
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:30] [stdout]: Train Epoch: 7 [41600/60000 (69%)]	Loss: 0.058379
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:31] [stdout]: Train Epoch: 7 [44800/60000 (75%)]	Loss: 0.138831
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:31] [stdout]: Train Epoch: 7 [48000/60000 (80%)]	Loss: 0.251697
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:32] [stdout]: Train Epoch: 7 [51200/60000 (85%)]	Loss: 0.498436
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:32] [stdout]: Train Epoch: 7 [54400/60000 (91%)]	Loss: 0.103899
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:33] [stdout]: Train Epoch: 7 [57600/60000 (96%)]	Loss: 0.162733
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:35] [stdout]: 
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:35] [stdout]: Test set: Average loss: 0.1158, Accuracy: 9644/10000 (96%)
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:35] [stdout]: 
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:35] [stdout]: Train Epoch: 8 [0/60000 (0%)]	Loss: 0.211506
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:35] [stdout]: Train Epoch: 8 [3200/60000 (5%)]	Loss: 0.101347
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:36] [stdout]: Train Epoch: 8 [6400/60000 (11%)]	Loss: 0.172136
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:36] [stdout]: Train Epoch: 8 [9600/60000 (16%)]	Loss: 0.035952
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:37] [stdout]: Train Epoch: 8 [12800/60000 (21%)]	Loss: 0.214432
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:37] [stdout]: Train Epoch: 8 [16000/60000 (27%)]	Loss: 0.127569
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:38] [stdout]: Train Epoch: 8 [19200/60000 (32%)]	Loss: 0.351355
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:38] [stdout]: Train Epoch: 8 [22400/60000 (37%)]	Loss: 0.164318
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:39] [stdout]: Train Epoch: 8 [25600/60000 (43%)]	Loss: 0.146245
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:39] [stdout]: Train Epoch: 8 [28800/60000 (48%)]	Loss: 0.189349
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:40] [stdout]: Train Epoch: 8 [32000/60000 (53%)]	Loss: 0.085352
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:40] [stdout]: Train Epoch: 8 [35200/60000 (59%)]	Loss: 0.034080
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:41] [stdout]: Train Epoch: 8 [38400/60000 (64%)]	Loss: 0.171350
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:41] [stdout]: Train Epoch: 8 [41600/60000 (69%)]	Loss: 0.259403
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:41] [stdout]: Train Epoch: 8 [44800/60000 (75%)]	Loss: 0.079780
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:42] [stdout]: Train Epoch: 8 [48000/60000 (80%)]	Loss: 0.176074
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:43] [stdout]: Train Epoch: 8 [51200/60000 (85%)]	Loss: 0.233286
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:43] [stdout]: Train Epoch: 8 [54400/60000 (91%)]	Loss: 0.187936
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:44] [stdout]: Train Epoch: 8 [57600/60000 (96%)]	Loss: 0.293092
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:45] [stdout]: 
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:45] [stdout]: Test set: Average loss: 0.1124, Accuracy: 9670/10000 (97%)
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:45] [stdout]: 
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:45] [stdout]: Train Epoch: 9 [0/60000 (0%)]	Loss: 0.081845
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:46] [stdout]: Train Epoch: 9 [3200/60000 (5%)]	Loss: 0.099376
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:46] [stdout]: Train Epoch: 9 [6400/60000 (11%)]	Loss: 0.135532
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:47] [stdout]: Train Epoch: 9 [9600/60000 (16%)]	Loss: 0.208034
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:47] [stdout]: Train Epoch: 9 [12800/60000 (21%)]	Loss: 0.117180
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:48] [stdout]: Train Epoch: 9 [16000/60000 (27%)]	Loss: 0.130200
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:48] [stdout]: Train Epoch: 9 [19200/60000 (32%)]	Loss: 0.127471
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:49] [stdout]: Train Epoch: 9 [22400/60000 (37%)]	Loss: 0.093316
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:49] [stdout]: Train Epoch: 9 [25600/60000 (43%)]	Loss: 0.112556
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:50] [stdout]: Train Epoch: 9 [28800/60000 (48%)]	Loss: 0.065529
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:50] [stdout]: Train Epoch: 9 [32000/60000 (53%)]	Loss: 0.392122
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:51] [stdout]: Train Epoch: 9 [35200/60000 (59%)]	Loss: 0.179014
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:51] [stdout]: Train Epoch: 9 [38400/60000 (64%)]	Loss: 0.064296
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:52] [stdout]: Train Epoch: 9 [41600/60000 (69%)]	Loss: 0.076229
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:52] [stdout]: Train Epoch: 9 [44800/60000 (75%)]	Loss: 0.035628
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:53] [stdout]: Train Epoch: 9 [48000/60000 (80%)]	Loss: 0.095813
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:53] [stdout]: Train Epoch: 9 [51200/60000 (85%)]	Loss: 0.107578
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:54] [stdout]: Train Epoch: 9 [54400/60000 (91%)]	Loss: 0.082271
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:54] [stdout]: Train Epoch: 9 [57600/60000 (96%)]	Loss: 0.048008
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:56] [stdout]: 
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:56] [stdout]: Test set: Average loss: 0.1057, Accuracy: 9678/10000 (97%)
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:56] [stdout]: 
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:56] [stdout]: Train Epoch: 10 [0/60000 (0%)]	Loss: 0.113570
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:57] [stdout]: Train Epoch: 10 [3200/60000 (5%)]	Loss: 0.108851
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:57] [stdout]: Train Epoch: 10 [6400/60000 (11%)]	Loss: 0.133826
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:58] [stdout]: Train Epoch: 10 [9600/60000 (16%)]	Loss: 0.211092
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:58] [stdout]: Train Epoch: 10 [12800/60000 (21%)]	Loss: 0.105940
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:59] [stdout]: Train Epoch: 10 [16000/60000 (27%)]	Loss: 0.176144
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:50:59] [stdout]: Train Epoch: 10 [19200/60000 (32%)]	Loss: 0.159901
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:51:00] [stdout]: Train Epoch: 10 [22400/60000 (37%)]	Loss: 0.138478
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:51:00] [stdout]: Train Epoch: 10 [25600/60000 (43%)]	Loss: 0.145080
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:51:01] [stdout]: Train Epoch: 10 [28800/60000 (48%)]	Loss: 0.206126
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:51:01] [stdout]: Train Epoch: 10 [32000/60000 (53%)]	Loss: 0.010295
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:51:02] [stdout]: Train Epoch: 10 [35200/60000 (59%)]	Loss: 0.258801
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:51:02] [stdout]: Train Epoch: 10 [38400/60000 (64%)]	Loss: 0.213898
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:51:03] [stdout]: Train Epoch: 10 [41600/60000 (69%)]	Loss: 0.146943
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:51:03] [stdout]: Train Epoch: 10 [44800/60000 (75%)]	Loss: 0.083091
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:51:04] [stdout]: Train Epoch: 10 [48000/60000 (80%)]	Loss: 0.154216
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:51:04] [stdout]: Train Epoch: 10 [51200/60000 (85%)]	Loss: 0.047584
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:51:05] [stdout]: Train Epoch: 10 [54400/60000 (91%)]	Loss: 0.400316
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:51:05] [stdout]: Train Epoch: 10 [57600/60000 (96%)]	Loss: 0.388808
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:51:07] [stdout]: 
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:51:07] [stdout]: Test set: Average loss: 0.1050, Accuracy: 9698/10000 (97%)
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:51:07] [stdout]: 
[17 Ð¼Ð°Ñ€Ñ‚Ð° 2018 Ð³. 15:51:07] [stdout]: Sat Mar 17 10:21:07 UTC 2018