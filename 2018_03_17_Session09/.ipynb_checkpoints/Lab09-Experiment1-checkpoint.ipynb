{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of AI & ML\n",
    "## Session 09\n",
    "### Experiment 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "Itâ€™s a Python based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "1. A replacement for NumPy to use the power of GPUs\n",
    "\n",
    "2. a deep learning research platform that provides maximum flexibility and speed\n",
    "\n",
    "http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html\n",
    "\n",
    "In this experiment we will use MNIST dataset and will be implementing MLP using Pytorch. We are going to do this step-by-step\n",
    "\n",
    "1. Loading MNIST dataset and Visualize\n",
    "2. Defining Loss functions\n",
    "3. Doing forward pass\n",
    "4. Run the classifier the complete test set and compute accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install the pytorch run the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip3\n",
      "\u001b[31m  Could not find a version that satisfies the requirement pip3 (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for pip3\u001b[0m\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 9.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade pip3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtorch-0.3.1-cp35-cp35m-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 9.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install http://download.pytorch.org/whl/cpu/torch-0.3.1-cp35-cp35m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing required pytorch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters \n",
    "input_size = 784\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll load the MNIST data. First time we may have to download the data, which can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Loading the train set file\n",
    "train_dataset = dsets.MNIST(root='../data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),  \n",
    "                            download=True)\n",
    "#Loading the test set file\n",
    "test_dataset = dsets.MNIST(root='../data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dsets.MNIST?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the train dataset\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# loading the test dataset\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test data are provided via data loaders that provide iterators over the datasets. Loading X and Y train values from the loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([10, 1, 28, 28]) type: torch.FloatTensor\n",
      "y_train: torch.Size([10]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting first 10 training digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAABeCAYAAAAHQJEfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAHIVJREFUeJzt3Xm8lGX5x/HPJZuyCa4gopjLTxHkl5KpKaKQgFJmhrkQLqCWS6YQm6EgLimklSkuqZCYaaKFmIi+foLklgtJuWAuIKaRmgug4DGe3x/PXPcM5xxh5jyzPDN+368XL3Bmzsxz+czMuZ/rvu7rtiiKEBEREZGm2aTSByAiIiJSzTSYEhEREUlAgykRERGRBDSYEhEREUlAgykRERGRBDSYEhEREUkgdYMpM5toZjMrfRylUuvxgWKsFbUeY63HB4qxVtR6jLUQX0UGU2Z2vJk9bWarzOxtM7vfzA6sxLHUO64dMseU+ycys5EFPk8q4wMwswPM7C9mttLMFjf1uFIe48Nm9o6ZfWRmz5nZkU18njTH2C0T58dm9pKZ9W/i86Q2RgAzO8fMXjez1Wb2opntVuDPpzY+M/tfM1toZh+a2ZtmNqGJz5PKGM1sGzO73czeysT4qJl9tYnPlcoYAcxsspn9zcw+M7OJCZ4nlTEW6zymNT4ozmex7IMpMzsP+DlwKbAtsANwLdCkX3jFFEXRG1EUtfU/QE9gHTAr3+dIc3xmtgVwLzAF6ABcAdxrZh0LfJ7UxphxDtA5iqL2wGnATDPrXMgTVEGMtwOLgC2B84G7zGzrQp4g7TGa2QhgOHAE0BYYDLxbwM+nOj7gt8AjwBbAwcAZZvbNQp4g5TG2BZ4C9iGOcQZwn5m1LeRJUh4jwCvAaOC+pj5BymNMfB5THh8U4bNIFEVl+wNsDqwChmzgMROBmTn//XvgX8CHmWD3zLnvcOAFYCXwT2BU5vatgDnAB8B/gIXAJk043guBh2slPuJfRs/Xu+1lYHitxNjIsewLrAH2rZUYgd2AtUC7nNsWAt+voRg3AZYD/Qo959UQX+ZnPwa613v9cbUUYyPH8xGwTy3GCMwEJtbiezXJeayG+JJ+FqMoKntman9gU+CeAn7mfmBXYBvgWeC2nPtuAk6Poqgd0AP4v8ztI4E3ga2JR8HjgQjAzK41s2s39qJmZsAw4lF4vqohPmvkv3sUcLzVECNmNsfM1gBPAvOBpws43rTHuCfwWhRFK3Nuey5ze77SHuP2mT89zGx5Zqpvkpnl+52V9vggvlIfZmYtzOx/Msf8UAHHWw0xBmb2v0BL4kxOvqoqxiaqqhibcB6rIb6kn0WaF/LgItgSeDeKos/y/YEoim72f2fmo983s82jKPoQqAO6m9lzURS9D7yfeWgd0BnYMYqiV4hHqP58Z+T50gcSn5C78j1W0h/f48B2ZnYccVzHAzsDrfM9XtIfoz9msJm1APoDe0RRtC7f4yX9MbYlvmLL9SHQJd/jJf0xbp/5+zDi6fYOwDziL8sb8zjctMcH8VX0b4BRQDPgoiiKnsr3eKmOGP212gO3ApMyr5WvqokxgaqJsYnnsRriS/pZLHtm6j1gKzPLaxBnZs3M7Kdm9qqZfQQszdy1Vebvo4lTfsvMbIGZ7Z+5fQrxqHmemb1mZmObcKwnArOiKFpVwM+kOr4oit4jnqM+D1gBDCQefb+Zz89npDrGXFEU1UVRdD9wWIHz32mPcRXQvt5t7YnT3vlKe4yfZP6+IoqiD6IoWgpcn3mNfKQ6PovrF+cCFxFftXcFBphZIb+4Ux1jzutuRlyr+UQURZcV8rNUSYwJVUWMCc5jquMr0mexIjVTq4HvbOAxE8nMnQLfA14EdiKejupAnLbbpd7PtADOBZY38nw9gH9TQO0FsBnxlf6htRhfzs82B94ABtRqjJmffwg4t1ZiJK6ZWsP6NVOPUHjNVJpjbE1cF9Yn57bzgHtqJL7ewPv1bvsRMKdWzmHm8a2AB4inaZpS85j6GHN+LknNVKpjTHIe0x5fMT6LUVTmmqkoTtFdAFxjZt8ys9YWz1EOMrMrGvmRdsRfqO8Rf7le6neYWUszOyGT+qsjLohbl7lvsJntYmZGPCj6r9+Xp6OIU4cP11p8ZvblzDG1B6YSvxEfqJUYzWz3zLFsljmuoUAfYEGtxBhF0cvAX4ELzWxTMzsK2IsCVp1WQYwfA3cAo82snZltT7wyc04txEe88MMsXi6+iZl1Ar4LLM4nvmqI0eJp9ruIs4wnRoVNtVdFjB6nmW1KPNPTPPOZbFYrMSY9j2mPjyJ8Fj3Qsv8BTiAuCF5NXLF/H3BAIyPUtsAfiacvlhEXhEfALsQFcHOJBz0fES/dPDDzc+cSpwZXE09hTch57euA6zZyfA8Ak2sxPuIl9R9m/twBbFNLMQJ7EBedryRe1fEUcFQtxZi5vxtxYf0nwBKgfw3G2B74XeY1lxN/IVsNxXdo5rk+zBzbjUDrWjmHxEvMI+KVUqty/hxUKzFm7p+eeY3cPyfVSozFOo9pja9Yn0XLPJGIiIiINEHqtpMRERERqSYaTImIiIgkoMGUiIiISAIaTImIiIgkoMGUiIiISAJl3U7GzKp66WAURfX3tWug1mOs9fhAMVYDxVj78YFirAaKMabMlIiIiEgC5d7oWES+YLp27QrA7373OwBeeuklAIYPH16xYxIRKSZlpkREREQSUGZKRErq2GOPBeCAAw4A4JNPPqnk4UiOQYMGATBp0iQA9tlnn3Dfb37zGwDGjh3LihUryn9w0mRHHnkkI0aMAOCII44A4NVXX2XIkCEA/PWvf63YsdUqZaZEREREEijr3nxfhIr+Wo+xmPG1bNkSgL59+3LhhRcCsHLlSgA6depEz549AbjzzjuB7NXzsmXLmpzd0DmMlSvGkSNHcskllwDQqlUrAMaPHw/AZZdd1uTnTVOMpVLKz+KBBx4IwP333w9A69at/TUbPPbFF1+kX79+APz73/9u6ks2UKlzOHHixPB9Y7bRQ0ik3DEecsghAMyZM4dNN920wf1r164FYOeddwbg7bffTvya+izGlJkSERERSUCZqQJoBF6c+LbeemsAbr75ZiCu2/ArxHzej3PmzOE73/kOAJ999llBr522c/ilL30JiOsZAJ5++mkAfvazn4XVb4WqZIyeffrlL38JwLBhw8IV8h/+8AcAjj76aADWrVvX5NdJ23k86qijADjllFOAbJ2KmbFmzRoAfvvb3wL5r2Is5WfxJz/5CRBnaQDq6uoAmD9/PltuuSUAe++9d3j82LFjAZg6dWpTX7KBSp3Dvn378vDDDwPZbLf/fyi2csc4b948APr16xc+Xy+88AIAPXr0CN+zO+ywAwBvvvlm4tdM22fxtNNOA+Dqq68GsjMg06ZN44wzzmjSc+YTY9UVoB955JFA9s3Qp08fvv3tbzd43CabxEk3/7C89tprQLaoUsrvsMMOA+COO+4AoF27duG+9957D8gun3/22WfZdtttAfjBD34AwPbbbw/A4MGDGTduHJCdKip0UFVJm222Wfi3vy+//OUvA3DWWWcB8WBkp512ApJNh5VTy5Ytw3nxLzSIzyVkBxFJBlFpsuOOOwLxl3bu4AnglVdeAeCZZ55h9uzZQPZcV9qIESMYM2YMkP1F64sEXnjhBXr06AHEAyuADh06lP8gS2j+/PkhtlrRvn17IHtxBvDDH/4QgN///vcAPP/88+FC9oQTTgDg8ssvL+dhlkznzp0BmDFjRpiS9s/iW2+9BcTfSU899RQAt9xyS9GPQdN8IiIiIgmkeprPR5aXXHJJmBrwEagXTJpZo1ND9aeNPI191VVXhQLYQpU7nbnvvvsCcTZtt912A7LTQePHjw9XHMVUqqmFSZMmhazL5ptvvt59K1asCNMOjV0xbLXVVgAhNb/HHnuE+7p06RKeIx+VTEl73HPnzgXiK36fArr77rsB+M9//gPEman3338fgClTpgD5X0VWKsZJkyZxwQUXrHfb3XffHbJUnn0shjRMLfz6178G4qm9VatWAXDllVcChKJ7/95pilJ9FufOnUv//v0B2GWXXQBYunRpg8fdeOONAJx88snhfn98MVTyHPp3yYIFC4Dqn+bzjNNDDz0EwFNPPcWpp57qxwDAmDFjQpZ7yZIlwPrfpU1VyfPYrFkzIDu9ecghh4TvTZ/5eP311wHYddddw+dywoQJBb2OCtBFRERESqwqMlNvvfVWGHnX9+qrrzZag9GxY0cgm9XI1bx500rFyjUC/9rXvgbANddcA8Bee+3V4DFr167l5JNPBmhyoXJjSnU1vGDBghCX87nsyZMnh6vgXG3btgXgG9/4BpAtau7YsSMPPvggAN/61reA7JLfjankVdTChQuBbPNKyNbReGGvF2tffPHFLFq0CMjWrBx77LGhxmVDyh2jn4M777yTFi1aADBr1iwATj/99KJmpFwlz6MXZv/lL38B4OOPPw6Zc39fFkOpPot33303//rXvwA2WJC75557AvDYY4+Fmpzvf//7AFx//fVNeen1pCG7WGrljtF/T65du5aPPvqowf3+u7JWMlNnnnkmkC02f/7558PvRZ8JuO+++4A4S7zffvuFxxWi6gvQfaA3cOBADjrooEYf86tf/arR288++2wgntarNj691717dyCeMvA3hPcHGTNmTBiA7LrrrkC20HDJkiV5rYorN+9p4t13vVj573//e4PHtmnThmnTpgFw3HHHNXge7xOT7yCq0qZNm9ZgMDly5MjPfX/+4he/CNNj/vchhxyS12Cq3HwaskWLFqHA04/Zpy1riS9u8b9nzZpV1EFUqZ155plhenlD/BfO6tWrw4XN6aefDhRnMFVJffv2Bai5QvR33nknr8d5ecTuu+8OZPfLrCYHHnggP/3pT9e77Z577gkror/+9a8D2c/pokWLCh5EFULTfCIiIiIJpDoz5Z577jmee+65Sh9GyXla3ae1zjnnHICQoQF44oknALjttttCEZ33XPI2ENOnT+dHP/oRQKOp3koYPnx4aF/QWLFrfeeff36DjJS76aabwhRL2nn/oQEDBvDGG28A2VYPG8tm3HDDDQBsscUWAIwaNSr0KvIiy0qaPHkykC1KXrlyJaNGjQJqMyPlvD2L21jLg+222w7ILlvv27dvWHBQiUzjhrpe9+jRIxyvT8PXIs9s+9/eOfyLok2bNkB1tr3wRWhXX311iMMtX748LE7z34++OGnYsGElPS5lpkREREQSqIrMVFMUo7Cu3Lx7stcB3XTTTRt8vGcGvGbF61SmTp3Ko48+mtdzlIs3Mfw8Xotx3nnnAbDNNtuE+/zKwhvNPf7446U4xKIaOHAgkC2af/vtt0NG6plnninouRrbY6uSOnXqBMQ1X7lmzpzJI4880uDxvpDE/594m49rrrmmqpqtugceeACIs6cAvXr1Co1Ye/XqBWQb1Hbu3Dk0xPSr6Mcff5yZM2eW9Zg3xusvjzrqqJCt+OCDD4DsQpBa4jVTX1TesuPTTz+t8JEUzpv/9urVK8xYeTPuww8/PHTw9/0nPcOfz4xIEspMiYiIiCRQk5mprbfeOmQ66q9qy60/SitfcZDvVYOvzPEsyAUXXFBVmbkZM2YwdOhQIHu+Pvjgg7AtwN/+9jegOupwvN7Em8P5Vf6gQYMaXbVYja699loguy2Ov/8aayrarFkzTjrpJCDb5NJ99tlnof1HNam/Eq5Pnz5h30FfQeRWrFgRVr7deeedQHZ7nUry+kyvKxkyZAiw/vdlNdbTSH68kWUa3ouF+uY3vxn+7Z8pXxX98ssvh3pEX1nstcelVpODKU+/5/Ilo431M0oLHzx4F9tCf877/bRt2zZ0+E2z3r17A9kvcoiXYUO8UeyTTz5ZkeNqqu22244ZM2YA2b32vKVFMQZSPl1WSZ06dQrvN+efqWXLloXbfKA1bty4z+02fOKJJ1bdYKpNmzahe7jbaqutwiDKp+hvu+02IG5hUoo+W0l5q5n6g6gZM2aEqdcRI0aEx/vy8jS8B5OqlSm+Ll26hClz54tBcssq/Ht0yJAh4fxV43n0C1VvBbRy5cowhee9I0899dTwXvXdGMrVPkfTfCIiIiIJ1GRmyszC6NQ7vnqLgMWLF1fsuDZm++23B7L773lDtdylzN7hdurUqeFq0lspeMzPPvtsaKGQZl/5ylcAaNWqVYMr32bNmoWrjWopUt57771D1ubPf/4zkG2NUAxpaMR63HHHhW7YzlPtkN0ryzsSDx8+PBy3fxb9MdXEi8fPPvtsLr300vXuW7NmDbNnzwYITQS9MW1a1S+D8CL5u+66Kyw99+zq3nvvHc5dGt6DSdVKo87p06dz6KGHbvRx3rpjzZo14fxV43n09+B///vf8LdnfceMGQPEU4CeKfeFIuWizJSIiIhIAjWVmerWrRsAQ4cObXAlVco28sV28cUXA9CyZUtg/QaNvryzefPmYZ8h98knnwBw3XXXpbJOoz5fDNCvX7+QwfGGa4888gj3338/kL3a94LCtC3n9UxLbqsA37G9GFk1L3hu3rx5k/eVLJbjjz++wW25WVB/7/ry5bq6ulCX49lHLxCtJt/73vcAGmSlIN7SavTo0eU+pKK66667wr89E+7ZxVtuuSXc53su5vLvXVfqJegS8+/KjfFmsblyZwAgm+1JM99P0jNte+21V9ib76yzzgLitjMb2m+ylGpqMOUFar7BIWQLmq+88sqKHFMhvHv0zTffvN7tuV1e/Ytq+vTpYRNnLzocO3Ys0HDVVNpddNFFzJs3D4Af//jHAOy0004MGjQIIPzt+zBefvnlG+ziXG4+RdKnT5+wP6KvlknCvwR9Y905c+bktadapXTo0IETTzxxvdsuvfRS/vjHPwINN+S+4447ynZsTeWLJC666KIG9/l0kRfBVov99tsvfGcUqk+fPkB2tSpkd2A499xzgXhfySuuuCLhUUo+vAffddddB2Q76nfr1i2sequ/Jyhki7h9D9uf//znJT/WYvFB/owZM8K/3bp169hnn30Ayr5Lhqb5RERERBKoicxU9+7dAbj11lsb3Od71HlBcJr58nlPpXfs2DHc5/u4eXF6mzZtQidtv8r0NGi1Wbx4cVgY4J1sr7322nBl5f1uPJW7++67h/3RyrXsdUM8c2RmofixGFORnmn1hQjHHnssK1euTPy8STzzzDMhW+O84/Do0aND8bJ3Jr7kkkv47ne/C2Rbd/hiEN9nMI18rzafivYs8OzZsznggAOA7BXyxrr7p83o0aND64oN8Sv8XL4ApjHeGf/ss88OixI05VdaS5YsAbLTz15W0KFDh7wWv0ydOhWIu4VPnDgRyGa71q5dm5q9XXN525HevXuHzJrr2rUrCxcuBLL7LnqZSKkpMyUiIiKSgJVziaSZleTFvKZmzpw54TYvdvWsjWd0koiiaKOdzkoVY32tW7cOV8Qe24ABAwD4+OOPm/y8G4uxXPFBNp7LLrsMiAsOnTeC9PvyVcxz6JkjL95t3rw5++67b0HH83n69u0bCtr/8Y9/AHEzWl9ksCGlfJ9269YtLObwAlivierSpUvIWvkV76abbsoxxxwDZPdb9Pojv3JsilJ/Fj3L7ftB/vOf/wTirI7XX/bo0QOgZIs9SvlZ9NYNPXv2BLJLy/28QbYZ6ymnnNKg1Yy/55ctWxZanHgzzE8//TTsTbhgwYLPPYZKfp/W/71XqiaWpYzx0UcfDRlwbxrre0NeddVVDWqlFi1aFPZqHTduHBB/Zp3/3vA9GefNmxcat25onFDu8+gLHubNmxd2xfBZi8GDB4dmnb5PaDGatOYTozJTIiIiIglUfc1Ut27dwnxx7ujZs1RJsjRpdvDBB4caBV85U2uxetM1X3rvq6d69uzZ6HLfcjv44IMB+OpXvwrEV4pJtWvXDoivprbddlsg25Q1DZYuXRq2yPGWB16/lstbI+TW/f3pT38CYNKkSaU+zET222+/sELT+RX966+/zvLly4HSZaTKwc+hZ9dyeb2Y19xEURQyIF6zd/TRRzf4Oc9azZ8/f4MZqTTw92CS7Gga5NZTApx22mnA+iv43n33XQAGDhwY/u2ZV69tGzBgQMg0+98nnXRSaGPi75dKatWqFUBofdC5c+ewktRrbr3xdSVU/WDq3nvvbbCp77Rp0xoUptWKrl27AvGb26ceGiu8rwQvah08eHC4zT+8he4V2Lt379BH6+STTwYa75dSSaXYCNanLwcOHNhoT6c08E2p/RdxY5tq+yCqrq4uDKK895b/0k2rdu3ahWkt5+9jn3qudi+99NJ6/+3TOZtvvnnoju7v72XLlnHKKacA2fIJb2uxxRZbhOfwzZ59ykVKz6fOfU/QXL4nny/88PcwwKpVq4DspsH9+/cPBeg+bfvee+/RokWL0hx4E5x33nlAtoXQ9ddf32BHk9zvonL3ltQ0n4iIiEgCVZ+Z6t69e4PiOF8uWot8LyYzC8WiaZlu8CvZ22+/PdzmBdNTpkwJ0yON8Smz/fffH4ivuHzKq76lS5euVyhbKd6EctiwYUBckO5TX968c2O8iN2vLNu2bQvE/z9yO9+niU/5eEsSz2qMHj06NAP0bNSUKVOqbi+0Bx98MDT79fPhhb1Dhw5dr1t4tfIyCC/g9fPmhcm5Jk+e3GDazhda+P8fyLZ2qUZ9+/atuvfpE0880WAXDM/6XnzxxaER54cffvi5z+GtFObOnRtmD3z2Y/Xq1alqjuyZem99MHr06LBwwJs9jxo1KrTjKHRxUlLKTImIiIgkUHWZKS929Wacm2yySRiN+5LO+vOotSiKokb3yaokv5q/9dZbw15mXkfly1U/j19hbGgJrsc7YcIEXn755cTHm5Rnjnz7nhtuuIHLL78cgDfeeAPIvhdzWxp4PVH//v1DdsfrkLyWwf9fVgOPv9q2MdoQv0r34n8vqF+3bh3jx4+v2HEVi793fYum3ELsZcuWAXFGCrKNdHPVWjPOasxMjRs3LtRDvfPOO0C2eefs2bMLfj5vgJy2JrTeZsNbIsydOxdYf3bGtzKqq6sLn9U333yzrMdZNYMp77zrX2TenXfdunXhF7Cnor2/RC3xFRbeTXrx4sWpmd5z3i13+PDhYcNbL6j2wcLnqaurA+JpIecFrV4s6/vSpW1TTv9l06tXr9CX6LHHHgMIg741a9aEAaNPhx5zzDE89NBDAKmd0vui8u7mRxxxBJAtvH7yySc3OF1dbbznV2N7D0q6ffrpp+y8886VPoyS8x0HfI9aX0l6+OGHh4Jz3/x45MiRoe9duWmaT0RERCSBqumA7gXKnr70UaqZhWJXX0KfuwS0mCrZsdf3f3v66aeBeFm67xBeTGnqgF4KpT6H3i7Ai/A9NT1r1qyQmfIC5vnz55dkr700deovlXLF6NPVPp0wYcIEbrjhhqRPmxd9FhVjNSh1jO3btwfiNkgABx10EBDvDOF9sjwb5b8fi00d0EVERERKrGoyU847sXrzvIULF4YivA0tAS2GSl5lnH/++UA2Q9VYB+Ji0NWwYqwGirH24wPFWA0UY0yZKREREZEEqmY1n/PVNV9UvmJORERE0qHqpvkqSenM2o8PFGM1UIy1Hx8oxmqgGGOa5hMRERFJoKyZKREREZFao8yUiIiISAIaTImIiIgkoMGUiIiISAIaTImIiIgkoMGUiIiISAIaTImIiIgkoMGUiIiISAIaTImIiIgkoMGUiIiISAIaTImIiIgkoMGUiIiISAIaTImIiIgkoMGUiIiISAIaTImIiIgkoMGUiIiISAIaTImIiIgkoMGUiIiISAIaTImIiIgkoMGUiIiISAIaTImIiIgkoMGUiIiISAIaTImIiIgkoMGUiIiISAL/D0S5s+OjOiQvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f03ccbfdf28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pltsize=1\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i,:,:,:].numpy().reshape(28,28), cmap=\"gray\")\n",
    "    plt.title('Class: '+str(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the network as a Python class. We have to write the __init__() and forward() methods, and PyTorch will automatically generate a backward() method for computing the gradients for the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Linear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a neural network object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CrossEntropyLoss function uses inputs, labels  to calculate the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.CrossEntropyLoss?\n",
    "nn.Module?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/6000], Loss: 0.9758\n",
      "Epoch [1/5], Step [200/6000], Loss: 0.7842\n",
      "Epoch [1/5], Step [300/6000], Loss: 0.7631\n",
      "Epoch [1/5], Step [400/6000], Loss: 0.1692\n",
      "Epoch [1/5], Step [500/6000], Loss: 0.4007\n",
      "Epoch [1/5], Step [600/6000], Loss: 0.2340\n",
      "Epoch [1/5], Step [700/6000], Loss: 0.0822\n",
      "Epoch [1/5], Step [800/6000], Loss: 0.0739\n",
      "Epoch [1/5], Step [900/6000], Loss: 0.0803\n",
      "Epoch [1/5], Step [1000/6000], Loss: 0.5322\n",
      "Epoch [1/5], Step [1100/6000], Loss: 0.5537\n",
      "Epoch [1/5], Step [1200/6000], Loss: 0.5192\n",
      "Epoch [1/5], Step [1300/6000], Loss: 0.0393\n",
      "Epoch [1/5], Step [1400/6000], Loss: 0.2291\n",
      "Epoch [1/5], Step [1500/6000], Loss: 0.1167\n",
      "Epoch [1/5], Step [1600/6000], Loss: 0.1529\n",
      "Epoch [1/5], Step [1700/6000], Loss: 0.0657\n",
      "Epoch [1/5], Step [1800/6000], Loss: 0.1372\n",
      "Epoch [1/5], Step [1900/6000], Loss: 0.1007\n",
      "Epoch [1/5], Step [2000/6000], Loss: 0.2467\n",
      "Epoch [1/5], Step [2100/6000], Loss: 0.0241\n",
      "Epoch [1/5], Step [2200/6000], Loss: 0.1637\n",
      "Epoch [1/5], Step [2300/6000], Loss: 0.0227\n",
      "Epoch [1/5], Step [2400/6000], Loss: 0.0498\n",
      "Epoch [1/5], Step [2500/6000], Loss: 0.0220\n",
      "Epoch [1/5], Step [2600/6000], Loss: 0.1124\n",
      "Epoch [1/5], Step [2700/6000], Loss: 0.0831\n",
      "Epoch [1/5], Step [2800/6000], Loss: 0.0437\n",
      "Epoch [1/5], Step [2900/6000], Loss: 0.2974\n",
      "Epoch [1/5], Step [3000/6000], Loss: 0.1295\n",
      "Epoch [1/5], Step [3100/6000], Loss: 0.0337\n",
      "Epoch [1/5], Step [3200/6000], Loss: 0.1215\n",
      "Epoch [1/5], Step [3300/6000], Loss: 0.0053\n",
      "Epoch [1/5], Step [3400/6000], Loss: 0.0074\n",
      "Epoch [1/5], Step [3500/6000], Loss: 0.0185\n",
      "Epoch [1/5], Step [3600/6000], Loss: 0.0662\n",
      "Epoch [1/5], Step [3700/6000], Loss: 0.1169\n",
      "Epoch [1/5], Step [3800/6000], Loss: 0.1007\n",
      "Epoch [1/5], Step [3900/6000], Loss: 0.0238\n",
      "Epoch [1/5], Step [4000/6000], Loss: 0.0703\n",
      "Epoch [1/5], Step [4100/6000], Loss: 0.0444\n",
      "Epoch [1/5], Step [4200/6000], Loss: 0.0274\n",
      "Epoch [1/5], Step [4300/6000], Loss: 0.0266\n",
      "Epoch [1/5], Step [4400/6000], Loss: 0.1069\n",
      "Epoch [1/5], Step [4500/6000], Loss: 0.0025\n",
      "Epoch [1/5], Step [4600/6000], Loss: 0.2080\n",
      "Epoch [1/5], Step [4700/6000], Loss: 0.3254\n",
      "Epoch [1/5], Step [4800/6000], Loss: 0.0332\n",
      "Epoch [1/5], Step [4900/6000], Loss: 0.0090\n",
      "Epoch [1/5], Step [5000/6000], Loss: 0.9543\n",
      "Epoch [1/5], Step [5100/6000], Loss: 0.0419\n",
      "Epoch [1/5], Step [5200/6000], Loss: 0.2814\n",
      "Epoch [1/5], Step [5300/6000], Loss: 0.0012\n",
      "Epoch [1/5], Step [5400/6000], Loss: 0.0196\n",
      "Epoch [1/5], Step [5500/6000], Loss: 0.0113\n",
      "Epoch [1/5], Step [5600/6000], Loss: 0.0886\n",
      "Epoch [1/5], Step [5700/6000], Loss: 0.0332\n",
      "Epoch [1/5], Step [5800/6000], Loss: 0.0053\n",
      "Epoch [1/5], Step [5900/6000], Loss: 0.0827\n",
      "Epoch [1/5], Step [6000/6000], Loss: 0.0106\n",
      "Epoch [2/5], Step [100/6000], Loss: 0.0668\n",
      "Epoch [2/5], Step [200/6000], Loss: 0.0086\n",
      "Epoch [2/5], Step [300/6000], Loss: 0.0002\n",
      "Epoch [2/5], Step [400/6000], Loss: 0.0599\n",
      "Epoch [2/5], Step [500/6000], Loss: 0.0082\n",
      "Epoch [2/5], Step [600/6000], Loss: 0.1080\n",
      "Epoch [2/5], Step [700/6000], Loss: 0.0004\n",
      "Epoch [2/5], Step [800/6000], Loss: 0.0032\n",
      "Epoch [2/5], Step [900/6000], Loss: 0.0021\n",
      "Epoch [2/5], Step [1000/6000], Loss: 0.0091\n",
      "Epoch [2/5], Step [1100/6000], Loss: 0.0146\n",
      "Epoch [2/5], Step [1200/6000], Loss: 0.0195\n",
      "Epoch [2/5], Step [1300/6000], Loss: 0.0047\n",
      "Epoch [2/5], Step [1400/6000], Loss: 0.0509\n",
      "Epoch [2/5], Step [1500/6000], Loss: 0.0066\n",
      "Epoch [2/5], Step [1600/6000], Loss: 0.0258\n",
      "Epoch [2/5], Step [1700/6000], Loss: 0.0004\n",
      "Epoch [2/5], Step [1800/6000], Loss: 0.0162\n",
      "Epoch [2/5], Step [1900/6000], Loss: 0.1174\n",
      "Epoch [2/5], Step [2000/6000], Loss: 0.1955\n",
      "Epoch [2/5], Step [2100/6000], Loss: 0.0015\n",
      "Epoch [2/5], Step [2200/6000], Loss: 0.2117\n",
      "Epoch [2/5], Step [2300/6000], Loss: 0.1620\n",
      "Epoch [2/5], Step [2400/6000], Loss: 0.1825\n",
      "Epoch [2/5], Step [2500/6000], Loss: 0.1937\n",
      "Epoch [2/5], Step [2600/6000], Loss: 0.0035\n",
      "Epoch [2/5], Step [2700/6000], Loss: 0.0027\n",
      "Epoch [2/5], Step [2800/6000], Loss: 0.0102\n",
      "Epoch [2/5], Step [2900/6000], Loss: 0.0147\n",
      "Epoch [2/5], Step [3000/6000], Loss: 0.0035\n",
      "Epoch [2/5], Step [3100/6000], Loss: 0.0864\n",
      "Epoch [2/5], Step [3200/6000], Loss: 0.0020\n",
      "Epoch [2/5], Step [3300/6000], Loss: 0.0743\n",
      "Epoch [2/5], Step [3400/6000], Loss: 0.0756\n",
      "Epoch [2/5], Step [3500/6000], Loss: 1.0926\n",
      "Epoch [2/5], Step [3600/6000], Loss: 0.0047\n",
      "Epoch [2/5], Step [3700/6000], Loss: 0.0136\n",
      "Epoch [2/5], Step [3800/6000], Loss: 0.0296\n",
      "Epoch [2/5], Step [3900/6000], Loss: 0.2077\n",
      "Epoch [2/5], Step [4000/6000], Loss: 0.0086\n",
      "Epoch [2/5], Step [4100/6000], Loss: 0.0020\n",
      "Epoch [2/5], Step [4200/6000], Loss: 0.1441\n",
      "Epoch [2/5], Step [4300/6000], Loss: 0.1245\n",
      "Epoch [2/5], Step [4400/6000], Loss: 0.0431\n",
      "Epoch [2/5], Step [4500/6000], Loss: 0.1318\n",
      "Epoch [2/5], Step [4600/6000], Loss: 0.8154\n",
      "Epoch [2/5], Step [4700/6000], Loss: 0.1016\n",
      "Epoch [2/5], Step [4800/6000], Loss: 0.0007\n",
      "Epoch [2/5], Step [4900/6000], Loss: 0.0538\n",
      "Epoch [2/5], Step [5000/6000], Loss: 0.1804\n",
      "Epoch [2/5], Step [5100/6000], Loss: 0.1059\n",
      "Epoch [2/5], Step [5200/6000], Loss: 0.0017\n",
      "Epoch [2/5], Step [5300/6000], Loss: 0.0009\n",
      "Epoch [2/5], Step [5400/6000], Loss: 0.0015\n",
      "Epoch [2/5], Step [5500/6000], Loss: 0.0010\n",
      "Epoch [2/5], Step [5600/6000], Loss: 0.0007\n",
      "Epoch [2/5], Step [5700/6000], Loss: 0.0018\n",
      "Epoch [2/5], Step [5800/6000], Loss: 0.0001\n",
      "Epoch [2/5], Step [5900/6000], Loss: 0.0281\n",
      "Epoch [2/5], Step [6000/6000], Loss: 0.0004\n",
      "Epoch [3/5], Step [100/6000], Loss: 0.0101\n",
      "Epoch [3/5], Step [200/6000], Loss: 0.1686\n",
      "Epoch [3/5], Step [300/6000], Loss: 0.0034\n",
      "Epoch [3/5], Step [400/6000], Loss: 0.1009\n",
      "Epoch [3/5], Step [500/6000], Loss: 0.0537\n",
      "Epoch [3/5], Step [600/6000], Loss: 0.0006\n",
      "Epoch [3/5], Step [700/6000], Loss: 0.0012\n",
      "Epoch [3/5], Step [800/6000], Loss: 0.0021\n",
      "Epoch [3/5], Step [900/6000], Loss: 0.3303\n",
      "Epoch [3/5], Step [1000/6000], Loss: 0.1112\n",
      "Epoch [3/5], Step [1100/6000], Loss: 0.0004\n",
      "Epoch [3/5], Step [1200/6000], Loss: 0.0014\n",
      "Epoch [3/5], Step [1300/6000], Loss: 0.0863\n",
      "Epoch [3/5], Step [1400/6000], Loss: 0.0668\n",
      "Epoch [3/5], Step [1500/6000], Loss: 0.0056\n",
      "Epoch [3/5], Step [1600/6000], Loss: 0.0015\n",
      "Epoch [3/5], Step [1700/6000], Loss: 0.1661\n",
      "Epoch [3/5], Step [1800/6000], Loss: 0.0058\n",
      "Epoch [3/5], Step [1900/6000], Loss: 0.0072\n",
      "Epoch [3/5], Step [2000/6000], Loss: 0.2689\n",
      "Epoch [3/5], Step [2100/6000], Loss: 0.0053\n",
      "Epoch [3/5], Step [2200/6000], Loss: 0.0377\n",
      "Epoch [3/5], Step [2300/6000], Loss: 0.0289\n",
      "Epoch [3/5], Step [2400/6000], Loss: 0.0003\n",
      "Epoch [3/5], Step [2500/6000], Loss: 0.0009\n",
      "Epoch [3/5], Step [2600/6000], Loss: 0.3163\n",
      "Epoch [3/5], Step [2700/6000], Loss: 0.0046\n",
      "Epoch [3/5], Step [2800/6000], Loss: 0.1554\n",
      "Epoch [3/5], Step [2900/6000], Loss: 0.5684\n",
      "Epoch [3/5], Step [3000/6000], Loss: 0.0001\n",
      "Epoch [3/5], Step [3100/6000], Loss: 0.0013\n",
      "Epoch [3/5], Step [3200/6000], Loss: 0.0177\n",
      "Epoch [3/5], Step [3300/6000], Loss: 0.0029\n",
      "Epoch [3/5], Step [3400/6000], Loss: 0.0120\n",
      "Epoch [3/5], Step [3500/6000], Loss: 0.0008\n",
      "Epoch [3/5], Step [3600/6000], Loss: 0.0225\n",
      "Epoch [3/5], Step [3700/6000], Loss: 0.0055\n",
      "Epoch [3/5], Step [3800/6000], Loss: 0.0128\n",
      "Epoch [3/5], Step [3900/6000], Loss: 0.0001\n",
      "Epoch [3/5], Step [4000/6000], Loss: 0.0011\n",
      "Epoch [3/5], Step [4100/6000], Loss: 0.0079\n",
      "Epoch [3/5], Step [4200/6000], Loss: 0.0083\n",
      "Epoch [3/5], Step [4300/6000], Loss: 0.0006\n",
      "Epoch [3/5], Step [4400/6000], Loss: 0.3249\n",
      "Epoch [3/5], Step [4500/6000], Loss: 0.0583\n",
      "Epoch [3/5], Step [4600/6000], Loss: 0.0031\n",
      "Epoch [3/5], Step [4700/6000], Loss: 0.2073\n",
      "Epoch [3/5], Step [4800/6000], Loss: 0.2912\n",
      "Epoch [3/5], Step [4900/6000], Loss: 0.0527\n",
      "Epoch [3/5], Step [5000/6000], Loss: 0.0004\n",
      "Epoch [3/5], Step [5100/6000], Loss: 0.0012\n",
      "Epoch [3/5], Step [5200/6000], Loss: 0.0951\n",
      "Epoch [3/5], Step [5300/6000], Loss: 0.0146\n",
      "Epoch [3/5], Step [5400/6000], Loss: 0.0014\n",
      "Epoch [3/5], Step [5500/6000], Loss: 0.0229\n",
      "Epoch [3/5], Step [5600/6000], Loss: 0.0040\n",
      "Epoch [3/5], Step [5700/6000], Loss: 0.0762\n",
      "Epoch [3/5], Step [5800/6000], Loss: 0.0010\n",
      "Epoch [3/5], Step [5900/6000], Loss: 0.0138\n",
      "Epoch [3/5], Step [6000/6000], Loss: 0.3100\n",
      "Epoch [4/5], Step [100/6000], Loss: 0.1412\n",
      "Epoch [4/5], Step [200/6000], Loss: 0.0004\n",
      "Epoch [4/5], Step [300/6000], Loss: 0.0013\n",
      "Epoch [4/5], Step [400/6000], Loss: 0.6513\n",
      "Epoch [4/5], Step [500/6000], Loss: 0.0001\n",
      "Epoch [4/5], Step [600/6000], Loss: 0.0266\n",
      "Epoch [4/5], Step [700/6000], Loss: 0.0018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [800/6000], Loss: 0.0090\n",
      "Epoch [4/5], Step [900/6000], Loss: 0.0032\n",
      "Epoch [4/5], Step [1000/6000], Loss: 0.0238\n",
      "Epoch [4/5], Step [1100/6000], Loss: 0.3381\n",
      "Epoch [4/5], Step [1200/6000], Loss: 0.0026\n",
      "Epoch [4/5], Step [1300/6000], Loss: 0.1499\n",
      "Epoch [4/5], Step [1400/6000], Loss: 0.0425\n",
      "Epoch [4/5], Step [1500/6000], Loss: 0.0081\n",
      "Epoch [4/5], Step [1600/6000], Loss: 0.0003\n",
      "Epoch [4/5], Step [1700/6000], Loss: 0.0019\n",
      "Epoch [4/5], Step [1800/6000], Loss: 0.0060\n",
      "Epoch [4/5], Step [1900/6000], Loss: 0.0278\n",
      "Epoch [4/5], Step [2000/6000], Loss: 0.0038\n",
      "Epoch [4/5], Step [2100/6000], Loss: 0.0016\n",
      "Epoch [4/5], Step [2200/6000], Loss: 0.0013\n",
      "Epoch [4/5], Step [2300/6000], Loss: 0.0837\n",
      "Epoch [4/5], Step [2400/6000], Loss: 0.0000\n",
      "Epoch [4/5], Step [2500/6000], Loss: 0.0012\n",
      "Epoch [4/5], Step [2600/6000], Loss: 0.0079\n",
      "Epoch [4/5], Step [2700/6000], Loss: 0.0005\n",
      "Epoch [4/5], Step [2800/6000], Loss: 0.0588\n",
      "Epoch [4/5], Step [2900/6000], Loss: 0.0026\n",
      "Epoch [4/5], Step [3000/6000], Loss: 0.0071\n",
      "Epoch [4/5], Step [3100/6000], Loss: 0.0003\n",
      "Epoch [4/5], Step [3200/6000], Loss: 0.0002\n",
      "Epoch [4/5], Step [3300/6000], Loss: 0.0027\n",
      "Epoch [4/5], Step [3400/6000], Loss: 0.0019\n",
      "Epoch [4/5], Step [3500/6000], Loss: 0.0681\n",
      "Epoch [4/5], Step [3600/6000], Loss: 0.0016\n",
      "Epoch [4/5], Step [3700/6000], Loss: 0.0009\n",
      "Epoch [4/5], Step [3800/6000], Loss: 0.6716\n",
      "Epoch [4/5], Step [3900/6000], Loss: 0.3430\n",
      "Epoch [4/5], Step [4000/6000], Loss: 0.0163\n",
      "Epoch [4/5], Step [4100/6000], Loss: 0.0001\n",
      "Epoch [4/5], Step [4200/6000], Loss: 0.0803\n",
      "Epoch [4/5], Step [4300/6000], Loss: 0.0112\n",
      "Epoch [4/5], Step [4400/6000], Loss: 0.0010\n",
      "Epoch [4/5], Step [4500/6000], Loss: 0.0023\n",
      "Epoch [4/5], Step [4600/6000], Loss: 0.1166\n",
      "Epoch [4/5], Step [4700/6000], Loss: 0.0003\n",
      "Epoch [4/5], Step [4800/6000], Loss: 0.0001\n",
      "Epoch [4/5], Step [4900/6000], Loss: 0.0007\n",
      "Epoch [4/5], Step [5000/6000], Loss: 0.0722\n",
      "Epoch [4/5], Step [5100/6000], Loss: 0.0218\n",
      "Epoch [4/5], Step [5200/6000], Loss: 0.0021\n",
      "Epoch [4/5], Step [5300/6000], Loss: 0.1823\n",
      "Epoch [4/5], Step [5400/6000], Loss: 0.0616\n",
      "Epoch [4/5], Step [5500/6000], Loss: 0.0025\n",
      "Epoch [4/5], Step [5600/6000], Loss: 0.0132\n",
      "Epoch [4/5], Step [5700/6000], Loss: 0.0000\n",
      "Epoch [4/5], Step [5800/6000], Loss: 0.0218\n",
      "Epoch [4/5], Step [5900/6000], Loss: 0.0280\n",
      "Epoch [4/5], Step [6000/6000], Loss: 0.0113\n",
      "Epoch [5/5], Step [100/6000], Loss: 0.0010\n",
      "Epoch [5/5], Step [200/6000], Loss: 0.0006\n",
      "Epoch [5/5], Step [300/6000], Loss: 0.0076\n",
      "Epoch [5/5], Step [400/6000], Loss: 0.0422\n",
      "Epoch [5/5], Step [500/6000], Loss: 0.0002\n",
      "Epoch [5/5], Step [600/6000], Loss: 0.0083\n",
      "Epoch [5/5], Step [700/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [800/6000], Loss: 0.0000\n",
      "Epoch [5/5], Step [900/6000], Loss: 0.0065\n",
      "Epoch [5/5], Step [1000/6000], Loss: 0.0011\n",
      "Epoch [5/5], Step [1100/6000], Loss: 0.0064\n",
      "Epoch [5/5], Step [1200/6000], Loss: 0.0037\n",
      "Epoch [5/5], Step [1300/6000], Loss: 0.0015\n",
      "Epoch [5/5], Step [1400/6000], Loss: 0.0120\n",
      "Epoch [5/5], Step [1500/6000], Loss: 0.0054\n",
      "Epoch [5/5], Step [1600/6000], Loss: 0.0046\n",
      "Epoch [5/5], Step [1700/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [1800/6000], Loss: 0.0004\n",
      "Epoch [5/5], Step [1900/6000], Loss: 0.0015\n",
      "Epoch [5/5], Step [2000/6000], Loss: 0.0000\n",
      "Epoch [5/5], Step [2100/6000], Loss: 0.0023\n",
      "Epoch [5/5], Step [2200/6000], Loss: 0.0057\n",
      "Epoch [5/5], Step [2300/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [2400/6000], Loss: 0.0000\n",
      "Epoch [5/5], Step [2500/6000], Loss: 0.1020\n",
      "Epoch [5/5], Step [2600/6000], Loss: 0.7986\n",
      "Epoch [5/5], Step [2700/6000], Loss: 0.0958\n",
      "Epoch [5/5], Step [2800/6000], Loss: 0.0078\n",
      "Epoch [5/5], Step [2900/6000], Loss: 0.0000\n",
      "Epoch [5/5], Step [3000/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [3100/6000], Loss: 0.0002\n",
      "Epoch [5/5], Step [3200/6000], Loss: 0.1128\n",
      "Epoch [5/5], Step [3300/6000], Loss: 0.0006\n",
      "Epoch [5/5], Step [3400/6000], Loss: 0.0053\n",
      "Epoch [5/5], Step [3500/6000], Loss: 0.2268\n",
      "Epoch [5/5], Step [3600/6000], Loss: 0.0027\n",
      "Epoch [5/5], Step [3700/6000], Loss: 0.0144\n",
      "Epoch [5/5], Step [3800/6000], Loss: 0.0259\n",
      "Epoch [5/5], Step [3900/6000], Loss: 0.4670\n",
      "Epoch [5/5], Step [4000/6000], Loss: 0.0073\n",
      "Epoch [5/5], Step [4100/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [4200/6000], Loss: 0.0019\n",
      "Epoch [5/5], Step [4300/6000], Loss: 0.0000\n",
      "Epoch [5/5], Step [4400/6000], Loss: 0.0004\n",
      "Epoch [5/5], Step [4500/6000], Loss: 0.0044\n",
      "Epoch [5/5], Step [4600/6000], Loss: 0.0027\n",
      "Epoch [5/5], Step [4700/6000], Loss: 0.0000\n",
      "Epoch [5/5], Step [4800/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [4900/6000], Loss: 0.0002\n",
      "Epoch [5/5], Step [5000/6000], Loss: 0.0152\n",
      "Epoch [5/5], Step [5100/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [5200/6000], Loss: 0.0936\n",
      "Epoch [5/5], Step [5300/6000], Loss: 0.0019\n",
      "Epoch [5/5], Step [5400/6000], Loss: 0.0138\n",
      "Epoch [5/5], Step [5500/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [5600/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [5700/6000], Loss: 0.0052\n",
      "Epoch [5/5], Step [5800/6000], Loss: 0.0312\n",
      "Epoch [5/5], Step [5900/6000], Loss: 0.0007\n",
      "Epoch [5/5], Step [6000/6000], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Convert torch tensor to Variable\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()  ### zero the gradient buffer ------- Clears the gradients of all optimized\n",
    "        inputs = net(images) ### IAS: This will call forward() from __call__\n",
    "        loss = criterion(inputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.zero_grad?\n",
    "# loss.backward?\n",
    "# optimizer.step?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, 28*28))\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1:\n",
    "\n",
    "Change the number of epochs to 10 and batch size to 50. Check the output for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [200/1200], Loss: 0.0076\n",
      "Epoch [1/10], Step [300/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [400/1200], Loss: 0.0032\n",
      "Epoch [1/10], Step [500/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [600/1200], Loss: 0.0001\n",
      "Epoch [1/10], Step [700/1200], Loss: 0.0012\n",
      "Epoch [1/10], Step [800/1200], Loss: 0.0006\n",
      "Epoch [1/10], Step [900/1200], Loss: 0.0361\n",
      "Epoch [1/10], Step [1000/1200], Loss: 0.0005\n",
      "Epoch [1/10], Step [1100/1200], Loss: 0.0484\n",
      "Epoch [1/10], Step [1200/1200], Loss: 0.0041\n",
      "Epoch [1/10], Step [1300/1200], Loss: 0.0012\n",
      "Epoch [1/10], Step [1400/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [1500/1200], Loss: 0.0007\n",
      "Epoch [1/10], Step [1600/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [1700/1200], Loss: 0.0002\n",
      "Epoch [1/10], Step [1800/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [1900/1200], Loss: 0.0010\n",
      "Epoch [1/10], Step [2000/1200], Loss: 0.0751\n",
      "Epoch [1/10], Step [2100/1200], Loss: 0.0007\n",
      "Epoch [1/10], Step [2200/1200], Loss: 0.0010\n",
      "Epoch [1/10], Step [2300/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [2400/1200], Loss: 0.0012\n",
      "Epoch [1/10], Step [2500/1200], Loss: 0.0007\n",
      "Epoch [1/10], Step [2600/1200], Loss: 0.0009\n",
      "Epoch [1/10], Step [2700/1200], Loss: 0.1283\n",
      "Epoch [1/10], Step [2800/1200], Loss: 0.0011\n",
      "Epoch [1/10], Step [2900/1200], Loss: 0.0005\n",
      "Epoch [1/10], Step [3000/1200], Loss: 0.0018\n",
      "Epoch [1/10], Step [3100/1200], Loss: 0.8833\n",
      "Epoch [1/10], Step [3200/1200], Loss: 0.0046\n",
      "Epoch [1/10], Step [3300/1200], Loss: 0.0002\n",
      "Epoch [1/10], Step [3400/1200], Loss: 0.0004\n",
      "Epoch [1/10], Step [3500/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [3600/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [3700/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [3800/1200], Loss: 0.0007\n",
      "Epoch [1/10], Step [3900/1200], Loss: 0.0138\n",
      "Epoch [1/10], Step [4000/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [4100/1200], Loss: 0.0001\n",
      "Epoch [1/10], Step [4200/1200], Loss: 0.0002\n",
      "Epoch [1/10], Step [4300/1200], Loss: 0.0025\n",
      "Epoch [1/10], Step [4400/1200], Loss: 0.0001\n",
      "Epoch [1/10], Step [4500/1200], Loss: 0.0002\n",
      "Epoch [1/10], Step [4600/1200], Loss: 0.0039\n",
      "Epoch [1/10], Step [4700/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [4800/1200], Loss: 0.0054\n",
      "Epoch [1/10], Step [4900/1200], Loss: 0.0554\n",
      "Epoch [1/10], Step [5000/1200], Loss: 0.0055\n",
      "Epoch [1/10], Step [5100/1200], Loss: 0.0010\n",
      "Epoch [1/10], Step [5200/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [5300/1200], Loss: 0.0015\n",
      "Epoch [1/10], Step [5400/1200], Loss: 0.0001\n",
      "Epoch [1/10], Step [5500/1200], Loss: 0.0000\n",
      "Epoch [1/10], Step [5600/1200], Loss: 0.0070\n",
      "Epoch [1/10], Step [5700/1200], Loss: 0.5301\n",
      "Epoch [1/10], Step [5800/1200], Loss: 0.0425\n",
      "Epoch [1/10], Step [5900/1200], Loss: 0.0016\n",
      "Epoch [1/10], Step [6000/1200], Loss: 0.0001\n",
      "Epoch [2/10], Step [100/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [200/1200], Loss: 0.0006\n",
      "Epoch [2/10], Step [300/1200], Loss: 0.0569\n",
      "Epoch [2/10], Step [400/1200], Loss: 0.0008\n",
      "Epoch [2/10], Step [500/1200], Loss: 0.0029\n",
      "Epoch [2/10], Step [600/1200], Loss: 0.2956\n",
      "Epoch [2/10], Step [700/1200], Loss: 0.0025\n",
      "Epoch [2/10], Step [800/1200], Loss: 0.0188\n",
      "Epoch [2/10], Step [900/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [1000/1200], Loss: 0.0162\n",
      "Epoch [2/10], Step [1100/1200], Loss: 0.0016\n",
      "Epoch [2/10], Step [1200/1200], Loss: 0.0108\n",
      "Epoch [2/10], Step [1300/1200], Loss: 0.0001\n",
      "Epoch [2/10], Step [1400/1200], Loss: 0.0001\n",
      "Epoch [2/10], Step [1500/1200], Loss: 0.0001\n",
      "Epoch [2/10], Step [1600/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [1700/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [1800/1200], Loss: 0.0002\n",
      "Epoch [2/10], Step [1900/1200], Loss: 0.0001\n",
      "Epoch [2/10], Step [2000/1200], Loss: 0.0002\n",
      "Epoch [2/10], Step [2100/1200], Loss: 0.0028\n",
      "Epoch [2/10], Step [2200/1200], Loss: 0.0016\n",
      "Epoch [2/10], Step [2300/1200], Loss: 0.0007\n",
      "Epoch [2/10], Step [2400/1200], Loss: 0.0009\n",
      "Epoch [2/10], Step [2500/1200], Loss: 0.0001\n",
      "Epoch [2/10], Step [2600/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [2700/1200], Loss: 0.0388\n",
      "Epoch [2/10], Step [2800/1200], Loss: 0.0070\n",
      "Epoch [2/10], Step [2900/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [3000/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [3100/1200], Loss: 0.0001\n",
      "Epoch [2/10], Step [3200/1200], Loss: 0.0003\n",
      "Epoch [2/10], Step [3300/1200], Loss: 0.0012\n",
      "Epoch [2/10], Step [3400/1200], Loss: 0.0216\n",
      "Epoch [2/10], Step [3500/1200], Loss: 0.0001\n",
      "Epoch [2/10], Step [3600/1200], Loss: 0.1034\n",
      "Epoch [2/10], Step [3700/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [3800/1200], Loss: 0.0034\n",
      "Epoch [2/10], Step [3900/1200], Loss: 0.0223\n",
      "Epoch [2/10], Step [4000/1200], Loss: 0.0039\n",
      "Epoch [2/10], Step [4100/1200], Loss: 0.1392\n",
      "Epoch [2/10], Step [4200/1200], Loss: 0.0020\n",
      "Epoch [2/10], Step [4300/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [4400/1200], Loss: 0.0110\n",
      "Epoch [2/10], Step [4500/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [4600/1200], Loss: 0.0001\n",
      "Epoch [2/10], Step [4700/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [4800/1200], Loss: 0.0043\n",
      "Epoch [2/10], Step [4900/1200], Loss: 0.0003\n",
      "Epoch [2/10], Step [5000/1200], Loss: 0.0020\n",
      "Epoch [2/10], Step [5100/1200], Loss: 0.0139\n",
      "Epoch [2/10], Step [5200/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [5300/1200], Loss: 0.0105\n",
      "Epoch [2/10], Step [5400/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [5500/1200], Loss: 0.0004\n",
      "Epoch [2/10], Step [5600/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [5700/1200], Loss: 0.3133\n",
      "Epoch [2/10], Step [5800/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [5900/1200], Loss: 0.0000\n",
      "Epoch [2/10], Step [6000/1200], Loss: 0.0066\n",
      "Epoch [3/10], Step [100/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [200/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [300/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [400/1200], Loss: 0.0826\n",
      "Epoch [3/10], Step [500/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [600/1200], Loss: 0.0001\n",
      "Epoch [3/10], Step [700/1200], Loss: 0.0002\n",
      "Epoch [3/10], Step [800/1200], Loss: 0.0213\n",
      "Epoch [3/10], Step [900/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [1000/1200], Loss: 0.0001\n",
      "Epoch [3/10], Step [1100/1200], Loss: 0.0002\n",
      "Epoch [3/10], Step [1200/1200], Loss: 0.0002\n",
      "Epoch [3/10], Step [1300/1200], Loss: 0.0003\n",
      "Epoch [3/10], Step [1400/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [1500/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [1600/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [1700/1200], Loss: 0.3911\n",
      "Epoch [3/10], Step [1800/1200], Loss: 0.0006\n",
      "Epoch [3/10], Step [1900/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [2000/1200], Loss: 0.2860\n",
      "Epoch [3/10], Step [2100/1200], Loss: 0.0483\n",
      "Epoch [3/10], Step [2200/1200], Loss: 0.2351\n",
      "Epoch [3/10], Step [2300/1200], Loss: 0.0001\n",
      "Epoch [3/10], Step [2400/1200], Loss: 0.0009\n",
      "Epoch [3/10], Step [2500/1200], Loss: 0.9118\n",
      "Epoch [3/10], Step [2600/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [2700/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [2800/1200], Loss: 0.0001\n",
      "Epoch [3/10], Step [2900/1200], Loss: 0.0004\n",
      "Epoch [3/10], Step [3000/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [3100/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [3200/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [3300/1200], Loss: 0.0007\n",
      "Epoch [3/10], Step [3400/1200], Loss: 0.0001\n",
      "Epoch [3/10], Step [3500/1200], Loss: 0.0150\n",
      "Epoch [3/10], Step [3600/1200], Loss: 0.0012\n",
      "Epoch [3/10], Step [3700/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [3800/1200], Loss: 0.0265\n",
      "Epoch [3/10], Step [3900/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [4000/1200], Loss: 0.0002\n",
      "Epoch [3/10], Step [4100/1200], Loss: 0.0002\n",
      "Epoch [3/10], Step [4200/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [4300/1200], Loss: 0.1678\n",
      "Epoch [3/10], Step [4400/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [4500/1200], Loss: 0.0001\n",
      "Epoch [3/10], Step [4600/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [4700/1200], Loss: 0.0017\n",
      "Epoch [3/10], Step [4800/1200], Loss: 0.0002\n",
      "Epoch [3/10], Step [4900/1200], Loss: 0.3909\n",
      "Epoch [3/10], Step [5000/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [5100/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [5200/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [5300/1200], Loss: 0.0108\n",
      "Epoch [3/10], Step [5400/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [5500/1200], Loss: 0.0338\n",
      "Epoch [3/10], Step [5600/1200], Loss: 0.0000\n",
      "Epoch [3/10], Step [5700/1200], Loss: 0.0004\n",
      "Epoch [3/10], Step [5800/1200], Loss: 0.0021\n",
      "Epoch [3/10], Step [5900/1200], Loss: 0.0002\n",
      "Epoch [3/10], Step [6000/1200], Loss: 0.0009\n",
      "Epoch [4/10], Step [100/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [200/1200], Loss: 0.0018\n",
      "Epoch [4/10], Step [300/1200], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Step [400/1200], Loss: 0.0003\n",
      "Epoch [4/10], Step [500/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [600/1200], Loss: 1.4602\n",
      "Epoch [4/10], Step [700/1200], Loss: 0.0004\n",
      "Epoch [4/10], Step [800/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [900/1200], Loss: 0.0001\n",
      "Epoch [4/10], Step [1000/1200], Loss: 0.0001\n",
      "Epoch [4/10], Step [1100/1200], Loss: 0.0003\n",
      "Epoch [4/10], Step [1200/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [1300/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [1400/1200], Loss: 1.5628\n",
      "Epoch [4/10], Step [1500/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [1600/1200], Loss: 0.0005\n",
      "Epoch [4/10], Step [1700/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [1800/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [1900/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [2000/1200], Loss: 0.0001\n",
      "Epoch [4/10], Step [2100/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [2200/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [2300/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [2400/1200], Loss: 0.0151\n",
      "Epoch [4/10], Step [2500/1200], Loss: 1.0561\n",
      "Epoch [4/10], Step [2600/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [2700/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [2800/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [2900/1200], Loss: 0.0002\n",
      "Epoch [4/10], Step [3000/1200], Loss: 0.0057\n",
      "Epoch [4/10], Step [3100/1200], Loss: 0.0010\n",
      "Epoch [4/10], Step [3200/1200], Loss: 0.0001\n",
      "Epoch [4/10], Step [3300/1200], Loss: 0.3311\n",
      "Epoch [4/10], Step [3400/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [3500/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [3600/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [3700/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [3800/1200], Loss: 0.0010\n",
      "Epoch [4/10], Step [3900/1200], Loss: 0.0441\n",
      "Epoch [4/10], Step [4000/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [4100/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [4200/1200], Loss: 0.0002\n",
      "Epoch [4/10], Step [4300/1200], Loss: 0.0176\n",
      "Epoch [4/10], Step [4400/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [4500/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [4600/1200], Loss: 0.0003\n",
      "Epoch [4/10], Step [4700/1200], Loss: 0.0205\n",
      "Epoch [4/10], Step [4800/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [4900/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [5000/1200], Loss: 0.0003\n",
      "Epoch [4/10], Step [5100/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [5200/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [5300/1200], Loss: 0.0011\n",
      "Epoch [4/10], Step [5400/1200], Loss: 0.0429\n",
      "Epoch [4/10], Step [5500/1200], Loss: 0.0088\n",
      "Epoch [4/10], Step [5600/1200], Loss: 0.0007\n",
      "Epoch [4/10], Step [5700/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [5800/1200], Loss: 0.0031\n",
      "Epoch [4/10], Step [5900/1200], Loss: 0.0000\n",
      "Epoch [4/10], Step [6000/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [100/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [200/1200], Loss: 0.0001\n",
      "Epoch [5/10], Step [300/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [400/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [500/1200], Loss: 0.1755\n",
      "Epoch [5/10], Step [600/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [700/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [800/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [900/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [1000/1200], Loss: 0.0001\n",
      "Epoch [5/10], Step [1100/1200], Loss: 0.0001\n",
      "Epoch [5/10], Step [1200/1200], Loss: 0.0180\n",
      "Epoch [5/10], Step [1300/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [1400/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [1500/1200], Loss: 0.0001\n",
      "Epoch [5/10], Step [1600/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [1700/1200], Loss: 0.0008\n",
      "Epoch [5/10], Step [1800/1200], Loss: 0.0004\n",
      "Epoch [5/10], Step [1900/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [2000/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [2100/1200], Loss: 0.0136\n",
      "Epoch [5/10], Step [2200/1200], Loss: 0.0748\n",
      "Epoch [5/10], Step [2300/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [2400/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [2500/1200], Loss: 0.0004\n",
      "Epoch [5/10], Step [2600/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [2700/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [2800/1200], Loss: 0.0016\n",
      "Epoch [5/10], Step [2900/1200], Loss: 0.0001\n",
      "Epoch [5/10], Step [3000/1200], Loss: 0.0001\n",
      "Epoch [5/10], Step [3100/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [3200/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [3300/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [3400/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [3500/1200], Loss: 0.0006\n",
      "Epoch [5/10], Step [3600/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [3700/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [3800/1200], Loss: 0.0004\n",
      "Epoch [5/10], Step [3900/1200], Loss: 0.0679\n",
      "Epoch [5/10], Step [4000/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [4100/1200], Loss: 1.7015\n",
      "Epoch [5/10], Step [4200/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [4300/1200], Loss: 0.0223\n",
      "Epoch [5/10], Step [4400/1200], Loss: 0.0053\n",
      "Epoch [5/10], Step [4500/1200], Loss: 0.0001\n",
      "Epoch [5/10], Step [4600/1200], Loss: 0.0072\n",
      "Epoch [5/10], Step [4700/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [4800/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [4900/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [5000/1200], Loss: 0.0069\n",
      "Epoch [5/10], Step [5100/1200], Loss: 0.0002\n",
      "Epoch [5/10], Step [5200/1200], Loss: 0.0001\n",
      "Epoch [5/10], Step [5300/1200], Loss: 0.0040\n",
      "Epoch [5/10], Step [5400/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [5500/1200], Loss: 0.0003\n",
      "Epoch [5/10], Step [5600/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [5700/1200], Loss: 0.0000\n",
      "Epoch [5/10], Step [5800/1200], Loss: 0.0008\n",
      "Epoch [5/10], Step [5900/1200], Loss: 0.0250\n",
      "Epoch [5/10], Step [6000/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [100/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [200/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [300/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [400/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [500/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [600/1200], Loss: 0.0513\n",
      "Epoch [6/10], Step [700/1200], Loss: 0.0003\n",
      "Epoch [6/10], Step [800/1200], Loss: 0.0011\n",
      "Epoch [6/10], Step [900/1200], Loss: 0.0055\n",
      "Epoch [6/10], Step [1000/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [1100/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [1200/1200], Loss: 0.0003\n",
      "Epoch [6/10], Step [1300/1200], Loss: 0.0001\n",
      "Epoch [6/10], Step [1400/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [1500/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [1600/1200], Loss: 0.0013\n",
      "Epoch [6/10], Step [1700/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [1800/1200], Loss: 0.1093\n",
      "Epoch [6/10], Step [1900/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [2000/1200], Loss: 0.0002\n",
      "Epoch [6/10], Step [2100/1200], Loss: 0.0049\n",
      "Epoch [6/10], Step [2200/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [2300/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [2400/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [2500/1200], Loss: 0.0006\n",
      "Epoch [6/10], Step [2600/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [2700/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [2800/1200], Loss: 0.0002\n",
      "Epoch [6/10], Step [2900/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [3000/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [3100/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [3200/1200], Loss: 0.0003\n",
      "Epoch [6/10], Step [3300/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [3400/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [3500/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [3600/1200], Loss: 0.0001\n",
      "Epoch [6/10], Step [3700/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [3800/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [3900/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [4000/1200], Loss: 0.0001\n",
      "Epoch [6/10], Step [4100/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [4200/1200], Loss: 0.0020\n",
      "Epoch [6/10], Step [4300/1200], Loss: 0.0002\n",
      "Epoch [6/10], Step [4400/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [4500/1200], Loss: 0.0017\n",
      "Epoch [6/10], Step [4600/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [4700/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [4800/1200], Loss: 0.0006\n",
      "Epoch [6/10], Step [4900/1200], Loss: 0.3541\n",
      "Epoch [6/10], Step [5000/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [5100/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [5200/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [5300/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [5400/1200], Loss: 0.0123\n",
      "Epoch [6/10], Step [5500/1200], Loss: 0.0182\n",
      "Epoch [6/10], Step [5600/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [5700/1200], Loss: 0.3342\n",
      "Epoch [6/10], Step [5800/1200], Loss: 0.0000\n",
      "Epoch [6/10], Step [5900/1200], Loss: 0.0282\n",
      "Epoch [6/10], Step [6000/1200], Loss: 0.0009\n",
      "Epoch [7/10], Step [100/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [200/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [300/1200], Loss: 0.0184\n",
      "Epoch [7/10], Step [400/1200], Loss: 0.0002\n",
      "Epoch [7/10], Step [500/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [600/1200], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Step [700/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [800/1200], Loss: 0.0004\n",
      "Epoch [7/10], Step [900/1200], Loss: 0.0180\n",
      "Epoch [7/10], Step [1000/1200], Loss: 0.0001\n",
      "Epoch [7/10], Step [1100/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [1200/1200], Loss: 0.0021\n",
      "Epoch [7/10], Step [1300/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [1400/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [1500/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [1600/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [1700/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [1800/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [1900/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [2000/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [2100/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [2200/1200], Loss: 0.0028\n",
      "Epoch [7/10], Step [2300/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [2400/1200], Loss: 0.0010\n",
      "Epoch [7/10], Step [2500/1200], Loss: 0.0015\n",
      "Epoch [7/10], Step [2600/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [2700/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [2800/1200], Loss: 0.0052\n",
      "Epoch [7/10], Step [2900/1200], Loss: 0.0011\n",
      "Epoch [7/10], Step [3000/1200], Loss: 0.0001\n",
      "Epoch [7/10], Step [3100/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [3200/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [3300/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [3400/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [3500/1200], Loss: 0.0182\n",
      "Epoch [7/10], Step [3600/1200], Loss: 0.0052\n",
      "Epoch [7/10], Step [3700/1200], Loss: 0.0004\n",
      "Epoch [7/10], Step [3800/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [3900/1200], Loss: 0.0016\n",
      "Epoch [7/10], Step [4000/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [4100/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [4200/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [4300/1200], Loss: 0.0001\n",
      "Epoch [7/10], Step [4400/1200], Loss: 0.0007\n",
      "Epoch [7/10], Step [4500/1200], Loss: 0.0001\n",
      "Epoch [7/10], Step [4600/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [4700/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [4800/1200], Loss: 0.0701\n",
      "Epoch [7/10], Step [4900/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [5000/1200], Loss: 0.0002\n",
      "Epoch [7/10], Step [5100/1200], Loss: 0.0012\n",
      "Epoch [7/10], Step [5200/1200], Loss: 0.0697\n",
      "Epoch [7/10], Step [5300/1200], Loss: 0.0001\n",
      "Epoch [7/10], Step [5400/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [5500/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [5600/1200], Loss: 0.3444\n",
      "Epoch [7/10], Step [5700/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [5800/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [5900/1200], Loss: 0.0000\n",
      "Epoch [7/10], Step [6000/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [100/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [200/1200], Loss: 0.0009\n",
      "Epoch [8/10], Step [300/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [400/1200], Loss: 0.0320\n",
      "Epoch [8/10], Step [500/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [600/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [700/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [800/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [900/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [1000/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [1100/1200], Loss: 0.0823\n",
      "Epoch [8/10], Step [1200/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [1300/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [1400/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [1500/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [1600/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [1700/1200], Loss: 0.0090\n",
      "Epoch [8/10], Step [1800/1200], Loss: 0.0001\n",
      "Epoch [8/10], Step [1900/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [2000/1200], Loss: 0.0002\n",
      "Epoch [8/10], Step [2100/1200], Loss: 0.1759\n",
      "Epoch [8/10], Step [2200/1200], Loss: 0.0003\n",
      "Epoch [8/10], Step [2300/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [2400/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [2500/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [2600/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [2700/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [2800/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [2900/1200], Loss: 0.0001\n",
      "Epoch [8/10], Step [3000/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [3100/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [3200/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [3300/1200], Loss: 0.0032\n",
      "Epoch [8/10], Step [3400/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [3500/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [3600/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [3700/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [3800/1200], Loss: 0.0055\n",
      "Epoch [8/10], Step [3900/1200], Loss: 0.0857\n",
      "Epoch [8/10], Step [4000/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [4100/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [4200/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [4300/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [4400/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [4500/1200], Loss: 0.3314\n",
      "Epoch [8/10], Step [4600/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [4700/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [4800/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [4900/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [5000/1200], Loss: 0.0001\n",
      "Epoch [8/10], Step [5100/1200], Loss: 0.3064\n",
      "Epoch [8/10], Step [5200/1200], Loss: 0.0002\n",
      "Epoch [8/10], Step [5300/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [5400/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [5500/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [5600/1200], Loss: 0.0347\n",
      "Epoch [8/10], Step [5700/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [5800/1200], Loss: 0.0000\n",
      "Epoch [8/10], Step [5900/1200], Loss: 0.0001\n",
      "Epoch [8/10], Step [6000/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [100/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [200/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [300/1200], Loss: 0.0018\n",
      "Epoch [9/10], Step [400/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [500/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [600/1200], Loss: 0.0060\n",
      "Epoch [9/10], Step [700/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [800/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [900/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [1000/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [1100/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [1200/1200], Loss: 0.0001\n",
      "Epoch [9/10], Step [1300/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [1400/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [1500/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [1600/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [1700/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [1800/1200], Loss: 0.0001\n",
      "Epoch [9/10], Step [1900/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [2000/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [2100/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [2200/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [2300/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [2400/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [2500/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [2600/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [2700/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [2800/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [2900/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [3000/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [3100/1200], Loss: 0.0013\n",
      "Epoch [9/10], Step [3200/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [3300/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [3400/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [3500/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [3600/1200], Loss: 0.0002\n",
      "Epoch [9/10], Step [3700/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [3800/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [3900/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [4000/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [4100/1200], Loss: 0.0577\n",
      "Epoch [9/10], Step [4200/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [4300/1200], Loss: 0.5151\n",
      "Epoch [9/10], Step [4400/1200], Loss: 0.0005\n",
      "Epoch [9/10], Step [4500/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [4600/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [4700/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [4800/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [4900/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [5000/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [5100/1200], Loss: 0.0012\n",
      "Epoch [9/10], Step [5200/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [5300/1200], Loss: 0.0039\n",
      "Epoch [9/10], Step [5400/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [5500/1200], Loss: 0.0046\n",
      "Epoch [9/10], Step [5600/1200], Loss: 0.0000\n",
      "Epoch [9/10], Step [5700/1200], Loss: 0.0003\n",
      "Epoch [9/10], Step [5800/1200], Loss: 0.0014\n",
      "Epoch [9/10], Step [5900/1200], Loss: 0.2504\n",
      "Epoch [9/10], Step [6000/1200], Loss: 0.0250\n",
      "Epoch [10/10], Step [100/1200], Loss: 0.0002\n",
      "Epoch [10/10], Step [200/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [300/1200], Loss: 0.0001\n",
      "Epoch [10/10], Step [400/1200], Loss: 0.0001\n",
      "Epoch [10/10], Step [500/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [600/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [700/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [800/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [900/1200], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Step [1000/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [1100/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [1200/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [1300/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [1400/1200], Loss: 0.0001\n",
      "Epoch [10/10], Step [1500/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [1600/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [1700/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [1800/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [1900/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [2000/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [2100/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [2200/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [2300/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [2400/1200], Loss: 0.0002\n",
      "Epoch [10/10], Step [2500/1200], Loss: 0.0016\n",
      "Epoch [10/10], Step [2600/1200], Loss: 0.0002\n",
      "Epoch [10/10], Step [2700/1200], Loss: 0.0024\n",
      "Epoch [10/10], Step [2800/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [2900/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [3000/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [3100/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [3200/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [3300/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [3400/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [3500/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [3600/1200], Loss: 0.0002\n",
      "Epoch [10/10], Step [3700/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [3800/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [3900/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [4000/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [4100/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [4200/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [4300/1200], Loss: 0.0001\n",
      "Epoch [10/10], Step [4400/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [4500/1200], Loss: 0.0719\n",
      "Epoch [10/10], Step [4600/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [4700/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [4800/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [4900/1200], Loss: 0.0002\n",
      "Epoch [10/10], Step [5000/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [5100/1200], Loss: 0.0001\n",
      "Epoch [10/10], Step [5200/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [5300/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [5400/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [5500/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [5600/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [5700/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [5800/1200], Loss: 0.0000\n",
      "Epoch [10/10], Step [5900/1200], Loss: 0.0002\n",
      "Epoch [10/10], Step [6000/1200], Loss: 0.0000\n",
      "Accuracy of the network on the 10000 test images: 98 %\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Convert torch tensor to Variable\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()  ### zero the gradient buffer ------- Clears the gradients of all optimized\n",
    "        inputs = net(images) ### IAS: This will call forward() from __call__\n",
    "        loss = criterion(inputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "            \n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, 28*28))\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We see that the accuracy increased by 1% :-) !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
