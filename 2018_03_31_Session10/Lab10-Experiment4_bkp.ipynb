{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of AI & ML\n",
    "## Session 10\n",
    "### Experiment 4\n",
    "#### RNNs for financial forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we will be using stock market dataset. The dataset contains historical daily price and volume data for all US-based stocks and E\n",
    "TFs trading on the NYSE, NASDAQ, and NYSE MKT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "### importing pytorch packages\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "##Loading the data folder\n",
    "print(os.listdir(\"../Dataset/Data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a model to load the stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x_data_to_process, y_data_to_process):\n",
    "        \"\"\" inputs for x and y values are given as pandas obj \"\"\"\n",
    "        self.data = pd.merge(x_data_to_process, y_data_to_process, on='Date')\n",
    "        self.data = self.data.values    # from pd to np\n",
    "\n",
    "        print('The shape of the data is {}'.format(self.data.shape))\n",
    "\n",
    "        self.x_data = self.data[:, 1:x_data_to_process.shape[1]].astype(np.float32)\n",
    "        self.y_data = self.data[:, x_data_to_process.shape[1]:].astype(np.float32)\n",
    "\n",
    "        \"\"\" Normalize x_data, putting it off for now \"\"\"\n",
    "        self.rebase_to_one()\n",
    "\n",
    "        \"\"\" convert to torch \"\"\"\n",
    "        self.x_data = torch.from_numpy(self.x_data)\n",
    "        self.y_data = torch.from_numpy(self.y_data)\n",
    "\n",
    "        self.len = self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def rebase_to_one(self):\n",
    "        \"\"\" self.x_data rebased to one on the firsts element \"\"\"\n",
    "        self.x_data = self.x_data.T\n",
    "        list_of_first_elem_price = [i[0] for i in self.x_data]\n",
    "        shape_row, shape_column = self.x_data.shape[0], self.x_data.shape[1]\n",
    "\n",
    "        for i in range(shape_row):\n",
    "            for j in range(shape_column):\n",
    "                self.x_data[i][j] /= list_of_first_elem_price[i]\n",
    "        self.x_data = self.x_data.T\n",
    "\n",
    "\n",
    "# We load the stock data from the dataset folder.\n",
    "def get_dictionary_of_data():\n",
    "    def get_dictionary_of_data_helper(list_of_stocks):\n",
    "        dict_of_stocks = {}\n",
    "        for i in range(len(list_of_stocks)):\n",
    "            key = str(list_of_stocks[i]).lower()\n",
    "            path ='../Dataset/Data/Stocks/{}.us.txt'.format(key)\n",
    "            try:\n",
    "                dict_of_stocks[key] = pd.read_csv(path)\n",
    "            except FileNotFoundError:\n",
    "                print(\"File of {} not found\".format(key))\n",
    "\n",
    "        return dict_of_stocks\n",
    "\n",
    "    # Use these tickers if possible to predict S&P's value\n",
    "    spx_partial_list = np.array(['MMM', 'AXP', 'AAPL', 'BA', 'CAT', 'CVX', 'CSCO',\n",
    "                                     'KO', 'DWDP', 'XOM', 'GE', 'GS', 'HD', 'IBM', 'INTC',\n",
    "                                     'JNJ', 'JPM', 'MCD', 'MRK', 'MSFT', 'NKE', 'PFE', 'PG',\n",
    "                                     'TRV', 'UNH', 'UTX', 'VZ', 'V', 'WMT', 'DIS'])\n",
    "\n",
    "    return get_dictionary_of_data_helper(spx_partial_list)\n",
    "\n",
    "### we are collecting closing price data \n",
    "def closing_prices_in_pd(dict_of_stocks):\n",
    "    \"\"\" merge stock data on Dates \"\"\"\n",
    "    temp_dict = {}\n",
    "    for i in dict_of_stocks.keys():\n",
    "        temp_dict[i] = (dict_of_stocks[i])[['Date', 'Close', 'Volume']]\n",
    "\n",
    "    merged = None\n",
    "    for i in temp_dict.keys():\n",
    "        if merged is None:\n",
    "            merged = temp_dict[i]\n",
    "        elif temp_dict[i].shape[0] > 3000: # Arbitrary selection, longer than 3000\n",
    "            merged = pd.merge(merged, temp_dict[i], on='Date')\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us build a rnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, rnn_hidden_size, output_size):\n",
    "\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.rnn = torch.nn.RNN(1, rnn_hidden_size,\n",
    "                                num_layers=2, nonlinearity='relu',\n",
    "                                batch_first=True)\n",
    "        self.h_0 = self.initialize_hidden(rnn_hidden_size)\n",
    "\n",
    "        self.linear = torch.nn.Linear(rnn_hidden_size, output_size)\n",
    "    ### Forward Pass\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.unsqueeze(0)\n",
    "        self.rnn.flatten_parameters()\n",
    "        out, self.h_0 = self.rnn(x, self.h_0)\n",
    "\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "    def initialize_hidden(self, rnn_hidden_size):\n",
    "        # n_layers * n_directions, batch_size, rnn_hidden_size\n",
    "        return Variable(torch.randn(2, 1, rnn_hidden_size),\n",
    "                        requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_HIDDEN_SIZE = 32\n",
    "\n",
    "\n",
    "def train(input_size, hidden_size, output_size, train_loader):\n",
    "    plt.figure(1, figsize=(12, 5))\n",
    "\n",
    "    file_path = 'my_model.model'\n",
    "\n",
    "    try:\n",
    "        model = torch.load(file_path)\n",
    "    except:\n",
    "        model = Model(input_size, hidden_size, output_size)\n",
    "    ## Defining loss function and optimizer\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    epochs = 25\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        predictions = []\n",
    "        correct_values = []\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            xs, ys = data\n",
    "            ## Converting xs,ys into tourch variables\n",
    "            xs, ys = Variable(xs), Variable(ys)\n",
    "            ## predicting the values\n",
    "            y_pred = model(xs)\n",
    "            ## Calculating the loss\n",
    "            loss = criterion(y_pred, ys)\n",
    "            optimizer.zero_grad()\n",
    "            ## Backward Pass\n",
    "            loss.backward(retain_graph=True)\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            ### Storing the predicted values into list in form of numpy array\n",
    "            predictions.append(y_pred.cpu().data.numpy().ravel())\n",
    "            correct_values.append(ys.cpu().data.numpy().ravel())\n",
    "            \n",
    "        def stacking_for_charting(given_list):\n",
    "            ret = np.array([0])\n",
    "            for i in given_list:\n",
    "                ret = np.hstack((ret, i.ravel()))\n",
    "            return ret[1:]\n",
    "        ## Stacking the predictions values\n",
    "        predictions_for_chart = stacking_for_charting(predictions)\n",
    "        correct_values_for_chart = stacking_for_charting(correct_values)\n",
    "\n",
    "        print(predictions_for_chart)\n",
    "\n",
    "        steps = np.linspace(epoch*predictions_for_chart.shape[0],\n",
    "                            (epoch+1)*predictions_for_chart.shape[0],\n",
    "                            predictions_for_chart.shape[0])\n",
    "        ## plotting the predicted values\n",
    "        plt.plot(steps, predictions_for_chart, 'r-')\n",
    "        ##plotting the original values\n",
    "        plt.plot(steps, correct_values_for_chart, 'b-')\n",
    "        plt.draw()\n",
    "        plt.pause(0.05)\n",
    "\n",
    "    torch.save(model, file_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    \"\"\" getting dictionary of stock data ETFs\"\"\"\n",
    "    X_data_source = get_dictionary_of_data()\n",
    "\n",
    "    \"\"\" etf data is used as Y variable\"\"\"\n",
    "    Y_data_source = {'spy': pd.read_csv('../Dataset/Data/ETFs/spy.us.txt')}\n",
    "\n",
    "    X_data = closing_prices_in_pd(X_data_source)\n",
    "    Y_data = closing_prices_in_pd(Y_data_source)\n",
    "\n",
    "    Y_data = Y_data.drop(['Volume'], axis=1)\n",
    "\n",
    "    dataset = StockDataset(X_data, Y_data)\n",
    "    ## Loading the dataset\n",
    "    train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=False, num_workers=1)\n",
    "\n",
    "    input_size = X_data.shape[1]-1\n",
    "    hidden_size = RNN_HIDDEN_SIZE\n",
    "    output_size = Y_data.shape[1]-1\n",
    "    ## training the model\n",
    "    train(input_size, hidden_size, output_size, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
